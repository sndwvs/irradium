diff -up mesa-23.3.0-rc5/.pick_status.json.8~ mesa-23.3.0-rc5/.pick_status.json
--- mesa-23.3.0-rc5/.pick_status.json.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/.pick_status.json	2023-11-24 23:33:24.959610812 +0100
@@ -10764,7 +10764,7 @@
         "description": "panfrost: use perf_debug instead of open-coding",
         "nominated": true,
         "nomination_type": 1,
-        "resolution": 1,
+        "resolution": 0,
         "main_sha": null,
         "because_sha": "bc55d150a915d5b2e91cd6ee11af4992d18fcf4f",
         "notes": null
diff -up mesa-23.3.0-rc5/include/drm-uapi/panthor_drm.h.8~ mesa-23.3.0-rc5/include/drm-uapi/panthor_drm.h
--- mesa-23.3.0-rc5/include/drm-uapi/panthor_drm.h.8~	2023-11-24 23:33:24.959610812 +0100
+++ mesa-23.3.0-rc5/include/drm-uapi/panthor_drm.h	2023-11-24 23:33:24.959610812 +0100
@@ -0,0 +1,863 @@
+/* SPDX-License-Identifier: MIT */
+/* Copyright (C) 2023 Collabora ltd. */
+#ifndef _PANTHOR_DRM_H_
+#define _PANTHOR_DRM_H_
+
+#include "drm.h"
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/**
+ * DOC: Introduction
+ *
+ * This documentation decribes the Panthor IOCTLs.
+ *
+ * Just a few generic rules about the data passed to the Panthor IOCTLs:
+ *
+ * - Structures must be aligned on 64-bit/8-byte. If the object is not
+ *   naturally aligned, a padding field must be added.
+ * - Fields must be explicity aligned to their natural type alignment with
+ *   pad[0..N] fields.
+ * - All padding fields will be checked by the driver to make sure they are
+ *   zeroed.
+ * - Flags can be added, but not removed/replaced.
+ * - New fields can be added to the main structures (the structures
+ *   directly passed to the ioctl). Those fiels can be added at the end of
+ *   the structure, or replace existing padding fields. Any new field being
+ *   added must preserve the behavior that existed before those fields were
+ *   added when a value of zero is passed.
+ * - New fields can be added to indirect objects (objects pointed by the
+ *   main structure), iff those objects are passed a size to reflect the
+ *   size known by the userspace driver (see drm_panthor_obj_array::stride
+ *   or drm_panthor_dev_query::size).
+ * - If the kernel driver is too old to know some fields, those will
+ *   be ignored (input) and set back to zero (output).
+ * - If userspace is too old to know some fields, those will be zeroed
+ *   (input) before the structure is parsed by the kernel driver.
+ * - Each new flag/field addition must come with a driver version update so
+ *   the userspace driver doesn't have to trial and error to know which
+ *   flags are supported.
+ * - Structures should not contain unions, as this would defeat the
+ *   extensibility of such structures.
+ * - IOCTLs can't be removed or replaced. New IOCTL IDs should be placed
+ *   at the end of the drm_panthor_ioctl_id enum.
+ */
+
+/**
+ * DOC: MMIO regions exposed to userspace.
+ *
+ * .. c:macro:: DRM_PANTHOR_USER_MMIO_OFFSET
+ *
+ * File offset for all MMIO regions being exposed to userspace. Don't use
+ * this value directly, use DRM_PANTHOR_USER_<name>_OFFSET values instead.
+ *
+ * .. c:macro:: DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET
+ *
+ * File offset for the LATEST_FLUSH_ID register. The Userspace driver controls
+ * GPU cache flushling through CS instructions, but the flush reduction
+ * mechanism requires a flush_id. This flush_id could be queried with an
+ * ioctl, but Arm provides a well-isolated register page containing only this
+ * read-only register, so let's expose this page through a static mmap offset
+ * and allow direct mapping of this MMIO region so we can avoid the
+ * user <-> kernel round-trip.
+ */
+#define DRM_PANTHOR_USER_MMIO_OFFSET		(0x1ull << 56)
+#define DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET	(DRM_PANTHOR_USER_MMIO_OFFSET | 0)
+
+/**
+ * DOC: IOCTL IDs
+ *
+ * enum drm_panthor_ioctl_id - IOCTL IDs
+ *
+ * Place new ioctls at the end, don't re-oder, don't replace or remove entries.
+ *
+ * These IDs are not meant to be used directly. Use the DRM_IOCTL_PANTHOR_xxx
+ * definitions instead.
+ */
+enum drm_panthor_ioctl_id {
+	/** @DRM_PANTHOR_DEV_QUERY: Query device information. */
+	DRM_PANTHOR_DEV_QUERY = 0,
+
+	/** @DRM_PANTHOR_VM_CREATE: Create a VM. */
+	DRM_PANTHOR_VM_CREATE,
+
+	/** @DRM_PANTHOR_VM_DESTROY: Destroy a VM. */
+	DRM_PANTHOR_VM_DESTROY,
+
+	/** @DRM_PANTHOR_VM_BIND: Bind/unbind memory to a VM. */
+	DRM_PANTHOR_VM_BIND,
+
+	/** @DRM_PANTHOR_BO_CREATE: Create a buffer object. */
+	DRM_PANTHOR_BO_CREATE,
+
+	/**
+	 * @DRM_PANTHOR_BO_MMAP_OFFSET: Get the file offset to pass to
+	 * mmap to map a GEM object.
+	 */
+	DRM_PANTHOR_BO_MMAP_OFFSET,
+
+	/** @DRM_PANTHOR_GROUP_CREATE: Create a scheduling group. */
+	DRM_PANTHOR_GROUP_CREATE,
+
+	/** @DRM_PANTHOR_GROUP_DESTROY: Destroy a scheduling group. */
+	DRM_PANTHOR_GROUP_DESTROY,
+
+	/**
+	 * @DRM_PANTHOR_GROUP_SUBMIT: Submit jobs to queues belonging
+	 * to a specific scheduling group.
+	 */
+	DRM_PANTHOR_GROUP_SUBMIT,
+
+	/** @DRM_PANTHOR_GROUP_GET_STATE: Get the state of a scheduling group. */
+	DRM_PANTHOR_GROUP_GET_STATE,
+
+	/** @DRM_PANTHOR_TILER_HEAP_CREATE: Create a tiler heap. */
+	DRM_PANTHOR_TILER_HEAP_CREATE,
+
+	/** @DRM_PANTHOR_TILER_HEAP_DESTROY: Destroy a tiler heap. */
+	DRM_PANTHOR_TILER_HEAP_DESTROY,
+};
+
+
+/**
+ * DRM_IOCTL_PANTHOR() - Build a Panthor IOCTL number
+ * @__access: Access type. Must be R, W or RW.
+ * @__id: One of the DRM_PANTHOR_xxx id.
+ * @__type: Suffix of the type being passed to the IOCTL.
+ *
+ * Don't use this macro directly, use the DRM_IOCTL_PANTHOR_xxx
+ * values instead.
+ *
+ * Return: An IOCTL number to be passed to ioctl() from userspace.
+ */
+#define DRM_IOCTL_PANTHOR(__access, __id, __type) \
+	DRM_IO ## __access(DRM_COMMAND_BASE + DRM_PANTHOR_ ## __id, \
+			   struct drm_panthor_ ## __type)
+
+#define DRM_IOCTL_PANTHOR_DEV_QUERY \
+	DRM_IOCTL_PANTHOR(WR, DEV_QUERY, dev_query)
+#define DRM_IOCTL_PANTHOR_VM_CREATE \
+	DRM_IOCTL_PANTHOR(WR, VM_CREATE, vm_create)
+#define DRM_IOCTL_PANTHOR_VM_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, VM_DESTROY, vm_destroy)
+#define DRM_IOCTL_PANTHOR_VM_BIND \
+	DRM_IOCTL_PANTHOR(WR, VM_BIND, vm_bind)
+#define DRM_IOCTL_PANTHOR_BO_CREATE \
+	DRM_IOCTL_PANTHOR(WR, BO_CREATE, bo_create)
+#define DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET \
+	DRM_IOCTL_PANTHOR(WR, BO_MMAP_OFFSET, bo_mmap_offset)
+#define DRM_IOCTL_PANTHOR_GROUP_CREATE \
+	DRM_IOCTL_PANTHOR(WR, GROUP_CREATE, group_create)
+#define DRM_IOCTL_PANTHOR_GROUP_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, GROUP_DESTROY, group_destroy)
+#define DRM_IOCTL_PANTHOR_GROUP_SUBMIT \
+	DRM_IOCTL_PANTHOR(WR, GROUP_SUBMIT, group_submit)
+#define DRM_IOCTL_PANTHOR_GROUP_GET_STATE \
+	DRM_IOCTL_PANTHOR(WR, GROUP_GET_STATE, group_get_state)
+#define DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE \
+	DRM_IOCTL_PANTHOR(WR, TILER_HEAP_CREATE, tiler_heap_create)
+#define DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY \
+	DRM_IOCTL_PANTHOR(WR, TILER_HEAP_DESTROY, tiler_heap_destroy)
+
+/**
+ * DOC: IOCTL arguments
+ */
+
+/**
+ * struct drm_panthor_obj_array - Object array.
+ *
+ * This object is used to pass an array of objects whose size it subject to changes in
+ * future versions of the driver. In order to support this mutability, we pass a stride
+ * describing the size of the object as known by userspace.
+ *
+ * You shouldn't fill drm_panthor_obj_array fields directly. You should instead use
+ * the DRM_PANTHOR_OBJ_ARRAY() macro that takes care of initializing the stride to
+ * the object size.
+ */
+struct drm_panthor_obj_array {
+	/** @stride: Stride of object struct. Used for versioning. */
+	__u32 stride;
+
+	/** @count: Number of objects in the array. */
+	__u32 count;
+
+	/** @array: User pointer to an array of objects. */
+	__u64 array;
+};
+
+/**
+ * DRM_PANTHOR_OBJ_ARRAY() - Initialize a drm_panthor_obj_array field.
+ * @cnt: Number of elements in the array.
+ * @ptr: Pointer to the array to pass to the kernel.
+ *
+ * Macro initializing a drm_panthor_obj_array based on the object size as known
+ * by userspace.
+ */
+#define DRM_PANTHOR_OBJ_ARRAY(cnt, ptr) \
+	{ .stride = sizeof((ptr)[0]), .count = (cnt), .array = (__u64)(uintptr_t)(ptr) }
+
+/**
+ * enum drm_panthor_sync_op_flags - Synchronization operation flags.
+ */
+enum drm_panthor_sync_op_flags {
+	/** @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK: Synchronization handle type mask. */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK = 0xff,
+
+	/** @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ: Synchronization object type. */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ = 0,
+
+	/**
+	 * @DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ: Timeline synchronization
+	 * object type.
+	 */
+	DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ = 1,
+
+	/** @DRM_PANTHOR_SYNC_OP_WAIT: Wait operation. */
+	DRM_PANTHOR_SYNC_OP_WAIT = 0 << 31,
+
+	/** @DRM_PANTHOR_SYNC_OP_SIGNAL: Signal operation. */
+	DRM_PANTHOR_SYNC_OP_SIGNAL = 1 << 31,
+};
+
+/**
+ * struct drm_panthor_sync_op - Synchronization operation.
+ */
+struct drm_panthor_sync_op {
+	/** @flags: Synchronization operation flags. Combination of DRM_PANTHOR_SYNC_OP values. */
+	__u32 flags;
+
+	/** @handle: Sync handle. */
+	__u32 handle;
+
+	/**
+	 * @timeline_value: MBZ if
+	 * (flags & DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_MASK) !=
+	 * DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ.
+	 */
+	__u64 timeline_value;
+};
+
+/**
+ * enum drm_panthor_dev_query_type - Query type
+ *
+ * Place new types at the end, don't re-oder, don't remove or replace.
+ */
+enum drm_panthor_dev_query_type {
+	/** @DRM_PANTHOR_DEV_QUERY_GPU_INFO: Query GPU information. */
+	DRM_PANTHOR_DEV_QUERY_GPU_INFO = 0,
+
+	/** @DRM_PANTHOR_DEV_QUERY_CSIF_INFO: Query command-stream interface information. */
+	DRM_PANTHOR_DEV_QUERY_CSIF_INFO,
+};
+
+/**
+ * struct drm_panthor_gpu_info - GPU information
+ *
+ * Structure grouping all queryable information relating to the GPU.
+ */
+struct drm_panthor_gpu_info {
+	/** @gpu_id : GPU ID. */
+	__u32 gpu_id;
+#define DRM_PANTHOR_ARCH_MAJOR(x)		((x) >> 28)
+#define DRM_PANTHOR_ARCH_MINOR(x)		(((x) >> 24) & 0xf)
+#define DRM_PANTHOR_ARCH_REV(x)			(((x) >> 20) & 0xf)
+#define DRM_PANTHOR_PRODUCT_MAJOR(x)		(((x) >> 16) & 0xf)
+#define DRM_PANTHOR_VERSION_MAJOR(x)		(((x) >> 12) & 0xf)
+#define DRM_PANTHOR_VERSION_MINOR(x)		(((x) >> 4) & 0xff)
+#define DRM_PANTHOR_VERSION_STATUS(x)		((x) & 0xf)
+
+	/** @gpu_rev: GPU revision. */
+	__u32 gpu_rev;
+
+	/** @csf_id: Command stream frontend ID. */
+	__u32 csf_id;
+#define DRM_PANTHOR_CSHW_MAJOR(x)		(((x) >> 26) & 0x3f)
+#define DRM_PANTHOR_CSHW_MINOR(x)		(((x) >> 20) & 0x3f)
+#define DRM_PANTHOR_CSHW_REV(x)			(((x) >> 16) & 0xf)
+#define DRM_PANTHOR_MCU_MAJOR(x)		(((x) >> 10) & 0x3f)
+#define DRM_PANTHOR_MCU_MINOR(x)		(((x) >> 4) & 0x3f)
+#define DRM_PANTHOR_MCU_REV(x)			((x) & 0xf)
+
+	/** @l2_features: L2-cache features. */
+	__u32 l2_features;
+
+	/** @tiler_features: Tiler features. */
+	__u32 tiler_features;
+
+	/** @mem_features: Memory features. */
+	__u32 mem_features;
+
+	/** @mmu_features: MMU features. */
+	__u32 mmu_features;
+#define DRM_PANTHOR_MMU_VA_BITS(x)		((x) & 0xff)
+
+	/** @thread_features: Thread features. */
+	__u32 thread_features;
+
+	/** @max_threads: Maximum number of threads. */
+	__u32 max_threads;
+
+	/** @thread_max_workgroup_size: Maximum workgroup size. */
+	__u32 thread_max_workgroup_size;
+
+	/**
+	 * @thread_max_barrier_size: Maximum number of threads that can wait
+	 * simultaneously on a barrier.
+	 */
+	__u32 thread_max_barrier_size;
+
+	/** @coherency_features: Coherency features. */
+	__u32 coherency_features;
+
+	/** @texture_features: Texture features. */
+	__u32 texture_features[4];
+
+	/** @as_present: Bitmask encoding the number of address-space exposed by the MMU. */
+	__u32 as_present;
+
+	/** @core_group_count: Number of core groups. */
+	__u32 core_group_count;
+
+	/** @pad: Zero on return. */
+	__u32 pad;
+
+	/** @shader_present: Bitmask encoding the shader cores exposed by the GPU. */
+	__u64 shader_present;
+
+	/** @l2_present: Bitmask encoding the L2 caches exposed by the GPU. */
+	__u64 l2_present;
+
+	/** @tiler_present: Bitmask encoding the tiler unit exposed by the GPU. */
+	__u64 tiler_present;
+};
+
+/**
+ * struct drm_panthor_csif_info - Command stream interface information
+ *
+ * Structure grouping all queryable information relating to the command stream interface.
+ */
+struct drm_panthor_csif_info {
+	/** @csg_slot_count: Number of command stream group slots exposed by the firmware. */
+	__u32 csg_slot_count;
+
+	/** @cs_slot_count: Number of command stream slot per group. */
+	__u32 cs_slot_count;
+
+	/** @cs_reg_count: Number of command stream register. */
+	__u32 cs_reg_count;
+
+	/** @scoreboard_slot_count: Number of scoreboard slot. */
+	__u32 scoreboard_slot_count;
+
+	/**
+	 * @unpreserved_cs_reg_count: Number of command stream registers reserved by
+	 * the kernel driver to call a userspace command stream.
+	 *
+	 * All registers can be used by a userspace command stream, but the
+	 * [cs_slot_count - unpreserved_cs_reg_count .. cs_slot_count] registers are
+	 * used by the kernel when DRM_PANTHOR_IOCTL_GROUP_SUBMIT is called.
+	 */
+	__u32 unpreserved_cs_reg_count;
+
+	/**
+	 * @pad: Padding field, set to zero.
+	 */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_dev_query - Arguments passed to DRM_PANTHOR_IOCTL_DEV_QUERY
+ */
+struct drm_panthor_dev_query {
+	/** @type: the query type (see drm_panthor_dev_query_type). */
+	__u32 type;
+
+	/**
+	 * @size: size of the type being queried.
+	 *
+	 * If pointer is NULL, size is updated by the driver to provide the
+	 * output structure size. If pointer is not NULL, the driver will
+	 * only copy min(size, actual_structure_size) bytes to the pointer,
+	 * and update the size accordingly. This allows us to extend query
+	 * types without breaking userspace.
+	 */
+	__u32 size;
+
+	/**
+	 * @pointer: user pointer to a query type struct.
+	 *
+	 * Pointer can be NULL, in which case, nothing is copied, but the
+	 * actual structure size is returned. If not NULL, it must point to
+	 * a location that's large enough to hold size bytes.
+	 */
+	__u64 pointer;
+};
+
+/**
+ * struct drm_panthor_vm_create - Arguments passed to DRM_PANTHOR_IOCTL_VM_CREATE
+ */
+struct drm_panthor_vm_create {
+	/** @flags: VM flags, MBZ. */
+	__u32 flags;
+
+	/** @id: Returned VM ID. */
+	__u32 id;
+
+	/**
+	 * @kernel_va_range: Size of the VA space reserved for kernel objects.
+	 *
+	 * If kernel_va_range is zero, we pick half of the VA space for kernel objects.
+	 *
+	 * Kernel VA space is always placed at the top of the supported VA range.
+	 */
+	__u64 kernel_va_range;
+};
+
+/**
+ * struct drm_panthor_vm_destroy - Arguments passed to DRM_PANTHOR_IOCTL_VM_DESTROY
+ */
+struct drm_panthor_vm_destroy {
+	/** @id: ID of the VM to destroy. */
+	__u32 id;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+};
+
+/**
+ * enum drm_panthor_vm_bind_op_flags - VM bind operation flags
+ */
+enum drm_panthor_vm_bind_op_flags {
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_READONLY: Map the memory read-only.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_READONLY = 1 << 0,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC: Map the memory not-executable.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC = 1 << 1,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED: Map the memory uncached.
+	 *
+	 * Only valid with DRM_PANTHOR_VM_BIND_OP_TYPE_MAP.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED = 1 << 2,
+
+	/**
+	 * @DRM_PANTHOR_VM_BIND_OP_TYPE_MASK: Mask used to determine the type of operation.
+	 */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_MASK = 0xf << 28,
+
+	/** @DRM_PANTHOR_VM_BIND_OP_TYPE_MAP: Map operation. */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_MAP = 0 << 28,
+
+	/** @DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP: Unmap operation. */
+	DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP = 1 << 28,
+};
+
+/**
+ * struct drm_panthor_vm_bind_op - VM bind operation
+ */
+struct drm_panthor_vm_bind_op {
+	/** @flags: Combination of drm_panthor_vm_bind_op_flags flags. */
+	__u32 flags;
+
+	/**
+	 * @bo_handle: Handle of the buffer object to map.
+	 * MBZ for unmap operations.
+	 */
+	__u32 bo_handle;
+
+	/**
+	 * @bo_offset: Buffer object offset.
+	 * MBZ for unmap operations.
+	 */
+	__u64 bo_offset;
+
+	/**
+	 * @va: Virtual address to map/unmap.
+	 */
+	__u64 va;
+
+	/** @size: Size to map/unmap. */
+	__u64 size;
+
+	/**
+	 * @syncs: Array of synchronization operations.
+	 *
+	 * This array must be empty if %DRM_PANTHOR_VM_BIND_ASYNC is not set on
+	 * the drm_panthor_vm_bind object containing this VM bind operation.
+	 */
+	struct drm_panthor_obj_array syncs;
+
+};
+
+/**
+ * enum drm_panthor_vm_bind_flags - VM bind flags
+ */
+enum drm_panthor_vm_bind_flags {
+	/**
+	 * @DRM_PANTHOR_VM_BIND_ASYNC: VM bind operations are queued to the VM
+	 * queue instead of being executed synchronously.
+	 */
+	DRM_PANTHOR_VM_BIND_ASYNC = 1 << 0,
+};
+
+/**
+ * struct drm_panthor_vm_bind - Arguments passed to DRM_IOCTL_PANTHOR_VM_BIND
+ */
+struct drm_panthor_vm_bind {
+	/** @vm_id: VM targeted by the bind request. */
+	__u32 vm_id;
+
+	/** @flags: Combination of drm_panthor_vm_bind_flags flags. */
+	__u32 flags;
+
+	/** @ops: Array of bind operations. */
+	struct drm_panthor_obj_array ops;
+};
+
+/**
+ * enum drm_panthor_bo_flags - Buffer object flags, passed at creation time.
+ */
+enum drm_panthor_bo_flags {
+	/** @DRM_PANTHOR_BO_NO_MMAP: The buffer object will never be CPU-mapped in userspace. */
+	DRM_PANTHOR_BO_NO_MMAP = (1 << 0),
+};
+
+/**
+ * struct drm_panthor_bo_create - Arguments passed to DRM_IOCTL_PANTHOR_BO_CREATE.
+ */
+struct drm_panthor_bo_create {
+	/**
+	 * @size: Requested size for the object
+	 *
+	 * The (page-aligned) allocated size for the object will be returned.
+	 */
+	__u64 size;
+
+	/**
+	 * @flags: Flags. Must be a combination of drm_panthor_bo_flags flags.
+	 */
+	__u32 flags;
+
+	/**
+	 * @exclusive_vm_id: Exclusive VM this buffer object will be mapped to.
+	 *
+	 * If not zero, the field must refer to a valid VM ID, and implies that:
+	 *  - the buffer object will only ever be bound to that VM
+	 *  - cannot be exported as a PRIME fd
+	 */
+	__u32 exclusive_vm_id;
+
+	/**
+	 * @handle: Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_bo_mmap_offset - Arguments passed to DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET.
+ */
+struct drm_panthor_bo_mmap_offset {
+	/** @handle: Handle of the object we want an mmap offset for. */
+	__u32 handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @offset: The fake offset to use for subsequent mmap calls. */
+	__u64 offset;
+};
+
+/**
+ * struct drm_panthor_queue_create - Queue creation arguments.
+ */
+struct drm_panthor_queue_create {
+	/**
+	 * @priority: Defines the priority of queues inside a group. Goes from 0 to 15,
+	 * 15 being the highest priority.
+	 */
+	__u8 priority;
+
+	/** @pad: Padding fields, MBZ. */
+	__u8 pad[3];
+
+	/** @ringbuf_size: Size of the ring buffer to allocate to this queue. */
+	__u32 ringbuf_size;
+};
+
+/**
+ * enum drm_panthor_group_priority - Scheduling group priority
+ */
+enum drm_panthor_group_priority {
+	/** @PANTHOR_GROUP_PRIORITY_LOW: Low priority group. */
+	PANTHOR_GROUP_PRIORITY_LOW = 0,
+
+	/** @PANTHOR_GROUP_PRIORITY_MEDIUM: Medium priority group. */
+	PANTHOR_GROUP_PRIORITY_MEDIUM,
+
+	/** @PANTHOR_GROUP_PRIORITY_HIGH: High priority group. */
+	PANTHOR_GROUP_PRIORITY_HIGH,
+};
+
+/**
+ * struct drm_panthor_group_create - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_CREATE
+ */
+struct drm_panthor_group_create {
+	/** @queues: Array of drm_panthor_create_cs_queue elements. */
+	struct drm_panthor_obj_array queues;
+
+	/**
+	 * @max_compute_cores: Maximum number of cores that can be used by compute
+	 * jobs across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @compute_core_mask.
+	 */
+	__u8 max_compute_cores;
+
+	/**
+	 * @max_fragment_cores: Maximum number of cores that can be used by fragment
+	 * jobs across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @fragment_core_mask.
+	 */
+	__u8 max_fragment_cores;
+
+	/**
+	 * @max_tiler_cores: Maximum number of tilers that can be used by tiler jobs
+	 * across CS queues bound to this group.
+	 *
+	 * Must be less or equal to the number of bits set in @tiler_core_mask.
+	 */
+	__u8 max_tiler_cores;
+
+	/** @priority: Group priority (see drm_drm_panthor_cs_group_priority). */
+	__u8 priority;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+
+	/**
+	 * @compute_core_mask: Mask encoding cores that can be used for compute jobs.
+	 *
+	 * This field must have at least @max_compute_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::shader_present.
+	 */
+	__u64 compute_core_mask;
+
+	/**
+	 * @fragment_core_mask: Mask encoding cores that can be used for fragment jobs.
+	 *
+	 * This field must have at least @max_fragment_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::shader_present.
+	 */
+	__u64 fragment_core_mask;
+
+	/**
+	 * @tiler_core_mask: Mask encoding cores that can be used for tiler jobs.
+	 *
+	 * This field must have at least @max_tiler_cores bits set.
+	 *
+	 * The bits set here should also be set in drm_panthor_gpu_info::tiler_present.
+	 */
+	__u64 tiler_core_mask;
+
+	/**
+	 * @vm_id: VM ID to bind this group to.
+	 *
+	 * All submission to queues bound to this group will use this VM.
+	 */
+	__u32 vm_id;
+
+	/**
+	 * @group_handle: Returned group handle. Passed back when submitting jobs or
+	 * destroying a group.
+	 */
+	__u32 group_handle;
+};
+
+/**
+ * struct drm_panthor_group_destroy - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_DESTROY
+ */
+struct drm_panthor_group_destroy {
+	/** @group_handle: Group to destroy */
+	__u32 group_handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_queue_submit - Job submission arguments.
+ *
+ * This is describing the userspace command stream to call from the kernel
+ * command stream ring-buffer. Queue submission is always part of a group
+ * submission, taking one or more jobs to submit to the underlying queues.
+ */
+struct drm_panthor_queue_submit {
+	/** @queue_index: Index of the queue inside a group. */
+	__u32 queue_index;
+
+	/**
+	 * @stream_size: Size of the command stream to execute.
+	 *
+	 * Must be 64-bit/8-byte aligned (the size of a CS instruction)
+	 *
+	 * Can be zero if stream_addr is zero too.
+	 */
+	__u32 stream_size;
+
+	/**
+	 * @stream_addr: GPU address of the command stream to execute.
+	 *
+	 * Must be aligned on 64-byte.
+	 *
+	 * Can be zero is stream_size is zero too.
+	 */
+	__u64 stream_addr;
+
+	/**
+	 * @latest_flush: FLUSH_ID read at the time the stream was built.
+	 *
+	 * This allows cache flush elimination for the automatic
+	 * flush+invalidate(all) done at submission time, which is needed to
+	 * ensure the GPU doesn't get garbage when reading the indirect command
+	 * stream buffers. If you want the cache flush to happen
+	 * unconditionally, pass a zero here.
+	 */
+	__u32 latest_flush;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @syncs: Array of sync operations. */
+	struct drm_panthor_obj_array syncs;
+};
+
+/**
+ * struct drm_panthor_group_submit - Arguments passed to DRM_IOCTL_PANTHOR_VM_BIND
+ */
+struct drm_panthor_group_submit {
+	/** @group_handle: Handle of the group to queue jobs to. */
+	__u32 group_handle;
+
+	/** @pad: MBZ. */
+	__u32 pad;
+
+	/** @queue_submits: Array of drm_panthor_queue_submit objects. */
+	struct drm_panthor_obj_array queue_submits;
+};
+
+/**
+ * enum drm_panthor_group_state_flags - Group state flags
+ */
+enum drm_panthor_group_state_flags {
+	/**
+	 * @DRM_PANTHOR_GROUP_STATE_TIMEDOUT: Group had unfinished jobs.
+	 *
+	 * When a group ends up with this flag set, no jobs can be submitted to its queues.
+	 */
+	DRM_PANTHOR_GROUP_STATE_TIMEDOUT = 1 << 0,
+
+	/**
+	 * @DRM_PANTHOR_GROUP_STATE_FATAL_FAULT: Group had fatal faults.
+	 *
+	 * When a group ends up with this flag set, no jobs can be submitted to its queues.
+	 */
+	DRM_PANTHOR_GROUP_STATE_FATAL_FAULT = 1 << 1,
+};
+
+/**
+ * struct drm_panthor_group_get_state - Arguments passed to DRM_IOCTL_PANTHOR_GROUP_GET_STATE
+ *
+ * Used to query the state of a group and decide whether a new group should be created to
+ * replace it.
+ */
+struct drm_panthor_group_get_state {
+	/** @group_handle: Handle of the group to query state on */
+	__u32 group_handle;
+
+	/**
+	 * @state: Combination of DRM_PANTHOR_GROUP_STATE_* flags encoding the
+	 * group state.
+	 */
+	__u32 state;
+
+	/** @fatal_queues: Bitmask of queues that faced fatal faults. */
+	__u32 fatal_queues;
+
+	/** @pad: MBZ */
+	__u32 pad;
+};
+
+/**
+ * struct drm_panthor_tiler_heap_create - Arguments passed to DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE
+ */
+struct drm_panthor_tiler_heap_create {
+	/** @vm_id: VM ID the tiler heap should be mapped to */
+	__u32 vm_id;
+
+	/** @initial_chunk_count: Initial number of chunks to allocate. */
+	__u32 initial_chunk_count;
+
+	/** @chunk_size: Chunk size. Must be a power of two at least 256KB large. */
+	__u32 chunk_size;
+
+	/** @max_chunks: Maximum number of chunks that can be allocated. */
+	__u32 max_chunks;
+
+	/**
+	 * @target_in_flight: Maximum number of in-flight render passes.
+	 *
+	 * If the heap has more than tiler jobs in-flight, the FW will wait for render
+	 * passes to finish before queuing new tiler jobs.
+	 */
+	__u32 target_in_flight;
+
+	/** @handle: Returned heap handle. Passed back to DESTROY_TILER_HEAP. */
+	__u32 handle;
+
+	/** @tiler_heap_ctx_gpu_va: Returned heap GPU virtual address returned */
+	__u64 tiler_heap_ctx_gpu_va;
+
+	/**
+	 * @first_heap_chunk_gpu_va: First heap chunk.
+	 *
+	 * The tiler heap is formed of heap chunks forming a single-link list. This
+	 * is the first element in the list.
+	 */
+	__u64 first_heap_chunk_gpu_va;
+};
+
+/**
+ * struct drm_panthor_tiler_heap_destroy - Arguments passed to DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY
+ */
+struct drm_panthor_tiler_heap_destroy {
+	/** @handle: Handle of the tiler heap to destroy */
+	__u32 handle;
+
+	/** @pad: Padding field, MBZ. */
+	__u32 pad;
+};
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* _PANTHOR_DRM_H_ */
diff -up mesa-23.3.0-rc5/src/.clang-format.8~ mesa-23.3.0-rc5/src/.clang-format
--- mesa-23.3.0-rc5/src/.clang-format.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/.clang-format	2023-11-24 23:33:24.959610812 +0100
@@ -312,6 +312,7 @@ ForEachMacros:
   - pan_foreach_instr_in_block_rev
   - pan_foreach_predecessor
   - pan_foreach_successor
+  - ceu_emit
 
 # Disable clang formatting by default. Drivers that use clang-format
 # inherit from this .clang-format file and re-enable formatting:
diff -up mesa-23.3.0-rc5/src/egl/drivers/dri2/platform_drm.c.8~ mesa-23.3.0-rc5/src/egl/drivers/dri2/platform_drm.c
--- mesa-23.3.0-rc5/src/egl/drivers/dri2/platform_drm.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/egl/drivers/dri2/platform_drm.c	2023-11-24 23:33:24.960610818 +0100
@@ -562,20 +562,6 @@ static const struct dri2_egl_display_vtb
    .get_dri_drawable = dri2_surface_get_dri_drawable,
 };
 
-static int
-get_fd_render_gpu_drm(struct gbm_dri_device *gbm_dri, int fd_display_gpu)
-{
-   /* This doesn't make sense for the software case. */
-   assert(!gbm_dri->software);
-
-   /* Render-capable device, so just return the same fd. */
-   if (loader_is_device_render_capable(fd_display_gpu))
-      return fd_display_gpu;
-
-   /* Display-only device, so return a compatible render-only device. */
-   return gbm_dri->mesa->queryCompatibleRenderOnlyDeviceFd(fd_display_gpu);
-}
-
 EGLBoolean
 dri2_initialize_drm(_EGLDisplay *disp)
 {
@@ -602,37 +588,30 @@ dri2_initialize_drm(_EGLDisplay *disp)
             goto cleanup;
          }
 
-         dri2_dpy->fd_display_gpu =
+         dri2_dpy->fd_render_gpu =
             loader_open_device(drm->nodes[DRM_NODE_PRIMARY]);
       } else {
          char buf[64];
          int n = snprintf(buf, sizeof(buf), DRM_DEV_NAME, DRM_DIR_NAME, 0);
          if (n != -1 && n < sizeof(buf))
-            dri2_dpy->fd_display_gpu = loader_open_device(buf);
+            dri2_dpy->fd_render_gpu = loader_open_device(buf);
       }
 
-      gbm = gbm_create_device(dri2_dpy->fd_display_gpu);
+      gbm = gbm_create_device(dri2_dpy->fd_render_gpu);
       if (gbm == NULL) {
          err = "DRI2: failed to create gbm device";
          goto cleanup;
       }
       dri2_dpy->own_device = true;
    } else {
-      dri2_dpy->fd_display_gpu = os_dupfd_cloexec(gbm_device_get_fd(gbm));
-      if (dri2_dpy->fd_display_gpu < 0) {
+      dri2_dpy->fd_render_gpu = os_dupfd_cloexec(gbm_device_get_fd(gbm));
+      if (dri2_dpy->fd_render_gpu < 0) {
          err = "DRI2: failed to fcntl() existing gbm device";
          goto cleanup;
       }
    }
+   dri2_dpy->fd_display_gpu = dri2_dpy->fd_render_gpu;
    dri2_dpy->gbm_dri = gbm_dri_device(gbm);
-   if (!dri2_dpy->gbm_dri->software) {
-      dri2_dpy->fd_render_gpu =
-         get_fd_render_gpu_drm(dri2_dpy->gbm_dri, dri2_dpy->fd_display_gpu);
-      if (dri2_dpy->fd_render_gpu < 0) {
-         err = "DRI2: failed to get compatible render device";
-         goto cleanup;
-      }
-   }
 
    if (strcmp(gbm_device_get_backend_name(gbm), "drm") != 0) {
       err = "DRI2: gbm device using incorrect/incompatible backend";
diff -up mesa-23.3.0-rc5/src/egl/main/egldevice.c.8~ mesa-23.3.0-rc5/src/egl/main/egldevice.c
--- mesa-23.3.0-rc5/src/egl/main/egldevice.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/egl/main/egldevice.c	2023-11-24 23:33:24.960610818 +0100
@@ -153,9 +153,6 @@ _eglAddDRMDevice(drmDevicePtr device)
 
 /* Finds a device in DeviceList, for the given fd.
  *
- * The fd must be of a render-capable device, as there are only render-capable
- * devices in DeviceList.
- *
  * If a software device, the fd is ignored.
  */
 _EGLDevice *
@@ -185,16 +182,10 @@ _eglFindDevice(int fd, bool software)
 
       if (_eglDeviceSupports(dev, _EGL_DEVICE_DRM) &&
           drmDevicesEqual(device, dev->device) != 0) {
-         goto cleanup_drm;
+         goto out;
       }
    }
 
-   /* Couldn't find an EGLDevice for the device. */
-   dev = NULL;
-
-cleanup_drm:
-   drmFreeDevice(&device);
-
 #else
    _eglLog(_EGL_FATAL,
            "Driver bug: Built without libdrm, yet looking for HW device");
diff -up mesa-23.3.0-rc5/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c.8~ mesa-23.3.0-rc5/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c
--- mesa-23.3.0-rc5/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/auxiliary/pipe-loader/pipe_loader_drm.c	2023-11-24 23:33:24.960610818 +0100
@@ -82,6 +82,7 @@ static const struct drm_driver_descripto
    &v3d_driver_descriptor,
    &vc4_driver_descriptor,
    &panfrost_driver_descriptor,
+   &panthor_driver_descriptor,
    &asahi_driver_descriptor,
    &etnaviv_driver_descriptor,
    &tegra_driver_descriptor,
diff -up mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper.h.8~ mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper.h
--- mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper.h	2023-11-24 23:33:24.960610818 +0100
@@ -335,9 +335,11 @@ pipe_panfrost_create_screen(int fd, cons
    return screen ? debug_screen_wrap(screen) : NULL;
 }
 DRM_DRIVER_DESCRIPTOR(panfrost, NULL, 0)
+DRM_DRIVER_DESCRIPTOR_ALIAS(panfrost, panthor, NULL, 0)
 
 #else
 DRM_DRIVER_DESCRIPTOR_STUB(panfrost)
+DRM_DRIVER_DESCRIPTOR_STUB(panthor)
 #endif
 
 #ifdef GALLIUM_ASAHI
diff -up mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper_public.h.8~ mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper_public.h
--- mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper_public.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/auxiliary/target-helpers/drm_helper_public.h	2023-11-24 23:33:24.960610818 +0100
@@ -18,6 +18,7 @@ extern const struct drm_driver_descripto
 extern const struct drm_driver_descriptor v3d_driver_descriptor;
 extern const struct drm_driver_descriptor vc4_driver_descriptor;
 extern const struct drm_driver_descriptor panfrost_driver_descriptor;
+extern const struct drm_driver_descriptor panthor_driver_descriptor;
 extern const struct drm_driver_descriptor asahi_driver_descriptor;
 extern const struct drm_driver_descriptor etnaviv_driver_descriptor;
 extern const struct drm_driver_descriptor tegra_driver_descriptor;
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/meson.build.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/meson.build
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/meson.build	2023-11-24 23:33:24.960610818 +0100
@@ -20,7 +20,6 @@
 # SOFTWARE.
 
 files_panfrost = files(
-  'pan_afbc_cso.c',
   'pan_disk_cache.c',
   'pan_fence.c',
   'pan_helpers.c',
@@ -54,13 +53,18 @@ compile_args_panfrost = [
   '-Wno-pointer-arith'
 ]
 
-panfrost_versions = ['4', '5', '6', '7', '9']
+panfrost_versions = ['4', '5', '6', '7', '9', '10']
 libpanfrost_versions = []
 
 foreach ver : panfrost_versions
+  files_panfrost_vx = ['pan_cmdstream.c', pan_packers]
+  if ver in ['4', '5', '6', '7', '9']
+    files_panfrost_vx += ['pan_jm.c']
+  elif ver in ['10']
+    files_panfrost_vx += ['pan_csf.c']
+  endif
   libpanfrost_versions += static_library(
-    'panfrost-v' + ver,
-    ['pan_cmdstream.c', pan_packers],
+    'panfrost-v' + ver, files_panfrost_vx,
     include_directories : panfrost_includes,
     c_args : ['-DPAN_ARCH=' + ver],
     gnu_symbol_visibility : 'hidden',
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_afbc_cso.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_afbc_cso.c
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_afbc_cso.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_afbc_cso.h
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_blit.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_blit.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_blit.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_blit.c	2023-11-24 23:33:24.960610818 +0100
@@ -90,10 +90,6 @@ panfrost_blit(struct pipe_context *pipe,
    if (!util_blitter_is_blit_supported(ctx->blitter, info))
       unreachable("Unsupported blit\n");
 
-   /* Legalize here because it could trigger a recursive blit otherwise */
-   pan_legalize_afbc_format(ctx, pan_resource(info->dst.resource),
-                            info->dst.format, true);
-
    panfrost_blitter_save(ctx, info->render_condition_enable
                                  ? PAN_RENDER_BLIT_COND
                                  : PAN_RENDER_BLIT);
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.c	2023-11-24 23:33:24.961610824 +0100
@@ -1,5 +1,4 @@
 /*
- * Copyright (C) 2023 Amazon.com, Inc. or its affiliates.
  * Copyright (C) 2018 Alyssa Rosenzweig
  * Copyright (C) 2020 Collabora Ltd.
  * Copyright Â© 2017 Intel Corporation
@@ -36,56 +35,34 @@
 #include "util/u_vbuf.h"
 #include "util/u_viewport.h"
 
+#include "genxml/ceu_builder.h"
 #include "genxml/gen_macros.h"
 
-#include "pan_afbc_cso.h"
+#include "decode.h"
+
 #include "pan_blend.h"
 #include "pan_blitter.h"
 #include "pan_bo.h"
+#include "pan_cmdstream.h"
 #include "pan_context.h"
+#include "pan_csf.h"
 #include "pan_indirect_dispatch.h"
+#include "pan_jm.h"
 #include "pan_job.h"
 #include "pan_pool.h"
 #include "pan_shader.h"
 #include "pan_texture.h"
 #include "pan_util.h"
 
-#define PAN_GPU_INDIRECTS (PAN_ARCH == 7)
-
-struct panfrost_rasterizer {
-   struct pipe_rasterizer_state base;
-
-#if PAN_ARCH <= 7
-   /* Partially packed RSD words */
-   struct mali_multisample_misc_packed multisample;
-   struct mali_stencil_mask_misc_packed stencil_misc;
-#endif
-};
-
-struct panfrost_zsa_state {
-   struct pipe_depth_stencil_alpha_state base;
-
-   /* Is any depth, stencil, or alpha testing enabled? */
-   bool enabled;
-
-   /* Does the depth and stencil tests always pass? This ignores write
-    * masks, we are only interested in whether pixels may be killed.
-    */
-   bool zs_always_passes;
+#define PAN_USE_CSF (PAN_ARCH >= 10)
 
-   /* Are depth or stencil writes possible? */
-   bool writes_zs;
-
-#if PAN_ARCH <= 7
-   /* Prepacked words from the RSD */
-   struct mali_multisample_misc_packed rsd_depth;
-   struct mali_stencil_mask_misc_packed rsd_stencil;
-   struct mali_stencil_packed stencil_front, stencil_back;
-#else
-   /* Depth/stencil descriptor template */
-   struct mali_depth_stencil_packed desc;
-#endif
-};
+/* JOBX() is used to select the job backend helpers to call from generic
+ * functions. */
+#if PAN_USE_CSF
+#define JOBX(__suffix) GENX(csf_##__suffix)
+#else /* PAN_USE_CSF */
+#define JOBX(__suffix) GENX(jm_##__suffix)
+#endif /* PAN_USE_CSF */
 
 struct panfrost_sampler_state {
    struct pipe_sampler_state base;
@@ -108,25 +85,6 @@ struct panfrost_sampler_view {
    struct panfrost_pool *pool;
 };
 
-struct panfrost_vertex_state {
-   unsigned num_elements;
-   struct pipe_vertex_element pipe[PIPE_MAX_ATTRIBS];
-   uint16_t strides[PIPE_MAX_ATTRIBS];
-
-#if PAN_ARCH >= 9
-   /* Packed attribute descriptors */
-   struct mali_attribute_packed attributes[PIPE_MAX_ATTRIBS];
-#else
-   /* buffers corresponds to attribute buffer, element_buffers corresponds
-    * to an index in buffers for each vertex element */
-   struct pan_vertex_buffer buffers[PIPE_MAX_ATTRIBS];
-   unsigned element_buffer[PIPE_MAX_ATTRIBS];
-   unsigned nr_bufs;
-
-   unsigned formats[PIPE_MAX_ATTRIBS];
-#endif
-};
-
 /* Statically assert that PIPE_* enums match the hardware enums.
  * (As long as they match, we don't need to translate them.)
  */
@@ -287,35 +245,6 @@ panfrost_create_sampler_state(struct pip
    return so;
 }
 
-static bool
-panfrost_fs_required(struct panfrost_compiled_shader *fs,
-                     struct panfrost_blend_state *blend,
-                     struct pipe_framebuffer_state *state,
-                     const struct panfrost_zsa_state *zsa)
-{
-   /* If we generally have side effects. This inclues use of discard,
-    * which can affect the results of an occlusion query. */
-   if (fs->info.fs.sidefx)
-      return true;
-
-   /* Using an empty FS requires early-z to be enabled, but alpha test
-    * needs it disabled. Alpha test is only native on Midgard, so only
-    * check there.
-    */
-   if (PAN_ARCH <= 5 && zsa->base.alpha_func != PIPE_FUNC_ALWAYS)
-      return true;
-
-   /* If colour is written we need to execute */
-   for (unsigned i = 0; i < state->nr_cbufs; ++i) {
-      if (state->cbufs[i] && blend->info[i].enabled)
-         return true;
-   }
-
-   /* If depth is written and not implied we need to execute.
-    * TODO: Predicate on Z/S writes being enabled */
-   return (fs->info.fs.writes_depth || fs->info.fs.writes_stencil);
-}
-
 /* Get pointers to the blend shaders bound to each active render target. Used
  * to emit the blend descriptors, as well as the fragment renderer state
  * descriptor.
@@ -354,34 +283,6 @@ pack_blend_constant(enum pipe_format for
    return unorm << (16 - chan_size);
 }
 
-/*
- * Determine whether to set the respective overdraw alpha flag.
- *
- * The overdraw alpha=1 flag should be set when alpha=1 implies full overdraw,
- * equivalently, all enabled render targets have alpha_one_store set. Likewise,
- * overdraw alpha=0 should be set when alpha=0 implies no overdraw,
- * equivalently, all enabled render targets have alpha_zero_nop set.
- */
-#if PAN_ARCH >= 6
-static bool
-panfrost_overdraw_alpha(const struct panfrost_context *ctx, bool zero)
-{
-   const struct panfrost_blend_state *so = ctx->blend;
-
-   for (unsigned i = 0; i < ctx->pipe_framebuffer.nr_cbufs; ++i) {
-      const struct pan_blend_info info = so->info[i];
-
-      bool enabled = ctx->pipe_framebuffer.cbufs[i] && !info.enabled;
-      bool flag = zero ? info.alpha_zero_nop : info.alpha_one_store;
-
-      if (enabled && !flag)
-         return false;
-   }
-
-   return true;
-}
-#endif
-
 static void
 panfrost_emit_blend(struct panfrost_batch *batch, void *rts,
                     mali_ptr *blend_shaders)
@@ -495,23 +396,6 @@ panfrost_emit_blend(struct panfrost_batc
 }
 #endif
 
-static inline bool
-pan_allow_forward_pixel_to_kill(struct panfrost_context *ctx,
-                                struct panfrost_compiled_shader *fs)
-{
-   /* Track if any colour buffer is reused across draws, either
-    * from reading it directly, or from failing to write it
-    */
-   unsigned rt_mask = ctx->fb_rt_mask;
-   uint64_t rt_written = (fs->info.outputs_written >> FRAG_RESULT_DATA0) &
-                         ctx->blend->enabled_mask;
-   bool blend_reads_dest = (ctx->blend->load_dest_mask & rt_mask);
-   bool alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
-
-   return fs->info.fs.can_fpk && !(rt_mask & ~rt_written) &&
-          !alpha_to_coverage && !blend_reads_dest;
-}
-
 static mali_ptr
 panfrost_emit_compute_shader_meta(struct panfrost_batch *batch,
                                   enum pipe_shader_type stage)
@@ -1888,7 +1772,7 @@ emit_image_bufs(struct panfrost_batch *b
          cfg.type = pan_modifier_to_attr_type(rsrc->image.layout.modifier);
          cfg.pointer = rsrc->image.data.bo->ptr.gpu + offset;
          cfg.stride = util_format_get_blocksize(image->format);
-         cfg.size = rsrc->image.data.bo->size - offset;
+         cfg.size = panfrost_bo_size(rsrc->image.data.bo) - offset;
       }
 
       if (is_buffer) {
@@ -2469,10 +2353,8 @@ pan_emit_special_input(struct mali_attri
 
 static void
 panfrost_emit_varying_descriptor(struct panfrost_batch *batch,
-                                 unsigned vertex_count, mali_ptr *vs_attribs,
-                                 mali_ptr *fs_attribs, mali_ptr *buffers,
-                                 unsigned *buffer_count, mali_ptr *position,
-                                 mali_ptr *psiz, bool point_coord_replace)
+                                 unsigned vertex_count,
+                                 bool point_coord_replace)
 {
    struct panfrost_context *ctx = batch->ctx;
    struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
@@ -2480,6 +2362,8 @@ panfrost_emit_varying_descriptor(struct
 
    uint16_t point_coord_mask = 0;
 
+   memset(&batch->varyings, 0, sizeof(batch->varyings));
+
 #if PAN_ARCH <= 5
    struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
 
@@ -2510,8 +2394,7 @@ panfrost_emit_varying_descriptor(struct
    struct mali_attribute_buffer_packed *varyings =
       (struct mali_attribute_buffer_packed *)T.cpu;
 
-   if (buffer_count)
-      *buffer_count = count;
+   batch->varyings.nr_bufs = count;
 
 #if PAN_ARCH >= 6
    /* Suppress prefetch on Bifrost */
@@ -2530,12 +2413,12 @@ panfrost_emit_varying_descriptor(struct
    }
 
    /* fp32 vec4 gl_Position */
-   *position = panfrost_emit_varyings(
+   batch->varyings.pos = panfrost_emit_varyings(
       batch, &varyings[pan_varying_index(present, PAN_VARY_POSITION)],
       sizeof(float) * 4, vertex_count);
 
    if (present & BITFIELD_BIT(PAN_VARY_PSIZ)) {
-      *psiz = panfrost_emit_varyings(
+      batch->varyings.psiz = panfrost_emit_varyings(
          batch, &varyings[pan_varying_index(present, PAN_VARY_PSIZ)], 2,
          vertex_count);
    }
@@ -2552,27 +2435,9 @@ panfrost_emit_varying_descriptor(struct
                           MALI_ATTRIBUTE_SPECIAL_FRAG_COORD);
 #endif
 
-   *buffers = T.gpu;
-   *vs_attribs = linkage->producer;
-   *fs_attribs = linkage->consumer;
-}
-
-/*
- * Emit jobs required for the rasterization pipeline. If there are side effects
- * from the vertex shader, these are handled ahead-of-time with a compute
- * shader. This function should not be called if rasterization is skipped.
- */
-static void
-panfrost_emit_vertex_tiler_jobs(struct panfrost_batch *batch,
-                                const struct panfrost_ptr *vertex_job,
-                                const struct panfrost_ptr *tiler_job)
-{
-   unsigned vertex = panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                                      MALI_JOB_TYPE_VERTEX, false, false, 0, 0,
-                                      vertex_job, false);
-
-   panfrost_add_job(&batch->pool.base, &batch->scoreboard, MALI_JOB_TYPE_TILER,
-                    false, false, vertex, 0, tiler_job, false);
+   batch->varyings.bufs = T.gpu;
+   batch->varyings.vs = linkage->producer;
+   batch->varyings.fs = linkage->consumer;
 }
 #endif
 
@@ -2638,7 +2503,7 @@ panfrost_initialize_surface(struct panfr
 /* Generate a fragment job. This should be called once per frame. (Usually,
  * this corresponds to eglSwapBuffers or one of glFlush, glFinish)
  */
-static mali_ptr
+static void
 emit_fragment_job(struct panfrost_batch *batch, const struct pan_fb_info *pfb)
 {
    /* Mark the affected buffers as initialized, since we're writing to it.
@@ -2671,42 +2536,9 @@ emit_fragment_job(struct panfrost_batch
    assert(batch->maxx > batch->minx);
    assert(batch->maxy > batch->miny);
 
-   struct panfrost_ptr transfer =
-      pan_pool_alloc_desc(&batch->pool.base, FRAGMENT_JOB);
-
-   GENX(pan_emit_fragment_job)(pfb, batch->framebuffer.gpu, transfer.cpu);
-
-   return transfer.gpu;
-}
-
-#define DEFINE_CASE(c)                                                         \
-   case MESA_PRIM_##c:                                                         \
-      return MALI_DRAW_MODE_##c;
-
-static uint8_t
-pan_draw_mode(enum mesa_prim mode)
-{
-   switch (mode) {
-      DEFINE_CASE(POINTS);
-      DEFINE_CASE(LINES);
-      DEFINE_CASE(LINE_LOOP);
-      DEFINE_CASE(LINE_STRIP);
-      DEFINE_CASE(TRIANGLES);
-      DEFINE_CASE(TRIANGLE_STRIP);
-      DEFINE_CASE(TRIANGLE_FAN);
-      DEFINE_CASE(QUADS);
-      DEFINE_CASE(POLYGON);
-#if PAN_ARCH <= 6
-      DEFINE_CASE(QUAD_STRIP);
-#endif
-
-   default:
-      unreachable("Invalid draw mode");
-   }
+   JOBX(emit_fragment_job)(batch, pfb);
 }
 
-#undef DEFINE_CASE
-
 /* Count generated primitives (when there is no geom/tess shaders) for
  * transform feedback */
 
@@ -2742,92 +2574,6 @@ panfrost_update_streamout_offsets(struct
    }
 }
 
-static inline enum mali_index_type
-panfrost_translate_index_size(unsigned size)
-{
-   STATIC_ASSERT(MALI_INDEX_TYPE_NONE == 0);
-   STATIC_ASSERT(MALI_INDEX_TYPE_UINT8 == 1);
-   STATIC_ASSERT(MALI_INDEX_TYPE_UINT16 == 2);
-
-   return (size == 4) ? MALI_INDEX_TYPE_UINT32 : size;
-}
-
-#if PAN_ARCH <= 7
-static inline void
-pan_emit_draw_descs(struct panfrost_batch *batch, struct MALI_DRAW *d,
-                    enum pipe_shader_type st)
-{
-   d->offset_start = batch->ctx->offset_start;
-   d->instance_size =
-      batch->ctx->instance_count > 1 ? batch->ctx->padded_count : 1;
-
-   d->uniform_buffers = batch->uniform_buffers[st];
-   d->push_uniforms = batch->push_uniforms[st];
-   d->textures = batch->textures[st];
-   d->samplers = batch->samplers[st];
-}
-
-static void
-panfrost_draw_emit_vertex_section(struct panfrost_batch *batch,
-                                  mali_ptr vs_vary, mali_ptr varyings,
-                                  mali_ptr attribs, mali_ptr attrib_bufs,
-                                  void *section)
-{
-   pan_pack(section, DRAW, cfg) {
-      cfg.state = batch->rsd[PIPE_SHADER_VERTEX];
-      cfg.attributes = attribs;
-      cfg.attribute_buffers = attrib_bufs;
-      cfg.varyings = vs_vary;
-      cfg.varying_buffers = vs_vary ? varyings : 0;
-      cfg.thread_storage = batch->tls.gpu;
-      pan_emit_draw_descs(batch, &cfg, PIPE_SHADER_VERTEX);
-   }
-}
-
-static void
-panfrost_draw_emit_vertex(struct panfrost_batch *batch,
-                          const struct pipe_draw_info *info,
-                          void *invocation_template, mali_ptr vs_vary,
-                          mali_ptr varyings, mali_ptr attribs,
-                          mali_ptr attrib_bufs, void *job)
-{
-   void *section = pan_section_ptr(job, COMPUTE_JOB, INVOCATION);
-   memcpy(section, invocation_template, pan_size(INVOCATION));
-
-   pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
-      cfg.job_task_split = 5;
-   }
-
-   section = pan_section_ptr(job, COMPUTE_JOB, DRAW);
-   panfrost_draw_emit_vertex_section(batch, vs_vary, varyings, attribs,
-                                     attrib_bufs, section);
-}
-#endif
-
-static void
-panfrost_emit_primitive_size(struct panfrost_context *ctx, bool points,
-                             mali_ptr size_array, void *prim_size)
-{
-   struct panfrost_rasterizer *rast = ctx->rasterizer;
-
-   pan_pack(prim_size, PRIMITIVE_SIZE, cfg) {
-      if (panfrost_writes_point_size(ctx)) {
-         cfg.size_array = size_array;
-      } else {
-         cfg.constant = points ? rast->base.point_size : rast->base.line_width;
-      }
-   }
-}
-
-static bool
-panfrost_is_implicit_prim_restart(const struct pipe_draw_info *info)
-{
-   /* As a reminder primitive_restart should always be checked before any
-      access to restart_index. */
-   return info->primitive_restart &&
-          info->restart_index == (unsigned)BITFIELD_MASK(info->index_size * 8);
-}
-
 /* On Bifrost and older, the Renderer State Descriptor aggregates many pieces of
  * 3D state. In particular, it groups the fragment shader descriptor with
  * depth/stencil, blend, polygon offset, and multisampling state. These pieces
@@ -2895,7 +2641,10 @@ panfrost_update_shader_state(struct panf
       batch->rsd[st] = panfrost_emit_frag_shader_meta(batch);
    }
 
-   if (frag && (dirty & PAN_DIRTY_STAGE_IMAGE)) {
+   /* Vertex shaders need to mix vertex data and image descriptors in the
+    * attribute array. This is taken care of in panfrost_update_state_3d().
+    */
+   if (st != PIPE_SHADER_VERTEX && (dirty & PAN_DIRTY_STAGE_IMAGE)) {
       batch->attribs[st] =
          panfrost_emit_image_attribs(batch, &batch->attrib_bufs[st], st);
    }
@@ -2931,468 +2680,26 @@ panfrost_update_state_3d(struct panfrost
       batch->attrib_bufs[PIPE_SHADER_VERTEX] =
          panfrost_emit_vertex_buffers(batch);
    }
-#endif
-}
-
-#if PAN_ARCH >= 6
-static mali_ptr
-panfrost_batch_get_bifrost_tiler(struct panfrost_batch *batch,
-                                 unsigned vertex_count)
-{
-   struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
-
-   if (!vertex_count)
-      return 0;
-
-   if (batch->tiler_ctx.bifrost)
-      return batch->tiler_ctx.bifrost;
-
-   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
-
-   GENX(pan_emit_tiler_heap)(dev, t.cpu);
-
-   mali_ptr heap = t.gpu;
-
-   t = pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
-   GENX(pan_emit_tiler_ctx)
-   (dev, batch->key.width, batch->key.height,
-    util_framebuffer_get_num_samples(&batch->key),
-    pan_tristate_get(batch->first_provoking_vertex), heap, t.cpu);
-
-   batch->tiler_ctx.bifrost = t.gpu;
-   return batch->tiler_ctx.bifrost;
-}
-#endif
-
-/* Packs a primitive descriptor, mostly common between Midgard/Bifrost tiler
- * jobs and Valhall IDVS jobs
- */
-static void
-panfrost_emit_primitive(struct panfrost_context *ctx,
-                        const struct pipe_draw_info *info,
-                        const struct pipe_draw_start_count_bias *draw,
-                        mali_ptr indices, bool secondary_shader, void *out)
-{
-   UNUSED struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-   bool lines =
-      (info->mode == MESA_PRIM_LINES || info->mode == MESA_PRIM_LINE_LOOP ||
-       info->mode == MESA_PRIM_LINE_STRIP);
-
-   pan_pack(out, PRIMITIVE, cfg) {
-      cfg.draw_mode = pan_draw_mode(info->mode);
-      if (panfrost_writes_point_size(ctx))
-         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
-
-#if PAN_ARCH <= 8
-      /* For line primitives, PRIMITIVE.first_provoking_vertex must
-       * be set to true and the provoking vertex is selected with
-       * DRAW.flat_shading_vertex.
-       */
-      if (lines)
-         cfg.first_provoking_vertex = true;
-      else
-         cfg.first_provoking_vertex = rast->flatshade_first;
-
-      if (panfrost_is_implicit_prim_restart(info)) {
-         cfg.primitive_restart = MALI_PRIMITIVE_RESTART_IMPLICIT;
-      } else if (info->primitive_restart) {
-         cfg.primitive_restart = MALI_PRIMITIVE_RESTART_EXPLICIT;
-         cfg.primitive_restart_index = info->restart_index;
-      }
-
-      cfg.job_task_split = 6;
 #else
-      struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
-
-      cfg.allow_rotating_primitives =
-         !(lines || fs->info.bifrost.uses_flat_shading);
-      cfg.primitive_restart = info->primitive_restart;
-
-      /* Non-fixed restart indices should have been lowered */
-      assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
-#endif
-
-      cfg.index_count = draw->count;
-      cfg.index_type = panfrost_translate_index_size(info->index_size);
+   unsigned vt_shader_dirty = ctx->dirty_shader[PIPE_SHADER_VERTEX];
 
-      if (PAN_ARCH >= 9) {
-         /* Base vertex offset on Valhall is used for both
-          * indexed and non-indexed draws, in a simple way for
-          * either. Handle both cases.
-          */
-         if (cfg.index_type)
-            cfg.base_vertex_offset = draw->index_bias;
-         else
-            cfg.base_vertex_offset = draw->start;
-
-         /* Indices are moved outside the primitive descriptor
-          * on Valhall, so we don't need to set that here
-          */
-      } else if (cfg.index_type) {
-         cfg.base_vertex_offset = draw->index_bias - ctx->offset_start;
-
-#if PAN_ARCH <= 7
-         cfg.indices = indices;
-#endif
-      }
-
-#if PAN_ARCH >= 6
-      cfg.secondary_shader = secondary_shader;
-#endif
-   }
-}
-
-#if PAN_ARCH >= 9
-static mali_ptr
-panfrost_emit_resources(struct panfrost_batch *batch,
-                        enum pipe_shader_type stage)
-{
-   struct panfrost_context *ctx = batch->ctx;
-   struct panfrost_ptr T;
-   unsigned nr_tables = 12;
-
-   /* Although individual resources need only 16 byte alignment, the
-    * resource table as a whole must be 64-byte aligned.
-    */
-   T = pan_pool_alloc_aligned(&batch->pool.base, nr_tables * pan_size(RESOURCE),
-                              64);
-   memset(T.cpu, 0, nr_tables * pan_size(RESOURCE));
-
-   panfrost_make_resource_table(T, PAN_TABLE_UBO, batch->uniform_buffers[stage],
-                                batch->nr_uniform_buffers[stage]);
-
-   panfrost_make_resource_table(T, PAN_TABLE_TEXTURE, batch->textures[stage],
-                                ctx->sampler_view_count[stage]);
-
-   /* We always need at least 1 sampler for txf to work */
-   panfrost_make_resource_table(T, PAN_TABLE_SAMPLER, batch->samplers[stage],
-                                MAX2(ctx->sampler_count[stage], 1));
-
-   panfrost_make_resource_table(T, PAN_TABLE_IMAGE, batch->images[stage],
-                                util_last_bit(ctx->image_mask[stage]));
-
-   if (stage == PIPE_SHADER_VERTEX) {
-      panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE,
-                                   batch->attribs[stage],
-                                   ctx->vertex->num_elements);
-
-      panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE_BUFFER,
-                                   batch->attrib_bufs[stage],
-                                   util_last_bit(ctx->vb_mask));
+   /* Vertex data, vertex shader and images accessed by the vertex shader have
+    * an impact on the attributes array, we need to re-emit anytime one of these
+    * parameters changes. */
+   if ((dirty & PAN_DIRTY_VERTEX) ||
+       (vt_shader_dirty & (PAN_DIRTY_STAGE_IMAGE | PAN_DIRTY_STAGE_SHADER))) {
+      batch->attribs[PIPE_SHADER_VERTEX] = panfrost_emit_vertex_data(
+         batch, &batch->attrib_bufs[PIPE_SHADER_VERTEX]);
    }
-
-   return T.gpu | nr_tables;
-}
-
-static void
-panfrost_emit_shader(struct panfrost_batch *batch,
-                     struct MALI_SHADER_ENVIRONMENT *cfg,
-                     enum pipe_shader_type stage, mali_ptr shader_ptr,
-                     mali_ptr thread_storage)
-{
-   cfg->resources = panfrost_emit_resources(batch, stage);
-   cfg->thread_storage = thread_storage;
-   cfg->shader = shader_ptr;
-
-   /* Each entry of FAU is 64-bits */
-   cfg->fau = batch->push_uniforms[stage];
-   cfg->fau_count = DIV_ROUND_UP(batch->nr_push_uniforms[stage], 2);
-}
 #endif
-
-static void
-panfrost_emit_draw(void *out, struct panfrost_batch *batch, bool fs_required,
-                   enum mesa_prim prim, mali_ptr pos, mali_ptr fs_vary,
-                   mali_ptr varyings)
-{
-   struct panfrost_context *ctx = batch->ctx;
-   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-   bool polygon = (prim == MESA_PRIM_TRIANGLES);
-
-   pan_pack(out, DRAW, cfg) {
-      /*
-       * From the Gallium documentation,
-       * pipe_rasterizer_state::cull_face "indicates which faces of
-       * polygons to cull". Points and lines are not considered
-       * polygons and should be drawn even if all faces are culled.
-       * The hardware does not take primitive type into account when
-       * culling, so we need to do that check ourselves.
-       */
-      cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
-      cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
-      cfg.front_face_ccw = rast->front_ccw;
-
-      if (ctx->occlusion_query && ctx->active_queries) {
-         if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
-            cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
-         else
-            cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
-
-         struct panfrost_resource *rsrc =
-            pan_resource(ctx->occlusion_query->rsrc);
-         cfg.occlusion = rsrc->image.data.bo->ptr.gpu;
-         panfrost_batch_write_rsrc(ctx->batch, rsrc, PIPE_SHADER_FRAGMENT);
-      }
-
-#if PAN_ARCH >= 9
-      struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
-
-      cfg.multisample_enable = rast->multisample;
-      cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
-
-      /* Use per-sample shading if required by API Also use it when a
-       * blend shader is used with multisampling, as this is handled
-       * by a single ST_TILE in the blend shader with the current
-       * sample ID, requiring per-sample shading.
-       */
-      cfg.evaluate_per_sample =
-         (rast->multisample &&
-          ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
-
-      cfg.single_sampled_lines = !rast->multisample;
-
-      cfg.vertex_array.packet = true;
-
-      cfg.minimum_z = batch->minimum_z;
-      cfg.maximum_z = batch->maximum_z;
-
-      cfg.depth_stencil = batch->depth_stencil;
-
-      if (fs_required) {
-         bool has_oq = ctx->occlusion_query && ctx->active_queries;
-
-         struct pan_earlyzs_state earlyzs = pan_earlyzs_get(
-            fs->earlyzs, ctx->depth_stencil->writes_zs || has_oq,
-            ctx->blend->base.alpha_to_coverage,
-            ctx->depth_stencil->zs_always_passes);
-
-         cfg.pixel_kill_operation = earlyzs.kill;
-         cfg.zs_update_operation = earlyzs.update;
-
-         cfg.allow_forward_pixel_to_kill =
-            pan_allow_forward_pixel_to_kill(ctx, fs);
-         cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
-
-         /* Mask of render targets that may be written. A render
-          * target may be written if the fragment shader writes
-          * to it AND it actually exists. If the render target
-          * doesn't actually exist, the blend descriptor will be
-          * OFF so it may be omitted from the mask.
-          *
-          * Only set when there is a fragment shader, since
-          * otherwise no colour updates are possible.
-          */
-         cfg.render_target_mask =
-            (fs->info.outputs_written >> FRAG_RESULT_DATA0) & ctx->fb_rt_mask;
-
-         /* Also use per-sample shading if required by the shader
-          */
-         cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
-
-         /* Unlike Bifrost, alpha-to-coverage must be included in
-          * this identically-named flag. Confusing, isn't it?
-          */
-         cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
-                                        fs->info.fs.can_discard ||
-                                        ctx->blend->base.alpha_to_coverage;
-
-         /* Blend descriptors are only accessed by a BLEND
-          * instruction on Valhall. It follows that if the
-          * fragment shader is omitted, we may also emit the
-          * blend descriptors.
-          */
-         cfg.blend = batch->blend;
-         cfg.blend_count = MAX2(batch->key.nr_cbufs, 1);
-         cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
-
-         cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
-         cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
-
-         panfrost_emit_shader(batch, &cfg.shader, PIPE_SHADER_FRAGMENT,
-                              batch->rsd[PIPE_SHADER_FRAGMENT], batch->tls.gpu);
-      } else {
-         /* These operations need to be FORCE to benefit from the
-          * depth-only pass optimizations.
-          */
-         cfg.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
-         cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_EARLY;
-
-         /* No shader and no blend => no shader or blend
-          * reasons to disable FPK. The only FPK-related state
-          * not covered is alpha-to-coverage which we don't set
-          * without blend.
-          */
-         cfg.allow_forward_pixel_to_kill = true;
-
-         /* No shader => no shader side effects */
-         cfg.allow_forward_pixel_to_be_killed = true;
-
-         /* Alpha isn't written so these are vacuous */
-         cfg.overdraw_alpha0 = true;
-         cfg.overdraw_alpha1 = true;
-      }
-#else
-      cfg.position = pos;
-      cfg.state = batch->rsd[PIPE_SHADER_FRAGMENT];
-      cfg.attributes = batch->attribs[PIPE_SHADER_FRAGMENT];
-      cfg.attribute_buffers = batch->attrib_bufs[PIPE_SHADER_FRAGMENT];
-      cfg.viewport = batch->viewport;
-      cfg.varyings = fs_vary;
-      cfg.varying_buffers = fs_vary ? varyings : 0;
-      cfg.thread_storage = batch->tls.gpu;
-
-      /* For all primitives but lines DRAW.flat_shading_vertex must
-       * be set to 0 and the provoking vertex is selected with the
-       * PRIMITIVE.first_provoking_vertex field.
-       */
-      if (prim == MESA_PRIM_LINES) {
-         /* The logic is inverted across arches. */
-         cfg.flat_shading_vertex = rast->flatshade_first ^ (PAN_ARCH <= 5);
-      }
-
-      pan_emit_draw_descs(batch, &cfg, PIPE_SHADER_FRAGMENT);
-#endif
-   }
-}
-
-#if PAN_ARCH >= 9
-static void
-panfrost_emit_malloc_vertex(struct panfrost_batch *batch,
-                            const struct pipe_draw_info *info,
-                            const struct pipe_draw_start_count_bias *draw,
-                            mali_ptr indices, bool secondary_shader, void *job)
-{
-   struct panfrost_context *ctx = batch->ctx;
-   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
-   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
-
-   bool fs_required = panfrost_fs_required(
-      fs, ctx->blend, &ctx->pipe_framebuffer, ctx->depth_stencil);
-
-   /* Varying shaders only feed data to the fragment shader, so if we omit
-    * the fragment shader, we should omit the varying shader too.
-    */
-   secondary_shader &= fs_required;
-
-   panfrost_emit_primitive(ctx, info, draw, 0, secondary_shader,
-                           pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE));
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, INSTANCE_COUNT, cfg) {
-      cfg.count = info->instance_count;
-   }
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
-      if (secondary_shader) {
-         unsigned v = vs->info.varyings.output_count;
-         unsigned f = fs->info.varyings.input_count;
-         unsigned slots = MAX2(v, f);
-         slots += util_bitcount(fs->key.fs.fixed_varying_mask);
-         unsigned size = slots * 16;
-
-         /* Assumes 16 byte slots. We could do better. */
-         cfg.vertex_packet_stride = size + 16;
-         cfg.vertex_attribute_stride = size;
-      } else {
-         /* Hardware requirement for "no varyings" */
-         cfg.vertex_packet_stride = 16;
-         cfg.vertex_attribute_stride = 0;
-      }
-   }
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, TILER, cfg) {
-      cfg.address = panfrost_batch_get_bifrost_tiler(batch, ~0);
-   }
-
-   STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
-   memcpy(pan_section_ptr(job, MALLOC_VERTEX_JOB, SCISSOR), &batch->scissor,
-          pan_size(SCISSOR));
-
-   panfrost_emit_primitive_size(
-      ctx, info->mode == MESA_PRIM_POINTS, 0,
-      pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE_SIZE));
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, INDICES, cfg) {
-      cfg.address = indices;
-   }
-
-   panfrost_emit_draw(pan_section_ptr(job, MALLOC_VERTEX_JOB, DRAW), batch,
-                      fs_required, u_reduced_prim(info->mode), 0, 0, 0);
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, POSITION, cfg) {
-      /* IDVS/points vertex shader */
-      mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
-
-      /* IDVS/triangle vertex shader */
-      if (vs_ptr && info->mode != MESA_PRIM_POINTS)
-         vs_ptr += pan_size(SHADER_PROGRAM);
-
-      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, vs_ptr,
-                           batch->tls.gpu);
-   }
-
-   pan_section_pack(job, MALLOC_VERTEX_JOB, VARYING, cfg) {
-      /* If a varying shader is used, we configure it with the same
-       * state as the position shader for backwards compatible
-       * behaviour with Bifrost. This could be optimized.
-       */
-      if (!secondary_shader)
-         continue;
-
-      mali_ptr ptr =
-         batch->rsd[PIPE_SHADER_VERTEX] + (2 * pan_size(SHADER_PROGRAM));
-
-      panfrost_emit_shader(batch, &cfg, PIPE_SHADER_VERTEX, ptr,
-                           batch->tls.gpu);
-   }
 }
-#endif
-
-#if PAN_ARCH <= 7
-static void
-panfrost_draw_emit_tiler(struct panfrost_batch *batch,
-                         const struct pipe_draw_info *info,
-                         const struct pipe_draw_start_count_bias *draw,
-                         void *invocation_template, mali_ptr indices,
-                         mali_ptr fs_vary, mali_ptr varyings, mali_ptr pos,
-                         mali_ptr psiz, bool secondary_shader, void *job)
-{
-   struct panfrost_context *ctx = batch->ctx;
-
-   void *section = pan_section_ptr(job, TILER_JOB, INVOCATION);
-   memcpy(section, invocation_template, pan_size(INVOCATION));
-
-   panfrost_emit_primitive(ctx, info, draw, indices, secondary_shader,
-                           pan_section_ptr(job, TILER_JOB, PRIMITIVE));
-
-   void *prim_size = pan_section_ptr(job, TILER_JOB, PRIMITIVE_SIZE);
-   enum mesa_prim prim = u_reduced_prim(info->mode);
-
-#if PAN_ARCH >= 6
-   pan_section_pack(job, TILER_JOB, TILER, cfg) {
-      cfg.address = panfrost_batch_get_bifrost_tiler(batch, ~0);
-   }
-
-   pan_section_pack(job, TILER_JOB, PADDING, cfg)
-      ;
-#endif
-
-   panfrost_emit_draw(pan_section_ptr(job, TILER_JOB, DRAW), batch, true, prim,
-                      pos, fs_vary, varyings);
-
-   panfrost_emit_primitive_size(ctx, prim == MESA_PRIM_POINTS, psiz, prim_size);
-}
-#endif
 
 static void
 panfrost_launch_xfb(struct panfrost_batch *batch,
-                    const struct pipe_draw_info *info, mali_ptr attribs,
-                    mali_ptr attrib_bufs, unsigned count)
+                    const struct pipe_draw_info *info, unsigned count)
 {
    struct panfrost_context *ctx = batch->ctx;
 
-   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-
    /* Nothing to do */
    if (batch->ctx->streamout.num_targets == 0)
       return;
@@ -3428,45 +2735,8 @@ panfrost_launch_xfb(struct panfrost_batc
                               &batch->push_uniforms[PIPE_SHADER_VERTEX],
                               &batch->nr_push_uniforms[PIPE_SHADER_VERTEX]);
 
-#if PAN_ARCH >= 9
-   pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
-      cfg.workgroup_size_x = 1;
-      cfg.workgroup_size_y = 1;
-      cfg.workgroup_size_z = 1;
-
-      cfg.workgroup_count_x = count;
-      cfg.workgroup_count_y = info->instance_count;
-      cfg.workgroup_count_z = 1;
-
-      panfrost_emit_shader(batch, &cfg.compute, PIPE_SHADER_VERTEX,
-                           batch->rsd[PIPE_SHADER_VERTEX], batch->tls.gpu);
-
-      /* TODO: Indexing. Also, this is a legacy feature... */
-      cfg.compute.attribute_offset = batch->ctx->offset_start;
-
-      /* Transform feedback shaders do not use barriers or shared
-       * memory, so we may merge workgroups.
-       */
-      cfg.allow_merging_workgroups = true;
-      cfg.task_increment = 1;
-      cfg.task_axis = MALI_TASK_AXIS_Z;
-   }
-#else
-   struct mali_invocation_packed invocation;
-
-   panfrost_pack_work_groups_compute(&invocation, 1, count,
-                                     info->instance_count, 1, 1, 1,
-                                     PAN_ARCH <= 5, false);
-
-   panfrost_draw_emit_vertex(batch, info, &invocation, 0, 0, attribs,
-                             attrib_bufs, t.cpu);
-#endif
-   enum mali_job_type job_type = MALI_JOB_TYPE_COMPUTE;
-#if PAN_ARCH <= 5
-   job_type = MALI_JOB_TYPE_VERTEX;
-#endif
-   panfrost_add_job(&batch->pool.base, &batch->scoreboard, job_type, true,
-                    false, 0, 0, &t, false);
+   JOBX(launch_xfb)(batch, info, count);
+   batch->any_compute = true;
 
    ctx->uncompiled[PIPE_SHADER_VERTEX] = vs_uncompiled;
    ctx->prog[PIPE_SHADER_VERTEX] = vs;
@@ -3491,16 +2761,38 @@ panfrost_increase_vertex_count(struct pa
       batch->tiler_ctx.vertex_count = UINT32_MAX;
 }
 
-static void
-panfrost_direct_draw(struct panfrost_batch *batch,
-                     const struct pipe_draw_info *info, unsigned drawid_offset,
-                     const struct pipe_draw_start_count_bias *draw)
+static bool
+panfrost_compatible_batch_state(struct panfrost_batch *batch, bool points)
 {
-   if (!draw->count || !info->instance_count)
-      return;
+   /* Only applies on Valhall */
+   if (PAN_ARCH < 9)
+      return true;
 
    struct panfrost_context *ctx = batch->ctx;
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
+   bool first = rast->flatshade_first;
 
+   /* gl_PointCoord orientation only matters when drawing points, but
+    * provoking vertex doesn't matter for points.
+    */
+   if (points)
+      return pan_tristate_set(&batch->sprite_coord_origin, coord);
+   else
+      return pan_tristate_set(&batch->first_provoking_vertex, first);
+}
+
+/*
+ * If we change whether we're drawing points, or whether point sprites are
+ * enabled (specified in the rasterizer), we may need to rebind shaders
+ * accordingly. This implicitly covers the case of rebinding framebuffers,
+ * because all dirty flags are set there.
+ */
+static void
+panfrost_update_point_sprite_shader(struct panfrost_context *ctx,
+                                    const struct pipe_draw_info *info)
+{
    /* If we change whether we're drawing points, or whether point sprites
     * are enabled (specified in the rasterizer), we may need to rebind
     * shaders accordingly. This implicitly covers the case of rebinding
@@ -3513,49 +2805,84 @@ panfrost_direct_draw(struct panfrost_bat
       ctx->active_prim = info->mode;
       panfrost_update_shader_variant(ctx, PIPE_SHADER_FRAGMENT);
    }
+}
 
-   /* Take into account a negative bias */
-   ctx->vertex_count =
-      draw->count + (info->index_size ? abs(draw->index_bias) : 0);
-   ctx->instance_count = info->instance_count;
-   ctx->base_vertex = info->index_size ? draw->index_bias : 0;
-   ctx->base_instance = info->start_instance;
-   ctx->active_prim = info->mode;
-   ctx->drawid = drawid_offset;
+/*
+ * Launch grid is the compute equivalent of draw_vbo. Set up the registers for a
+ * compute kernel and emit the run_compute command.
+ */
+static void
+panfrost_launch_grid(struct pipe_context *pipe,
+                     const struct pipe_grid_info *info)
+{
+   struct panfrost_context *ctx = pan_context(pipe);
 
-   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   /* XXX - shouldn't be necessary with working memory barriers. Affected
+    * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
+   panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
 
-   bool idvs = vs->info.vs.idvs;
-   bool secondary_shader = vs->info.vs.secondary_enable;
+   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
 
-   UNUSED struct panfrost_ptr tiler, vertex;
+   if (info->indirect && !PAN_GPU_INDIRECTS) {
+      struct pipe_transfer *transfer;
+      uint32_t *params =
+         pipe_buffer_map_range(pipe, info->indirect, info->indirect_offset,
+                               3 * sizeof(uint32_t), PIPE_MAP_READ, &transfer);
 
-   if (idvs) {
-#if PAN_ARCH >= 9
-      tiler = pan_pool_alloc_desc(&batch->pool.base, MALLOC_VERTEX_JOB);
-#elif PAN_ARCH >= 6
-      tiler = pan_pool_alloc_desc(&batch->pool.base, INDEXED_VERTEX_JOB);
-#else
-      unreachable("IDVS is unsupported on Midgard");
-#endif
-   } else {
-      vertex = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-      tiler = pan_pool_alloc_desc(&batch->pool.base, TILER_JOB);
+      struct pipe_grid_info direct = *info;
+      direct.indirect = NULL;
+      direct.grid[0] = params[0];
+      direct.grid[1] = params[1];
+      direct.grid[2] = params[2];
+      pipe_buffer_unmap(pipe, transfer);
+
+      if (params[0] && params[1] && params[2])
+         panfrost_launch_grid(pipe, &direct);
+
+      return;
    }
 
-   unsigned vertex_count = ctx->vertex_count;
+   ctx->compute_grid = info;
+
+   /* Conservatively assume workgroup size changes every launch */
+   ctx->dirty |= PAN_DIRTY_PARAMS;
+
+   panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
 
+   /* We want our compute thread descriptor to be per job.
+    * Save the global one, and restore it when we're done emitting
+    * the job.
+    */
+   mali_ptr saved_tls = batch->tls.gpu;
+
+   batch->tls.gpu = panfrost_emit_shared_memory(batch, info);
+   JOBX(launch_grid)(batch, info);
+   batch->any_compute = true;
+   batch->tls.gpu = saved_tls;
+
+   panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
+}
+
+static unsigned
+panfrost_draw_get_vertex_count(struct panfrost_batch *batch,
+                               const struct pipe_draw_info *info,
+                               const struct pipe_draw_start_count_bias *draw,
+                               bool idvs)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   unsigned vertex_count = ctx->vertex_count;
    unsigned min_index = 0, max_index = 0;
-   mali_ptr indices = 0;
 
+   batch->indices = 0;
    if (info->index_size && PAN_ARCH >= 9) {
-      indices = panfrost_get_index_buffer(batch, info, draw);
+      batch->indices = panfrost_get_index_buffer(batch, info, draw);
 
       /* Use index count to estimate vertex count */
-      panfrost_increase_vertex_count(batch, draw->count);
+      if (!PAN_USE_CSF)
+         panfrost_increase_vertex_count(batch, draw->count);
    } else if (info->index_size) {
-      indices = panfrost_get_index_buffer_bounded(batch, info, draw, &min_index,
-                                                  &max_index);
+      batch->indices = panfrost_get_index_buffer_bounded(
+         batch, info, draw, &min_index, &max_index);
 
       /* Use the corresponding values */
       vertex_count = max_index - min_index + 1;
@@ -3563,10 +2890,12 @@ panfrost_direct_draw(struct panfrost_bat
       panfrost_increase_vertex_count(batch, vertex_count);
    } else {
       ctx->offset_start = draw->start;
-      panfrost_increase_vertex_count(batch, vertex_count);
+
+      if (!PAN_USE_CSF)
+         panfrost_increase_vertex_count(batch, vertex_count);
    }
 
-   if (info->instance_count > 1) {
+   if (!PAN_USE_CSF && info->instance_count > 1) {
       unsigned count = vertex_count;
 
       /* Index-Driven Vertex Shading requires different instances to
@@ -3578,51 +2907,53 @@ panfrost_direct_draw(struct panfrost_bat
          count = ALIGN_POT(count, 4);
 
       ctx->padded_count = panfrost_padded_vertex_count(count);
-   } else
+   } else {
       ctx->padded_count = vertex_count;
+   }
 
-   panfrost_statistics_record(ctx, info, draw);
+   return vertex_count;
+}
 
-#if PAN_ARCH <= 7
-   struct mali_invocation_packed invocation;
-   if (info->instance_count > 1) {
-      panfrost_pack_work_groups_compute(&invocation, 1, vertex_count,
-                                        info->instance_count, 1, 1, 1, true,
-                                        false);
-   } else {
-      pan_pack(&invocation, INVOCATION, cfg) {
-         cfg.invocations = vertex_count - 1;
-         cfg.size_y_shift = 0;
-         cfg.size_z_shift = 0;
-         cfg.workgroups_x_shift = 0;
-         cfg.workgroups_y_shift = 0;
-         cfg.workgroups_z_shift = 32;
-         cfg.thread_group_split = MALI_SPLIT_MIN_EFFICIENT;
-      }
-   }
+/*
+ * Entrypoint for draws on JM/CSF Mali. Depending on generation, this requires
+ * emitting jobs for indirect drawing, transform feedback, vertex shading, and
+ * tiling.
+ */
+static void
+panfrost_direct_draw(struct panfrost_batch *batch,
+                     const struct pipe_draw_info *info, unsigned drawid_offset,
+                     const struct pipe_draw_start_count_bias *draw)
+{
+   if (!draw->count || !info->instance_count)
+      return;
 
-   /* Emit all sort of descriptors. */
-   mali_ptr varyings = 0, vs_vary = 0, fs_vary = 0, pos = 0, psiz = 0;
+   struct panfrost_context *ctx = batch->ctx;
 
-   panfrost_emit_varying_descriptor(
-      batch, ctx->padded_count * ctx->instance_count, &vs_vary, &fs_vary,
-      &varyings, NULL, &pos, &psiz, info->mode == MESA_PRIM_POINTS);
+   panfrost_update_point_sprite_shader(ctx, info);
 
-   mali_ptr attribs, attrib_bufs;
-   attribs = panfrost_emit_vertex_data(batch, &attrib_bufs);
-#endif
+   /* Take into account a negative bias */
+   ctx->vertex_count =
+      draw->count + (info->index_size ? abs(draw->index_bias) : 0);
+   ctx->instance_count = info->instance_count;
+   ctx->base_vertex = info->index_size ? draw->index_bias : 0;
+   ctx->base_instance = info->start_instance;
+   ctx->active_prim = info->mode;
+   ctx->drawid = drawid_offset;
+
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   bool idvs = vs->info.vs.idvs;
+   unsigned vertex_count =
+      panfrost_draw_get_vertex_count(batch, info, draw, idvs);
+
+   panfrost_statistics_record(ctx, info, draw);
 
    panfrost_update_state_3d(batch);
    panfrost_update_shader_state(batch, PIPE_SHADER_VERTEX);
    panfrost_update_shader_state(batch, PIPE_SHADER_FRAGMENT);
    panfrost_clean_state_3d(ctx);
 
-   if (ctx->uncompiled[PIPE_SHADER_VERTEX]->xfb) {
-#if PAN_ARCH >= 9
-      mali_ptr attribs = 0, attrib_bufs = 0;
-#endif
-      panfrost_launch_xfb(batch, info, attribs, attrib_bufs, draw->count);
-   }
+   if (ctx->uncompiled[PIPE_SHADER_VERTEX]->xfb)
+      panfrost_launch_xfb(batch, info, draw->count);
 
    /* Increment transform feedback offsets */
    panfrost_update_streamout_offsets(ctx);
@@ -3633,57 +2964,14 @@ panfrost_direct_draw(struct panfrost_bat
    if (panfrost_batch_skip_rasterization(batch))
       return;
 
-#if PAN_ARCH >= 9
-   assert(idvs && "Memory allocated IDVS required on Valhall");
-
-   panfrost_emit_malloc_vertex(batch, info, draw, indices, secondary_shader,
-                               tiler.cpu);
-
-   panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                    MALI_JOB_TYPE_MALLOC_VERTEX, false, false, 0, 0, &tiler,
-                    false);
-#else
-   /* Fire off the draw itself */
-   panfrost_draw_emit_tiler(batch, info, draw, &invocation, indices, fs_vary,
-                            varyings, pos, psiz, secondary_shader, tiler.cpu);
-   if (idvs) {
-#if PAN_ARCH >= 6
-      panfrost_draw_emit_vertex_section(
-         batch, vs_vary, varyings, attribs, attrib_bufs,
-         pan_section_ptr(tiler.cpu, INDEXED_VERTEX_JOB, VERTEX_DRAW));
-
-      panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                       MALI_JOB_TYPE_INDEXED_VERTEX, false, false, 0, 0, &tiler,
-                       false);
-#endif
-   } else {
-      panfrost_draw_emit_vertex(batch, info, &invocation, vs_vary, varyings,
-                                attribs, attrib_bufs, vertex.cpu);
-      panfrost_emit_vertex_tiler_jobs(batch, &vertex, &tiler);
-   }
+#if PAN_ARCH <= 7
+   panfrost_emit_varying_descriptor(batch,
+                                    ctx->padded_count * ctx->instance_count,
+                                    info->mode == MESA_PRIM_POINTS);
 #endif
-}
-
-static bool
-panfrost_compatible_batch_state(struct panfrost_batch *batch, bool points)
-{
-   /* Only applies on Valhall */
-   if (PAN_ARCH < 9)
-      return true;
-
-   struct panfrost_context *ctx = batch->ctx;
-   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
-
-   bool coord = (rast->sprite_coord_mode == PIPE_SPRITE_COORD_LOWER_LEFT);
-   bool first = rast->flatshade_first;
 
-   /* gl_PointCoord orientation only matters when drawing points, but
-    * provoking vertex doesn't matter for points.
-    */
-   if (points)
-      return pan_tristate_set(&batch->sprite_coord_origin, coord);
-   else
-      return pan_tristate_set(&batch->first_provoking_vertex, first);
+   JOBX(launch_draw)(batch, info, drawid_offset, draw, vertex_count);
+   batch->draw_count++;
 }
 
 static void
@@ -3715,7 +3003,7 @@ panfrost_draw_vbo(struct pipe_context *p
    /* Don't add too many jobs to a single batch. Hardware has a hard limit
     * of 65536 jobs, but we choose a smaller soft limit (arbitrary) to
     * avoid the risk of timeouts. This might not be a good idea. */
-   if (unlikely(batch->scoreboard.job_index > 10000))
+   if (unlikely(batch->draw_count > 10000))
       batch = panfrost_get_fresh_batch_for_fbo(ctx, "Too many draws");
 
    bool points = (info->mode == MESA_PRIM_POINTS);
@@ -3754,230 +3042,6 @@ panfrost_draw_vbo(struct pipe_context *p
    }
 }
 
-/* Launch grid is the compute equivalent of draw_vbo, so in this routine, we
- * construct the COMPUTE job and some of its payload.
- */
-
-static void
-panfrost_launch_grid_on_batch(struct pipe_context *pipe,
-                              struct panfrost_batch *batch,
-                              const struct pipe_grid_info *info)
-{
-   struct panfrost_context *ctx = pan_context(pipe);
-
-   if (info->indirect && !PAN_GPU_INDIRECTS) {
-      struct pipe_transfer *transfer;
-      uint32_t *params =
-         pipe_buffer_map_range(pipe, info->indirect, info->indirect_offset,
-                               3 * sizeof(uint32_t), PIPE_MAP_READ, &transfer);
-
-      struct pipe_grid_info direct = *info;
-      direct.indirect = NULL;
-      direct.grid[0] = params[0];
-      direct.grid[1] = params[1];
-      direct.grid[2] = params[2];
-      pipe_buffer_unmap(pipe, transfer);
-
-      if (params[0] && params[1] && params[2])
-         panfrost_launch_grid_on_batch(pipe, batch, &direct);
-
-      return;
-   }
-
-   ctx->compute_grid = info;
-
-   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
-
-   /* Invoke according to the grid info */
-
-   unsigned num_wg[3] = {info->grid[0], info->grid[1], info->grid[2]};
-
-   if (info->indirect)
-      num_wg[0] = num_wg[1] = num_wg[2] = 1;
-
-   /* Conservatively assume workgroup size changes every launch */
-   ctx->dirty |= PAN_DIRTY_PARAMS;
-
-   panfrost_update_shader_state(batch, PIPE_SHADER_COMPUTE);
-
-#if PAN_ARCH <= 7
-   panfrost_pack_work_groups_compute(
-      pan_section_ptr(t.cpu, COMPUTE_JOB, INVOCATION), num_wg[0], num_wg[1],
-      num_wg[2], info->block[0], info->block[1], info->block[2], false,
-      info->indirect != NULL);
-
-   pan_section_pack(t.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
-      cfg.job_task_split = util_logbase2_ceil(info->block[0] + 1) +
-                           util_logbase2_ceil(info->block[1] + 1) +
-                           util_logbase2_ceil(info->block[2] + 1);
-   }
-
-   pan_section_pack(t.cpu, COMPUTE_JOB, DRAW, cfg) {
-      cfg.state = batch->rsd[PIPE_SHADER_COMPUTE];
-      cfg.attributes = panfrost_emit_image_attribs(
-         batch, &cfg.attribute_buffers, PIPE_SHADER_COMPUTE);
-      cfg.thread_storage = panfrost_emit_shared_memory(batch, info);
-      cfg.uniform_buffers = batch->uniform_buffers[PIPE_SHADER_COMPUTE];
-      cfg.push_uniforms = batch->push_uniforms[PIPE_SHADER_COMPUTE];
-      cfg.textures = batch->textures[PIPE_SHADER_COMPUTE];
-      cfg.samplers = batch->samplers[PIPE_SHADER_COMPUTE];
-   }
-#else
-   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
-
-   pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
-      cfg.workgroup_size_x = info->block[0];
-      cfg.workgroup_size_y = info->block[1];
-      cfg.workgroup_size_z = info->block[2];
-
-      cfg.workgroup_count_x = num_wg[0];
-      cfg.workgroup_count_y = num_wg[1];
-      cfg.workgroup_count_z = num_wg[2];
-
-      panfrost_emit_shader(batch, &cfg.compute, PIPE_SHADER_COMPUTE,
-                           batch->rsd[PIPE_SHADER_COMPUTE],
-                           panfrost_emit_shared_memory(batch, info));
-
-      /* Workgroups may be merged if the shader does not use barriers
-       * or shared memory. This condition is checked against the
-       * static shared_size at compile-time. We need to check the
-       * variable shared size at launch_grid time, because the
-       * compiler doesn't know about that.
-       */
-      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
-                                     (info->variable_shared_mem == 0);
-
-      cfg.task_increment = 1;
-      cfg.task_axis = MALI_TASK_AXIS_Z;
-   }
-#endif
-
-   unsigned indirect_dep = 0;
-#if PAN_GPU_INDIRECTS
-   if (info->indirect) {
-      struct pan_indirect_dispatch_info indirect = {
-         .job = t.gpu,
-         .indirect_dim = pan_resource(info->indirect)->image.data.bo->ptr.gpu +
-                         info->indirect_offset,
-         .num_wg_sysval =
-            {
-               batch->num_wg_sysval[0],
-               batch->num_wg_sysval[1],
-               batch->num_wg_sysval[2],
-            },
-      };
-
-      indirect_dep = GENX(pan_indirect_dispatch_emit)(
-         &batch->pool.base, &batch->scoreboard, &indirect);
-   }
-#endif
-
-   panfrost_add_job(&batch->pool.base, &batch->scoreboard,
-                    MALI_JOB_TYPE_COMPUTE, true, false, indirect_dep, 0, &t,
-                    false);
-}
-
-static void
-panfrost_launch_grid(struct pipe_context *pipe,
-                     const struct pipe_grid_info *info)
-{
-   struct panfrost_context *ctx = pan_context(pipe);
-
-   /* XXX - shouldn't be necessary with working memory barriers. Affected
-    * test: KHR-GLES31.core.compute_shader.pipeline-post-xfb */
-   panfrost_flush_all_batches(ctx, "Launch grid pre-barrier");
-
-   struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
-   panfrost_launch_grid_on_batch(pipe, batch, info);
-
-   panfrost_flush_all_batches(ctx, "Launch grid post-barrier");
-}
-
-#define AFBC_BLOCK_ALIGN 16
-
-static void
-panfrost_launch_afbc_shader(struct panfrost_batch *batch, void *cso,
-                            struct pipe_constant_buffer *cbuf,
-                            unsigned nr_blocks)
-{
-   struct pipe_context *pctx = &batch->ctx->base;
-   void *saved_cso = NULL;
-   struct pipe_constant_buffer saved_const = {};
-   struct pipe_grid_info grid = {
-      .block[0] = 1,
-      .block[1] = 1,
-      .block[2] = 1,
-      .grid[0] = nr_blocks,
-      .grid[1] = 1,
-      .grid[2] = 1,
-   };
-
-   struct panfrost_constant_buffer *pbuf =
-      &batch->ctx->constant_buffer[PIPE_SHADER_COMPUTE];
-   saved_cso = batch->ctx->uncompiled[PIPE_SHADER_COMPUTE];
-   util_copy_constant_buffer(&pbuf->cb[0], &saved_const, true);
-
-   pctx->bind_compute_state(pctx, cso);
-   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, false, cbuf);
-
-   panfrost_launch_grid_on_batch(pctx, batch, &grid);
-
-   pctx->bind_compute_state(pctx, saved_cso);
-   pctx->set_constant_buffer(pctx, PIPE_SHADER_COMPUTE, 0, true, &saved_const);
-}
-
-#define LAUNCH_AFBC_SHADER(name, batch, rsrc, consts, nr_blocks)               \
-   struct pan_afbc_shader_data *shaders =                                      \
-      panfrost_afbc_get_shaders(batch->ctx, rsrc, AFBC_BLOCK_ALIGN);           \
-   struct pipe_constant_buffer constant_buffer = {                             \
-      .buffer_size = sizeof(consts),                                           \
-      .user_buffer = &consts};                                                 \
-   panfrost_launch_afbc_shader(batch, shaders->name##_cso, &constant_buffer,   \
-                               nr_blocks);
-
-static void
-panfrost_afbc_size(struct panfrost_batch *batch, struct panfrost_resource *src,
-                   struct panfrost_bo *metadata, unsigned offset,
-                   unsigned level)
-{
-   struct pan_image_slice_layout *slice = &src->image.layout.slices[level];
-   struct panfrost_afbc_size_info consts = {
-      .src =
-         src->image.data.bo->ptr.gpu + src->image.data.offset + slice->offset,
-      .metadata = metadata->ptr.gpu + offset,
-   };
-
-   panfrost_batch_read_rsrc(batch, src, PIPE_SHADER_COMPUTE);
-   panfrost_batch_write_bo(batch, metadata, PIPE_SHADER_COMPUTE);
-
-   LAUNCH_AFBC_SHADER(size, batch, src, consts, slice->afbc.nr_blocks);
-}
-
-static void
-panfrost_afbc_pack(struct panfrost_batch *batch, struct panfrost_resource *src,
-                   struct panfrost_bo *dst,
-                   struct pan_image_slice_layout *dst_slice,
-                   struct panfrost_bo *metadata, unsigned metadata_offset,
-                   unsigned level)
-{
-   struct pan_image_slice_layout *src_slice = &src->image.layout.slices[level];
-   struct panfrost_afbc_pack_info consts = {
-      .src = src->image.data.bo->ptr.gpu + src->image.data.offset +
-             src_slice->offset,
-      .dst = dst->ptr.gpu + dst_slice->offset,
-      .metadata = metadata->ptr.gpu + metadata_offset,
-      .header_size = dst_slice->afbc.header_size,
-      .src_stride = src_slice->afbc.stride,
-      .dst_stride = dst_slice->afbc.stride,
-   };
-
-   panfrost_batch_write_rsrc(batch, src, PIPE_SHADER_COMPUTE);
-   panfrost_batch_write_bo(batch, dst, PIPE_SHADER_COMPUTE);
-   panfrost_batch_add_bo(batch, metadata, PIPE_SHADER_COMPUTE);
-
-   LAUNCH_AFBC_SHADER(pack, batch, src, consts, dst_slice->afbc.nr_blocks);
-}
-
 static void *
 panfrost_create_rasterizer_state(struct pipe_context *pctx,
                                  const struct pipe_rasterizer_state *cso)
@@ -4224,8 +3288,7 @@ panfrost_create_sampler_view(struct pipe
    struct panfrost_sampler_view *so =
       rzalloc(pctx, struct panfrost_sampler_view);
 
-   pan_legalize_afbc_format(ctx, pan_resource(texture), template->format,
-                            false);
+   pan_legalize_afbc_format(ctx, pan_resource(texture), template->format);
 
    pipe_reference(NULL, &texture->reference);
 
@@ -4400,7 +3463,10 @@ prepare_shader(struct panfrost_compiled_
    /* Generic, or IDVS/points */
    pan_pack(ptr.cpu, SHADER_PROGRAM, cfg) {
       cfg.stage = pan_shader_stage(&state->info);
-      cfg.primary_shader = true;
+
+      if (PAN_ARCH == 9 || cfg.stage == MALI_SHADER_STAGE_FRAGMENT)
+         cfg.primary_shader = true;
+
       cfg.register_allocation =
          pan_register_allocation(state->info.work_reg_count);
       cfg.binary = state->bin.gpu;
@@ -4417,7 +3483,7 @@ prepare_shader(struct panfrost_compiled_
    /* IDVS/triangles */
    pan_pack(ptr.cpu + pan_size(SHADER_PROGRAM), SHADER_PROGRAM, cfg) {
       cfg.stage = pan_shader_stage(&state->info);
-      cfg.primary_shader = true;
+      cfg.primary_shader = (PAN_ARCH == 9);
       cfg.register_allocation =
          pan_register_allocation(state->info.work_reg_count);
       cfg.binary = state->bin.gpu + state->info.vs.no_psiz_offset;
@@ -4455,40 +3521,8 @@ static void
 preload(struct panfrost_batch *batch, struct pan_fb_info *fb)
 {
    GENX(pan_preload_fb)
-   (&batch->pool.base, &batch->scoreboard, fb, batch->tls.gpu,
-    PAN_ARCH >= 6 ? batch->tiler_ctx.bifrost : 0, NULL);
-}
-
-static void
-init_batch(struct panfrost_batch *batch)
-{
-   /* Reserve the framebuffer and local storage descriptors */
-   batch->framebuffer =
-#if PAN_ARCH == 4
-      pan_pool_alloc_desc(&batch->pool.base, FRAMEBUFFER);
-#else
-      pan_pool_alloc_desc_aggregate(
-         &batch->pool.base, PAN_DESC(FRAMEBUFFER), PAN_DESC(ZS_CRC_EXTENSION),
-         PAN_DESC_ARRAY(MAX2(batch->key.nr_cbufs, 1), RENDER_TARGET));
-#endif
-
-#if PAN_ARCH >= 6
-   batch->tls = pan_pool_alloc_desc(&batch->pool.base, LOCAL_STORAGE);
-#else
-   /* On Midgard, the TLS is embedded in the FB descriptor */
-   batch->tls = batch->framebuffer;
-
-#if PAN_ARCH == 5
-   struct mali_framebuffer_pointer_packed ptr;
-
-   pan_pack(ptr.opaque, FRAMEBUFFER_POINTER, cfg) {
-      cfg.pointer = batch->framebuffer.gpu;
-      cfg.render_target_count = 1; /* a necessary lie */
-   }
-
-   batch->tls.gpu = ptr.opaque[0];
-#endif
-#endif
+   (&batch->pool.base, !PAN_USE_CSF ? &batch->jm.jobs.vtc_jc : NULL, fb,
+    batch->tls.gpu, PAN_ARCH >= 6 ? batch->tiler_ctx.bifrost.ctx : 0, NULL);
 }
 
 static void
@@ -4503,7 +3537,7 @@ panfrost_sampler_view_destroy(struct pip
 }
 
 static void
-context_init(struct pipe_context *pipe)
+context_populate_vtbl(struct pipe_context *pipe)
 {
    pipe->draw_vbo = panfrost_draw_vbo;
    pipe->launch_grid = panfrost_launch_grid;
@@ -4531,7 +3565,7 @@ batch_get_polygon_list(struct panfrost_b
    struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
 
    if (!batch->tiler_ctx.midgard.polygon_list) {
-      bool has_draws = batch->scoreboard.first_tiler != NULL;
+      bool has_draws = batch->jm.jobs.vtc_jc.first_tiler != NULL;
       unsigned size = panfrost_tiler_get_polygon_list_size(
          dev, batch->key.width, batch->key.height,
          batch->tiler_ctx.vertex_count);
@@ -4573,30 +3607,66 @@ init_polygon_list(struct panfrost_batch
 {
 #if PAN_ARCH <= 5
    mali_ptr polygon_list = batch_get_polygon_list(batch);
-   panfrost_scoreboard_initialize_tiler(&batch->pool.base, &batch->scoreboard,
-                                        polygon_list);
+   panfrost_scoreboard_initialize_tiler(&batch->pool.base,
+                                        &batch->jm.jobs.vtc_jc, polygon_list);
 #endif
 }
 
+static void
+emit_tile_map(struct panfrost_batch *batch, struct pan_fb_info *fb)
+{
+   if (batch->key.nr_cbufs < 1 || !batch->key.cbufs[0])
+      return;
+
+   struct pipe_surface *surf = batch->key.cbufs[0];
+   struct panfrost_resource *pres = surf ? pan_resource(surf->texture) : NULL;
+
+   if (pres && pres->damage.tile_map.enable) {
+      fb->tile_map.base =
+         pan_pool_upload_aligned(&batch->pool.base, pres->damage.tile_map.data,
+                                 pres->damage.tile_map.size, 64);
+      fb->tile_map.stride = pres->damage.tile_map.stride;
+   }
+}
+
+static int
+submit_batch(struct panfrost_batch *batch, struct pan_fb_info *fb)
+{
+   bool has_frag = panfrost_has_fragment_job(batch);
+
+   preload(batch, fb);
+   init_polygon_list(batch);
+   emit_tls(batch);
+   emit_tile_map(batch, fb);
+
+   /* Now that all draws are in, we can finally prepare the
+    * FBD for the batch (if there is one). */
+   if (has_frag) {
+      emit_fbd(batch, fb);
+      emit_fragment_job(batch, fb);
+   }
+
+   JOBX(emit_batch_end)(batch);
+
+   return JOBX(submit_batch)(batch);
+}
+
 void
 GENX(panfrost_cmdstream_screen_init)(struct panfrost_screen *screen)
 {
    struct panfrost_device *dev = &screen->dev;
 
    screen->vtbl.prepare_shader = prepare_shader;
-   screen->vtbl.emit_tls = emit_tls;
-   screen->vtbl.emit_fbd = emit_fbd;
-   screen->vtbl.emit_fragment_job = emit_fragment_job;
    screen->vtbl.screen_destroy = screen_destroy;
-   screen->vtbl.preload = preload;
-   screen->vtbl.context_init = context_init;
-   screen->vtbl.init_batch = init_batch;
+   screen->vtbl.context_populate_vtbl = context_populate_vtbl;
+   screen->vtbl.context_init = JOBX(init_context);
+   screen->vtbl.context_cleanup = JOBX(cleanup_context);
+   screen->vtbl.init_batch = JOBX(init_batch);
+   screen->vtbl.cleanup_batch = JOBX(cleanup_batch);
+   screen->vtbl.submit_batch = submit_batch;
    screen->vtbl.get_blend_shader = GENX(pan_blend_get_shader_locked);
-   screen->vtbl.init_polygon_list = init_polygon_list;
    screen->vtbl.get_compiler_options = GENX(pan_shader_get_compiler_options);
    screen->vtbl.compile_shader = GENX(pan_shader_compile);
-   screen->vtbl.afbc_size = panfrost_afbc_size;
-   screen->vtbl.afbc_pack = panfrost_afbc_pack;
 
    GENX(pan_blitter_init)
    (dev, &screen->blitter.bin_pool.base, &screen->blitter.desc_pool.base);
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.h.8~	2023-11-24 23:33:24.961610824 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_cmdstream.h	2023-11-24 23:33:24.961610824 +0100
@@ -0,0 +1,323 @@
+/*
+ * Copyright (C) 2018 Alyssa Rosenzweig
+ * Copyright (C) 2020 Collabora Ltd.
+ * Copyright Â© 2017 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#ifndef __PAN_CMDSTREAM_H__
+#define __PAN_CMDSTREAM_H__
+
+#ifndef PAN_ARCH
+#error "PAN_ARCH undefined!"
+#endif
+
+#include "genxml/gen_macros.h"
+
+#include "pan_context.h"
+#include "pan_job.h"
+
+#include "pipe/p_defines.h"
+#include "pipe/p_state.h"
+
+#include "util/u_prim.h"
+
+#define PAN_GPU_INDIRECTS (PAN_ARCH == 7)
+
+struct panfrost_rasterizer {
+   struct pipe_rasterizer_state base;
+
+#if PAN_ARCH <= 7
+   /* Partially packed RSD words */
+   struct mali_multisample_misc_packed multisample;
+   struct mali_stencil_mask_misc_packed stencil_misc;
+#endif
+};
+
+struct panfrost_zsa_state {
+   struct pipe_depth_stencil_alpha_state base;
+
+   /* Is any depth, stencil, or alpha testing enabled? */
+   bool enabled;
+
+   /* Does the depth and stencil tests always pass? This ignores write
+    * masks, we are only interested in whether pixels may be killed.
+    */
+   bool zs_always_passes;
+
+   /* Are depth or stencil writes possible? */
+   bool writes_zs;
+
+#if PAN_ARCH <= 7
+   /* Prepacked words from the RSD */
+   struct mali_multisample_misc_packed rsd_depth;
+   struct mali_stencil_mask_misc_packed rsd_stencil;
+   struct mali_stencil_packed stencil_front, stencil_back;
+#else
+   /* Depth/stencil descriptor template */
+   struct mali_depth_stencil_packed desc;
+#endif
+};
+
+struct panfrost_vertex_state {
+   unsigned num_elements;
+   struct pipe_vertex_element pipe[PIPE_MAX_ATTRIBS];
+   uint16_t strides[PIPE_MAX_ATTRIBS];
+
+#if PAN_ARCH >= 9
+   /* Packed attribute descriptors */
+   struct mali_attribute_packed attributes[PIPE_MAX_ATTRIBS];
+#else
+   /* buffers corresponds to attribute buffer, element_buffers corresponds
+    * to an index in buffers for each vertex element */
+   struct pan_vertex_buffer buffers[PIPE_MAX_ATTRIBS];
+   unsigned element_buffer[PIPE_MAX_ATTRIBS];
+   unsigned nr_bufs;
+
+   unsigned formats[PIPE_MAX_ATTRIBS];
+#endif
+};
+
+static inline bool
+panfrost_is_implicit_prim_restart(const struct pipe_draw_info *info)
+{
+   /* As a reminder primitive_restart should always be checked before any
+      access to restart_index. */
+   return info->primitive_restart &&
+          info->restart_index == (unsigned)BITFIELD_MASK(info->index_size * 8);
+}
+
+static inline bool
+pan_allow_forward_pixel_to_kill(struct panfrost_context *ctx,
+                                struct panfrost_compiled_shader *fs)
+{
+   /* Track if any colour buffer is reused across draws, either
+    * from reading it directly, or from failing to write it
+    */
+   unsigned rt_mask = ctx->fb_rt_mask;
+   uint64_t rt_written = (fs->info.outputs_written >> FRAG_RESULT_DATA0) &
+                         ctx->blend->enabled_mask;
+   bool blend_reads_dest = (ctx->blend->load_dest_mask & rt_mask);
+   bool alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
+
+   return fs->info.fs.can_fpk && !(rt_mask & ~rt_written) &&
+          !alpha_to_coverage && !blend_reads_dest;
+}
+
+/*
+ * Determine whether to set the respective overdraw alpha flag.
+ *
+ * The overdraw alpha=1 flag should be set when alpha=1 implies full overdraw,
+ * equivalently, all enabled render targets have alpha_one_store set. Likewise,
+ * overdraw alpha=0 should be set when alpha=0 implies no overdraw,
+ * equivalently, all enabled render targets have alpha_zero_nop set.
+ */
+#if PAN_ARCH >= 6
+static inline bool
+panfrost_overdraw_alpha(const struct panfrost_context *ctx, bool zero)
+{
+   const struct panfrost_blend_state *so = ctx->blend;
+
+   for (unsigned i = 0; i < ctx->pipe_framebuffer.nr_cbufs; ++i) {
+      const struct pan_blend_info info = so->info[i];
+
+      bool enabled = ctx->pipe_framebuffer.cbufs[i] && !info.enabled;
+      bool flag = zero ? info.alpha_zero_nop : info.alpha_one_store;
+
+      if (enabled && !flag)
+         return false;
+   }
+
+   return true;
+}
+#endif
+
+static inline void
+panfrost_emit_primitive_size(struct panfrost_context *ctx, bool points,
+                             mali_ptr size_array, void *prim_size)
+{
+   struct panfrost_rasterizer *rast = ctx->rasterizer;
+
+   pan_pack(prim_size, PRIMITIVE_SIZE, cfg) {
+      if (panfrost_writes_point_size(ctx)) {
+         cfg.size_array = size_array;
+      } else {
+         cfg.constant = points ? rast->base.point_size : rast->base.line_width;
+      }
+   }
+}
+
+static inline uint8_t
+pan_draw_mode(enum mesa_prim mode)
+{
+   switch (mode) {
+
+#define DEFINE_CASE(c)                                                         \
+   case MESA_PRIM_##c:                                                         \
+      return MALI_DRAW_MODE_##c;
+
+      DEFINE_CASE(POINTS);
+      DEFINE_CASE(LINES);
+      DEFINE_CASE(LINE_LOOP);
+      DEFINE_CASE(LINE_STRIP);
+      DEFINE_CASE(TRIANGLES);
+      DEFINE_CASE(TRIANGLE_STRIP);
+      DEFINE_CASE(TRIANGLE_FAN);
+      DEFINE_CASE(QUADS);
+      DEFINE_CASE(POLYGON);
+#if PAN_ARCH <= 6
+      DEFINE_CASE(QUAD_STRIP);
+#endif
+
+#undef DEFINE_CASE
+
+   default:
+      unreachable("Invalid draw mode");
+   }
+}
+
+static inline enum mali_index_type
+panfrost_translate_index_size(unsigned size)
+{
+   STATIC_ASSERT(MALI_INDEX_TYPE_NONE == 0);
+   STATIC_ASSERT(MALI_INDEX_TYPE_UINT8 == 1);
+   STATIC_ASSERT(MALI_INDEX_TYPE_UINT16 == 2);
+
+   return (size == 4) ? MALI_INDEX_TYPE_UINT32 : size;
+}
+
+static inline bool
+panfrost_fs_required(struct panfrost_compiled_shader *fs,
+                     struct panfrost_blend_state *blend,
+                     struct pipe_framebuffer_state *state,
+                     const struct panfrost_zsa_state *zsa)
+{
+   /* If we generally have side effects. This inclues use of discard,
+    * which can affect the results of an occlusion query. */
+   if (fs->info.fs.sidefx)
+      return true;
+
+   /* Using an empty FS requires early-z to be enabled, but alpha test
+    * needs it disabled. Alpha test is only native on Midgard, so only
+    * check there.
+    */
+   if (PAN_ARCH <= 5 && zsa->base.alpha_func != PIPE_FUNC_ALWAYS)
+      return true;
+
+   /* If colour is written we need to execute */
+   for (unsigned i = 0; i < state->nr_cbufs; ++i) {
+      if (state->cbufs[i] && blend->info[i].enabled)
+         return true;
+   }
+
+   /* If depth is written and not implied we need to execute.
+    * TODO: Predicate on Z/S writes being enabled */
+   return (fs->info.fs.writes_depth || fs->info.fs.writes_stencil);
+}
+
+#if PAN_ARCH >= 9
+static inline mali_ptr
+panfrost_get_position_shader(struct panfrost_batch *batch,
+                             const struct pipe_draw_info *info)
+{
+   /* IDVS/points vertex shader */
+   mali_ptr vs_ptr = batch->rsd[PIPE_SHADER_VERTEX];
+
+   /* IDVS/triangle vertex shader */
+   if (vs_ptr && info->mode != MESA_PRIM_POINTS)
+      vs_ptr += pan_size(SHADER_PROGRAM);
+
+   return vs_ptr;
+}
+
+static inline mali_ptr
+panfrost_get_varying_shader(struct panfrost_batch *batch)
+{
+   return batch->rsd[PIPE_SHADER_VERTEX] + (2 * pan_size(SHADER_PROGRAM));
+}
+
+static inline unsigned
+panfrost_vertex_attribute_stride(struct panfrost_compiled_shader *vs,
+                                 struct panfrost_compiled_shader *fs)
+{
+   unsigned v = vs->info.varyings.output_count;
+   unsigned f = fs->info.varyings.input_count;
+   unsigned slots = MAX2(v, f);
+   slots += util_bitcount(fs->key.fs.fixed_varying_mask);
+
+   /* Assumes 16 byte slots. We could do better. */
+   return slots * 16;
+}
+
+static inline mali_ptr
+panfrost_emit_resources(struct panfrost_batch *batch,
+                        enum pipe_shader_type stage)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_ptr T;
+   unsigned nr_tables = 12;
+
+   /* Although individual resources need only 16 byte alignment, the
+    * resource table as a whole must be 64-byte aligned.
+    */
+   T = pan_pool_alloc_aligned(&batch->pool.base, nr_tables * pan_size(RESOURCE),
+                              64);
+   memset(T.cpu, 0, nr_tables * pan_size(RESOURCE));
+
+   panfrost_make_resource_table(T, PAN_TABLE_UBO, batch->uniform_buffers[stage],
+                                batch->nr_uniform_buffers[stage]);
+
+   panfrost_make_resource_table(T, PAN_TABLE_TEXTURE, batch->textures[stage],
+                                ctx->sampler_view_count[stage]);
+
+   /* We always need at least 1 sampler for txf to work */
+   panfrost_make_resource_table(T, PAN_TABLE_SAMPLER, batch->samplers[stage],
+                                MAX2(ctx->sampler_count[stage], 1));
+
+   panfrost_make_resource_table(T, PAN_TABLE_IMAGE, batch->images[stage],
+                                util_last_bit(ctx->image_mask[stage]));
+
+   if (stage == PIPE_SHADER_VERTEX) {
+      panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE,
+                                   batch->attribs[stage],
+                                   ctx->vertex->num_elements);
+
+      panfrost_make_resource_table(T, PAN_TABLE_ATTRIBUTE_BUFFER,
+                                   batch->attrib_bufs[stage],
+                                   util_last_bit(ctx->vb_mask));
+   }
+
+   return T.gpu | nr_tables;
+}
+#endif /* PAN_ARCH >= 9 */
+
+static inline bool
+pan_allow_rotating_primitives(const struct panfrost_compiled_shader *fs,
+                              const struct pipe_draw_info *info)
+{
+   bool lines =
+      (info->mode == MESA_PRIM_LINES || info->mode == MESA_PRIM_LINE_LOOP ||
+       info->mode == MESA_PRIM_LINE_STRIP);
+
+   return !lines && !fs->info.bifrost.uses_flat_shading;
+}
+
+#endif
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.c	2023-11-24 23:33:24.961610824 +0100
@@ -54,6 +54,9 @@
 #include "pan_screen.h"
 #include "pan_util.h"
 
+#include "drm-uapi/panfrost_drm.h"
+#include "drm-uapi/panthor_drm.h"
+
 static void
 panfrost_clear(struct pipe_context *pipe, unsigned buffers,
                const struct pipe_scissor_state *scissor_state,
@@ -70,7 +73,7 @@ panfrost_clear(struct pipe_context *pipe
    struct panfrost_batch *batch = panfrost_get_batch_for_fbo(ctx);
 
    /* At the start of the batch, we can clear for free */
-   if (!batch->scoreboard.first_job) {
+   if (batch->draw_count == 0) {
       panfrost_batch_clear(batch, buffers, color, depth, stencil);
       return;
    }
@@ -550,6 +553,8 @@ panfrost_destroy(struct pipe_context *pi
    struct panfrost_context *panfrost = pan_context(pipe);
    struct panfrost_device *dev = pan_device(pipe->screen);
 
+   pan_screen(pipe->screen)->vtbl.context_cleanup(panfrost);
+
    _mesa_hash_table_destroy(panfrost->writers, NULL);
 
    if (panfrost->blitter)
@@ -560,13 +565,12 @@ panfrost_destroy(struct pipe_context *pi
 
    panfrost_pool_cleanup(&panfrost->descs);
    panfrost_pool_cleanup(&panfrost->shaders);
-   panfrost_afbc_context_destroy(panfrost);
 
-   drmSyncobjDestroy(dev->fd, panfrost->in_sync_obj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), panfrost->in_sync_obj);
    if (panfrost->in_sync_fd != -1)
       close(panfrost->in_sync_fd);
 
-   drmSyncobjDestroy(dev->fd, panfrost->syncobj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), panfrost->syncobj);
    ralloc_free(pipe);
 }
 
@@ -860,7 +864,7 @@ panfrost_fence_server_sync(struct pipe_c
    struct panfrost_context *ctx = pan_context(pctx);
    int fd = -1, ret;
 
-   ret = drmSyncobjExportSyncFile(dev->fd, f->syncobj, &fd);
+   ret = drmSyncobjExportSyncFile(panfrost_device_fd(dev), f->syncobj, &fd);
    assert(!ret);
 
    sync_accumulate("panfrost", &ctx->in_sync_fd, fd);
@@ -940,11 +944,10 @@ panfrost_create_context(struct pipe_scre
    gallium->set_global_binding = panfrost_set_global_binding;
    gallium->memory_barrier = panfrost_memory_barrier;
 
-   pan_screen(screen)->vtbl.context_init(gallium);
+   pan_screen(screen)->vtbl.context_populate_vtbl(gallium);
 
    panfrost_resource_context_init(gallium);
    panfrost_shader_context_init(gallium);
-   panfrost_afbc_context_init(ctx);
 
    gallium->stream_uploader = u_upload_create_default(gallium);
    gallium->const_uploader = gallium->stream_uploader;
@@ -973,13 +976,23 @@ panfrost_create_context(struct pipe_scre
    /* Create a syncobj in a signaled state. Will be updated to point to the
     * last queued job out_sync every time we submit a new job.
     */
-   ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED, &ctx->syncobj);
+   ret = drmSyncobjCreate(panfrost_device_fd(dev), DRM_SYNCOBJ_CREATE_SIGNALED,
+                          &ctx->syncobj);
    assert(!ret && ctx->syncobj);
 
    /* Sync object/FD used for NATIVE_FENCE_FD. */
    ctx->in_sync_fd = -1;
-   ret = drmSyncobjCreate(dev->fd, 0, &ctx->in_sync_obj);
+   ret = drmSyncobjCreate(panfrost_device_fd(dev), 0, &ctx->in_sync_obj);
    assert(!ret);
 
+   pan_screen(screen)->vtbl.context_init(ctx);
+
    return gallium;
 }
+
+void
+panfrost_context_reinit(struct panfrost_context *ctx)
+{
+   pan_screen(ctx->base.screen)->vtbl.context_cleanup(ctx);
+   pan_screen(ctx->base.screen)->vtbl.context_init(ctx);
+}
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_context.h	2023-11-24 23:33:24.961610824 +0100
@@ -28,7 +28,6 @@
 #define _LARGEFILE64_SOURCE 1
 #include <assert.h>
 #include <sys/mman.h>
-#include "pan_afbc_cso.h"
 #include "pan_blend_cso.h"
 #include "pan_earlyzs.h"
 #include "pan_encoder.h"
@@ -50,6 +49,9 @@
 #include "compiler/shader_enums.h"
 #include "midgard/midgard_compile.h"
 
+#include "pan_csf.h"
+#include "pan_jm.h"
+
 #define SET_BIT(lval, bit, cond)                                               \
    if (cond)                                                                   \
       lval |= (bit);                                                           \
@@ -201,8 +203,6 @@ struct panfrost_context {
 
    struct blitter_context *blitter;
 
-   struct pan_afbc_shaders afbc_shaders;
-
    struct panfrost_blend_state *blend;
 
    /* On Valhall, does the current blend state use a blend shader for any
@@ -230,6 +230,11 @@ struct panfrost_context {
 
    int in_sync_fd;
    uint32_t in_sync_obj;
+
+   union {
+      struct panfrost_jm_context jm;
+      struct panfrost_csf_context csf;
+   };
 };
 
 /* Corresponds to the CSO */
@@ -521,4 +526,6 @@ void panfrost_track_image_access(struct
                                  enum pipe_shader_type stage,
                                  struct pipe_image_view *image);
 
+void panfrost_context_reinit(struct panfrost_context *ctx);
+
 #endif
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.c.8~	2023-11-24 23:33:24.961610824 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.c	2023-11-24 23:33:24.961610824 +0100
@@ -0,0 +1,909 @@
+/*
+ * Copyright (C) 2023 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "decode.h"
+
+#include "drm-uapi/panthor_drm.h"
+
+#include "genxml/ceu_builder.h"
+
+#include "pan_cmdstream.h"
+#include "pan_context.h"
+#include "pan_csf.h"
+#include "pan_job.h"
+
+#if PAN_ARCH < 10
+#error "CSF helpers are only used for gen >= 10"
+#endif
+
+static struct ceu_queue
+csf_alloc_ceu_queue(void *cookie)
+{
+   assert(cookie && "Self-contained queues can't be extended.");
+
+   struct panfrost_batch *batch = cookie;
+   unsigned capacity = 4096;
+   struct panfrost_bo *bo = panfrost_batch_create_bo(
+      batch, capacity * 8, 0, PIPE_SHADER_VERTEX, "Command queue");
+
+   memset(bo->ptr.cpu, 0xFF, capacity * 8);
+
+   return (struct ceu_queue){
+      .cpu = bo->ptr.cpu,
+      .gpu = bo->ptr.gpu,
+      .capacity = capacity,
+   };
+}
+
+void
+GENX(csf_cleanup_batch)(struct panfrost_batch *batch)
+{
+   free(batch->csf.cs.builder);
+}
+
+void
+GENX(csf_init_batch)(struct panfrost_batch *batch)
+{
+   /* Allocate and bind the command queue */
+   struct ceu_queue queue = csf_alloc_ceu_queue(batch);
+
+   /* Setup the queue builder */
+   batch->csf.cs.builder = malloc(sizeof(ceu_builder));
+   ceu_builder_init(batch->csf.cs.builder, 96, batch, csf_alloc_ceu_queue,
+                    queue);
+   ceu_require_all(batch->csf.cs.builder);
+
+   /* Set up entries */
+   ceu_builder *b = batch->csf.cs.builder;
+   ceu_set_scoreboard_entry(b, 2, 0);
+
+   /* Initialize the state vector */
+   for (unsigned i = 0; i < 64; i += 2)
+      ceu_move64_to(b, ceu_reg64(b, i), 0);
+
+   batch->framebuffer = pan_pool_alloc_desc_aggregate(
+      &batch->pool.base, PAN_DESC(FRAMEBUFFER), PAN_DESC(ZS_CRC_EXTENSION),
+      PAN_DESC_ARRAY(MAX2(batch->key.nr_cbufs, 1), RENDER_TARGET));
+   batch->tls = pan_pool_alloc_desc(&batch->pool.base, LOCAL_STORAGE);
+}
+
+static void
+csf_prepare_qsubmit(struct panfrost_context *ctx,
+                    struct drm_panthor_queue_submit *submit, uint8_t queue,
+                    uint64_t cs_start, uint32_t cs_size,
+                    struct drm_panthor_sync_op *syncs, uint32_t sync_count)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   *submit = (struct drm_panthor_queue_submit){
+      .queue_index = queue,
+      .stream_addr = cs_start,
+      .stream_size = cs_size,
+      .latest_flush = panthor_kmod_get_flush_id(dev->kmod.dev),
+      .syncs = DRM_PANTHOR_OBJ_ARRAY(sync_count, syncs),
+   };
+}
+
+static void
+csf_prepare_gsubmit(struct panfrost_context *ctx,
+                    struct drm_panthor_group_submit *gsubmit,
+                    struct drm_panthor_queue_submit *qsubmits,
+                    uint32_t qsubmit_count)
+{
+   *gsubmit = (struct drm_panthor_group_submit){
+      .group_handle = ctx->csf.group_handle,
+      .queue_submits = DRM_PANTHOR_OBJ_ARRAY(qsubmit_count, qsubmits),
+   };
+}
+
+static int
+csf_submit_gsubmit(struct panfrost_context *ctx,
+                   struct drm_panthor_group_submit *gsubmit)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   int ret = 0;
+
+   if (!ctx->is_noop) {
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_SUBMIT,
+                     gsubmit);
+   }
+
+   if (ret)
+      return errno;
+
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      /* Wait so we can get errors reported back */
+      drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                     NULL);
+
+      if ((dev->debug & PAN_DBG_TRACE) && dev->arch >= 10) {
+         const struct drm_panthor_queue_submit *qsubmits =
+            (void *)(uintptr_t)gsubmit->queue_submits.array;
+
+         for (unsigned i = 0; i < gsubmit->queue_submits.count; i++) {
+            uint32_t regs[256] = {0};
+            pandecode_cs(dev->decode_ctx, qsubmits[i].stream_addr,
+                         qsubmits[i].stream_size, panfrost_device_gpu_id(dev),
+                         regs);
+         }
+      }
+
+      if (dev->debug & PAN_DBG_DUMP)
+         pandecode_dump_mappings(dev->decode_ctx);
+   }
+
+   return 0;
+}
+
+int
+GENX(csf_submit_batch)(struct panfrost_batch *batch)
+{
+   uint32_t cs_instr_count = batch->csf.cs.builder->root_size;
+   uint64_t cs_start = batch->csf.cs.builder->root.gpu;
+   uint32_t cs_size = cs_instr_count * 8;
+   uint64_t vm_sync_signal_point, vm_sync_wait_point = 0, bo_sync_point;
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   uint32_t vm_sync_handle, bo_sync_handle, sync_count = 0;
+   struct drm_panthor_sync_op *syncs = NULL;
+   int ret;
+
+   panthor_kmod_vm_new_sync_point(dev->kmod.vm, &vm_sync_handle,
+                                  &vm_sync_signal_point);
+   assert(vm_sync_handle > 0 && vm_sync_signal_point > 0);
+
+   syncs = calloc(batch->num_bos + 5, sizeof(*syncs));
+   assert(syncs);
+
+   util_dynarray_foreach(&batch->bos, pan_bo_access, ptr) {
+      unsigned i = ptr - util_dynarray_element(&batch->bos, pan_bo_access, 0);
+      pan_bo_access flags = *ptr;
+
+      if (!flags)
+         continue;
+
+      /* Update the BO access flags so that panfrost_bo_wait() knows
+       * about all pending accesses.
+       * We only keep the READ/WRITE info since this is all the BO
+       * wait logic cares about.
+       * We also preserve existing flags as this batch might not
+       * be the first one to access the BO.
+       */
+      struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+      bo->gpu_access |= flags & (PAN_BO_ACCESS_RW);
+
+      ret = panthor_kmod_bo_get_sync_point(bo->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point,
+                                           !(flags & PAN_BO_ACCESS_WRITE));
+      if (ret)
+         return ret;
+
+      if (bo_sync_handle == vm_sync_handle) {
+         vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+      } else {
+         assert(bo_sync_point == 0 || !bo->kmod_bo->exclusive_vm);
+         syncs[sync_count++] = (struct drm_panthor_sync_op){
+            .flags =
+               DRM_PANTHOR_SYNC_OP_WAIT |
+               (bo_sync_point ? DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ
+                              : DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ),
+            .handle = bo_sync_handle,
+            .timeline_value = bo_sync_point,
+         };
+      }
+   }
+
+   util_dynarray_foreach(&batch->pool.bos, struct panfrost_bo *, bo) {
+      (*bo)->gpu_access |= PAN_BO_ACCESS_RW;
+
+      ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point, false);
+      if (ret)
+         return ret;
+
+      assert(bo_sync_handle == vm_sync_handle);
+      vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+   }
+
+   util_dynarray_foreach(&batch->invisible_pool.bos, struct panfrost_bo *, bo) {
+      (*bo)->gpu_access |= PAN_BO_ACCESS_RW;
+
+      ret = panthor_kmod_bo_get_sync_point((*bo)->kmod_bo, &bo_sync_handle,
+                                           &bo_sync_point, false);
+      if (ret)
+         return ret;
+
+      assert(bo_sync_handle == vm_sync_handle);
+      vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+   }
+
+   /* Always used on Bifrost, occassionally used on Midgard */
+   panthor_kmod_bo_get_sync_point(dev->sample_positions->kmod_bo,
+                                  &bo_sync_handle, &bo_sync_point, true);
+   dev->sample_positions->gpu_access |= PAN_BO_ACCESS_READ;
+   vm_sync_wait_point = MAX2(vm_sync_wait_point, bo_sync_point);
+
+   if (vm_sync_wait_point > 0) {
+      syncs[sync_count++] = (struct drm_panthor_sync_op){
+         .flags = DRM_PANTHOR_SYNC_OP_WAIT |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = vm_sync_handle,
+         .timeline_value = vm_sync_wait_point,
+      };
+   }
+
+   syncs[sync_count++] = (struct drm_panthor_sync_op){
+      .flags = DRM_PANTHOR_SYNC_OP_SIGNAL |
+               DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+      .handle = vm_sync_handle,
+      .timeline_value = vm_sync_signal_point,
+   };
+
+   syncs[sync_count++] = (struct drm_panthor_sync_op){
+      .flags =
+         DRM_PANTHOR_SYNC_OP_SIGNAL | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+      .handle = ctx->syncobj,
+   };
+
+   if (ctx->in_sync_fd >= 0) {
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), ctx->in_sync_obj,
+                                     ctx->in_sync_fd);
+      assert(!ret);
+
+      syncs[sync_count++] = (struct drm_panthor_sync_op){
+         .flags =
+            DRM_PANTHOR_SYNC_OP_WAIT | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+         .handle = ctx->in_sync_obj,
+      };
+      close(ctx->in_sync_fd);
+      ctx->in_sync_fd = -1;
+   }
+
+   struct drm_panthor_queue_submit qsubmit;
+   struct drm_panthor_group_submit gsubmit;
+
+   csf_prepare_qsubmit(ctx, &qsubmit, 0, cs_start, cs_size, syncs, sync_count);
+   csf_prepare_gsubmit(ctx, &gsubmit, &qsubmit, 1);
+   ret = csf_submit_gsubmit(ctx, &gsubmit);
+   if (!ret) {
+      util_dynarray_foreach(&batch->bos, pan_bo_access, ptr) {
+         unsigned i =
+            ptr - util_dynarray_element(&batch->bos, pan_bo_access, 0);
+         pan_bo_access flags = *ptr;
+
+         if (!flags)
+            continue;
+
+         struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+         panthor_kmod_bo_attach_sync_point(bo->kmod_bo, vm_sync_handle,
+                                           vm_sync_signal_point,
+                                           !(flags & PAN_BO_ACCESS_WRITE));
+      }
+
+      util_dynarray_foreach(&batch->pool.bos, struct panfrost_bo *, bo) {
+         panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
+                                           vm_sync_signal_point, false);
+      }
+
+      util_dynarray_foreach(&batch->invisible_pool.bos, struct panfrost_bo *,
+                            bo) {
+         panthor_kmod_bo_attach_sync_point((*bo)->kmod_bo, vm_sync_handle,
+                                           vm_sync_signal_point, false);
+      }
+
+      panthor_kmod_bo_attach_sync_point(dev->sample_positions->kmod_bo,
+                                        vm_sync_handle, vm_sync_signal_point,
+                                        true);
+   } else {
+      struct drm_panthor_group_get_state state = {
+         .group_handle = ctx->csf.group_handle,
+      };
+
+      ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_GET_STATE,
+                     &state);
+      assert(!ret);
+
+      if (state.state != 0)
+         panfrost_context_reinit(ctx);
+   }
+
+   free(syncs);
+
+   if (ret)
+      return errno;
+
+   /* Jobs won't be complete if blackhole rendering, that's ok */
+   if (!ctx->is_noop && (dev->debug & PAN_DBG_SYNC) &&
+       *((uint64_t *)batch->csf.cs.state.cpu) != 0) {
+      fprintf(stderr, "Incomplete job or timeout\n");
+      fflush(NULL);
+      abort();
+   }
+
+   return 0;
+}
+
+void
+GENX(csf_emit_batch_end)(struct panfrost_batch *batch)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   /* Barrier to let everything finish */
+   ceu_wait_slots(b, BITFIELD_MASK(8));
+
+   /* Get the CS state */
+   batch->csf.cs.state = pan_pool_alloc_aligned(&batch->pool.base, 8, 8);
+   memset(batch->csf.cs.state.cpu, ~0, 8);
+   ceu_move64_to(b, ceu_reg64(b, 90), batch->csf.cs.state.gpu);
+   ceu_store_state(b, 0, ceu_reg64(b, 90), MALI_CEU_STATE_ERROR_STATUS, 0, 0);
+
+   /* Flush caches now that we're done (synchronous) */
+   ceu_index flush_id = ceu_reg32(b, 74);
+   ceu_move32_to(b, flush_id, 0);
+   ceu_flush_caches(b, MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE,
+                    MALI_CEU_FLUSH_MODE_CLEAN_AND_INVALIDATE, true, flush_id, 0,
+                    0);
+
+   /* Finish the command stream */
+   ceu_finish(batch->csf.cs.builder);
+}
+
+void
+GENX(csf_emit_fragment_job)(struct panfrost_batch *batch,
+                            const struct pan_fb_info *pfb)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   if (batch->draw_count > 0) {
+      /* Finish tiling and wait for IDVS and tiling */
+      ceu_finish_tiling(b);
+      ceu_wait_slot(b, 2);
+      ceu_vt_end(b);
+   }
+
+   /* Set up the fragment job */
+   ceu_move64_to(b, ceu_reg64(b, 40), batch->framebuffer.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 42), (batch->miny << 16) | batch->minx);
+   ceu_move32_to(b, ceu_reg32(b, 43),
+                 ((batch->maxy - 1) << 16) | (batch->maxx - 1));
+
+   /* Run the fragment job and wait */
+   ceu_run_fragment(b, false);
+   ceu_wait_slot(b, 2);
+
+   /* Gather freed heap chunks and add them to the heap context free list
+    * so they can be re-used next time the tiler heap runs out of chunks.
+    * That's what ceu_finish_fragment() is all about. The list of freed
+    * chunks is in the tiler context descriptor
+    * (completed_{top,bottom fields}). */
+   if (batch->tiler_ctx.bifrost.ctx) {
+      ceu_move64_to(b, ceu_reg64(b, 94), batch->tiler_ctx.bifrost.ctx);
+      ceu_load_to(b, ceu_reg_tuple(b, 90, 4), ceu_reg64(b, 94),
+                  BITFIELD_MASK(4), 40);
+      ceu_wait_slot(b, 0);
+      ceu_finish_fragment(b, true, ceu_reg64(b, 90), ceu_reg64(b, 92), 0x0, 1);
+      ceu_wait_slot(b, 1);
+   }
+}
+
+static void
+csf_emit_shader_regs(struct panfrost_batch *batch, enum pipe_shader_type stage,
+                     mali_ptr shader)
+{
+   mali_ptr resources = panfrost_emit_resources(batch, stage);
+
+   assert(stage == PIPE_SHADER_VERTEX || stage == PIPE_SHADER_FRAGMENT ||
+          stage == PIPE_SHADER_COMPUTE);
+
+   unsigned offset = (stage == PIPE_SHADER_FRAGMENT) ? 4 : 0;
+   unsigned fau_count = DIV_ROUND_UP(batch->nr_push_uniforms[stage], 2);
+
+   ceu_builder *b = batch->csf.cs.builder;
+   ceu_move64_to(b, ceu_reg64(b, 0 + offset), resources);
+   ceu_move64_to(b, ceu_reg64(b, 8 + offset),
+                 batch->push_uniforms[stage] | ((uint64_t)fau_count << 56));
+   ceu_move64_to(b, ceu_reg64(b, 16 + offset), shader);
+}
+
+void
+GENX(csf_launch_grid)(struct panfrost_batch *batch,
+                      const struct pipe_grid_info *info)
+{
+   /* Empty compute programs are invalid and don't make sense */
+   if (batch->rsd[PIPE_SHADER_COMPUTE] == 0)
+      return;
+
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
+   ceu_builder *b = batch->csf.cs.builder;
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_COMPUTE,
+                        batch->rsd[PIPE_SHADER_COMPUTE]);
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+
+   /* Global attribute offset */
+   ceu_move32_to(b, ceu_reg32(b, 32), 0);
+
+   /* Compute workgroup size */
+   uint32_t wg_size[4];
+   pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+      cfg.workgroup_size_x = info->block[0];
+      cfg.workgroup_size_y = info->block[1];
+      cfg.workgroup_size_z = info->block[2];
+
+      /* Workgroups may be merged if the shader does not use barriers
+       * or shared memory. This condition is checked against the
+       * static shared_size at compile-time. We need to check the
+       * variable shared size at launch_grid time, because the
+       * compiler doesn't know about that.
+       */
+      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
+                                     (info->variable_shared_mem == 0);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+   /* Offset */
+   for (unsigned i = 0; i < 3; ++i)
+      ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+   if (info->indirect) {
+      /* Load size in workgroups per dimension from memory */
+      ceu_index address = ceu_reg64(b, 64);
+      ceu_move64_to(b, address,
+                    pan_resource(info->indirect)->image.data.bo->ptr.gpu +
+                       info->indirect_offset);
+
+      ceu_index grid_xyz = ceu_reg_tuple(b, 37, 3);
+      ceu_load_to(b, grid_xyz, address, BITFIELD_MASK(3), 0);
+
+      /* Wait for the load */
+      ceu_wait_slot(b, 0);
+
+      /* Copy to FAU */
+      for (unsigned i = 0; i < 3; ++i) {
+         if (batch->num_wg_sysval[i]) {
+            ceu_move64_to(b, address, batch->num_wg_sysval[i]);
+            ceu_store(b, ceu_extract32(b, grid_xyz, i), address,
+                      BITFIELD_MASK(1), 0);
+         }
+      }
+
+      /* Wait for the stores */
+      ceu_wait_slot(b, 0);
+   } else {
+      /* Set size in workgroups per dimension immediately */
+      for (unsigned i = 0; i < 3; ++i)
+         ceu_move32_to(b, ceu_reg32(b, 37 + i), info->grid[i]);
+   }
+
+   /* Dispatch. We could be much smarter choosing task size..
+    *
+    * TODO: How to choose correctly?
+    *
+    * XXX: Why are compute kernels failing if I make this smaller? Race
+    * condition maybe? Cache badnesss?
+    */
+   ceu_run_compute(b, 10, MALI_TASK_AXIS_Z);
+}
+
+void
+GENX(csf_launch_xfb)(struct panfrost_batch *batch,
+                     const struct pipe_draw_info *info, unsigned count)
+{
+   ceu_builder *b = batch->csf.cs.builder;
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+
+   /* TODO: Indexing. Also, attribute_offset is a legacy feature.. */
+   ceu_move32_to(b, ceu_reg32(b, 32), batch->ctx->offset_start);
+
+   /* Compute workgroup size */
+   uint32_t wg_size[4];
+   pan_pack(wg_size, COMPUTE_SIZE_WORKGROUP, cfg) {
+      cfg.workgroup_size_x = 1;
+      cfg.workgroup_size_y = 1;
+      cfg.workgroup_size_z = 1;
+
+      /* Transform feedback shaders do not use barriers or
+       * shared memory, so we may merge workgroups.
+       */
+      cfg.allow_merging_workgroups = true;
+   }
+   ceu_move32_to(b, ceu_reg32(b, 33), wg_size[0]);
+
+   /* Offset */
+   for (unsigned i = 0; i < 3; ++i)
+      ceu_move32_to(b, ceu_reg32(b, 34 + i), 0);
+
+   ceu_move32_to(b, ceu_reg32(b, 37), count);
+   ceu_move32_to(b, ceu_reg32(b, 38), info->instance_count);
+   ceu_move32_to(b, ceu_reg32(b, 39), 1);
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                        batch->rsd[PIPE_SHADER_VERTEX]);
+   /* XXX: Choose correctly */
+   ceu_run_compute(b, 1, MALI_TASK_AXIS_Z);
+
+   /* Reset registers expected to be 0 for IDVS */
+   ceu_move32_to(b, ceu_reg32(b, 31), 0);
+   ceu_move32_to(b, ceu_reg32(b, 32), 0);
+   ceu_move32_to(b, ceu_reg32(b, 37), 0);
+   ceu_move32_to(b, ceu_reg32(b, 38), 0);
+}
+
+static mali_ptr
+csf_get_tiler_desc(struct panfrost_batch *batch)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   if (batch->tiler_ctx.bifrost.ctx)
+      return batch->tiler_ctx.bifrost.ctx;
+
+   batch->tiler_ctx.bifrost.heap = batch->ctx->csf.heap.desc_bo->ptr.gpu;
+
+   struct panfrost_ptr t =
+      pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
+   GENX(pan_emit_tiler_ctx)
+   (dev, batch->key.width, batch->key.height,
+    util_framebuffer_get_num_samples(&batch->key),
+    pan_tristate_get(batch->first_provoking_vertex),
+    batch->ctx->csf.heap.desc_bo->ptr.gpu, ctx->csf.tmp_geom_bo->ptr.gpu,
+    ctx->csf.tmp_geom_bo->kmod_bo->size, t.cpu);
+
+   batch->tiler_ctx.bifrost.ctx = t.gpu;
+   return batch->tiler_ctx.bifrost.ctx;
+}
+
+void
+GENX(csf_launch_draw)(struct panfrost_batch *batch,
+                      const struct pipe_draw_info *info, unsigned drawid_offset,
+                      const struct pipe_draw_start_count_bias *draw,
+                      unsigned vertex_count)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+   bool idvs = vs->info.vs.idvs;
+   bool fs_required = panfrost_fs_required(
+      fs, ctx->blend, &ctx->pipe_framebuffer, ctx->depth_stencil);
+   bool secondary_shader = vs->info.vs.secondary_enable && fs_required;
+
+   assert(idvs && "IDVS required for CSF");
+
+   ceu_builder *b = batch->csf.cs.builder;
+
+   if (batch->draw_count == 0)
+      ceu_vt_start(batch->csf.cs.builder);
+
+   csf_emit_shader_regs(batch, PIPE_SHADER_VERTEX,
+                        panfrost_get_position_shader(batch, info));
+
+   if (fs_required) {
+      csf_emit_shader_regs(batch, PIPE_SHADER_FRAGMENT,
+                           batch->rsd[PIPE_SHADER_FRAGMENT]);
+   } else {
+      ceu_move64_to(b, ceu_reg64(b, 4), 0);
+      ceu_move64_to(b, ceu_reg64(b, 12), 0);
+      ceu_move64_to(b, ceu_reg64(b, 20), 0);
+   }
+
+   if (secondary_shader) {
+      ceu_move64_to(b, ceu_reg64(b, 18), panfrost_get_varying_shader(batch));
+   }
+
+   ceu_move64_to(b, ceu_reg64(b, 24), batch->tls.gpu);
+   ceu_move64_to(b, ceu_reg64(b, 30), batch->tls.gpu);
+   ceu_move32_to(b, ceu_reg32(b, 33), draw->count);
+   ceu_move32_to(b, ceu_reg32(b, 34), info->instance_count);
+   ceu_move32_to(b, ceu_reg32(b, 35), 0);
+
+   /* Base vertex offset on Valhall is used for both indexed and
+    * non-indexed draws, in a simple way for either. Handle both cases.
+    */
+   if (info->index_size) {
+      ceu_move32_to(b, ceu_reg32(b, 36), draw->index_bias);
+      ceu_move32_to(b, ceu_reg32(b, 39), info->index_size * draw->count);
+   } else {
+      ceu_move32_to(b, ceu_reg32(b, 36), draw->start);
+      ceu_move32_to(b, ceu_reg32(b, 39), 0);
+   }
+
+   ceu_move64_to(b, ceu_reg64(b, 40), csf_get_tiler_desc(batch));
+
+   STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
+   STATIC_ASSERT(sizeof(uint64_t) == pan_size(SCISSOR));
+   uint64_t *sbd = (uint64_t *)&batch->scissor[0];
+   ceu_move64_to(b, ceu_reg64(b, 42), *sbd);
+
+   ceu_move32_to(b, ceu_reg32(b, 44), fui(batch->minimum_z));
+   ceu_move32_to(b, ceu_reg32(b, 45), fui(batch->maximum_z));
+
+   if (ctx->occlusion_query && ctx->active_queries) {
+      struct panfrost_resource *rsrc = pan_resource(ctx->occlusion_query->rsrc);
+      ceu_move64_to(b, ceu_reg64(b, 46), rsrc->image.data.bo->ptr.gpu);
+      panfrost_batch_write_rsrc(ctx->batch, rsrc, PIPE_SHADER_FRAGMENT);
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 48), panfrost_vertex_attribute_stride(vs, fs));
+   ceu_move64_to(b, ceu_reg64(b, 50),
+                 batch->blend | MAX2(batch->key.nr_cbufs, 1));
+   ceu_move64_to(b, ceu_reg64(b, 52), batch->depth_stencil);
+
+   if (info->index_size)
+      ceu_move64_to(b, ceu_reg64(b, 54), batch->indices);
+
+   uint32_t primitive_flags = 0;
+   pan_pack(&primitive_flags, PRIMITIVE_FLAGS, cfg) {
+      if (panfrost_writes_point_size(ctx))
+         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
+
+      //                cfg.allow_rotating_primitives =
+      //                pan_allow_rotating_primitives(fs, info);
+
+      /* Non-fixed restart indices should have been lowered */
+      assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
+      cfg.primitive_restart = info->primitive_restart;
+
+      cfg.position_fifo_format = panfrost_writes_point_size(ctx)
+                                    ? MALI_FIFO_FORMAT_EXTENDED
+                                    : MALI_FIFO_FORMAT_BASIC;
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 56), primitive_flags);
+
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   uint32_t dcd_flags0 = 0, dcd_flags1 = 0;
+   pan_pack(&dcd_flags0, DCD_FLAGS_0, cfg) {
+      bool polygon = (u_reduced_prim(info->mode) == MESA_PRIM_TRIANGLES);
+
+      /*
+       * From the Gallium documentation,
+       * pipe_rasterizer_state::cull_face "indicates which faces of
+       * polygons to cull". Points and lines are not considered
+       * polygons and should be drawn even if all faces are culled.
+       * The hardware does not take primitive type into account when
+       * culling, so we need to do that check ourselves.
+       */
+      cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
+      cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
+      cfg.front_face_ccw = rast->front_ccw;
+
+      cfg.multisample_enable = rast->multisample;
+
+      /* Use per-sample shading if required by API Also use it when a
+       * blend shader is used with multisampling, as this is handled
+       * by a single ST_TILE in the blend shader with the current
+       * sample ID, requiring per-sample shading.
+       */
+      cfg.evaluate_per_sample =
+         (rast->multisample &&
+          ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
+
+      cfg.single_sampled_lines = !rast->multisample;
+
+      bool has_oq = ctx->occlusion_query && ctx->active_queries;
+      if (has_oq) {
+         if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
+         else
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
+      }
+
+      if (fs_required) {
+         struct pan_earlyzs_state earlyzs = pan_earlyzs_get(
+            fs->earlyzs, ctx->depth_stencil->writes_zs || has_oq,
+            ctx->blend->base.alpha_to_coverage,
+            ctx->depth_stencil->zs_always_passes);
+
+         cfg.pixel_kill_operation = earlyzs.kill;
+         cfg.zs_update_operation = earlyzs.update;
+
+         cfg.allow_forward_pixel_to_kill =
+            pan_allow_forward_pixel_to_kill(ctx, fs);
+         cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
+
+         cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
+         cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
+
+         /* Also use per-sample shading if required by the shader
+          */
+         cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
+
+         /* Unlike Bifrost, alpha-to-coverage must be included in
+          * this identically-named flag. Confusing, isn't it?
+          */
+         cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
+                                        fs->info.fs.can_discard ||
+                                        ctx->blend->base.alpha_to_coverage;
+
+         cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
+      } else {
+         /* These operations need to be FORCE to benefit from the
+          * depth-only pass optimizations.
+          */
+         cfg.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+         cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+
+         /* No shader and no blend => no shader or blend
+          * reasons to disable FPK. The only FPK-related state
+          * not covered is alpha-to-coverage which we don't set
+          * without blend.
+          */
+         cfg.allow_forward_pixel_to_kill = true;
+
+         /* No shader => no shader side effects */
+         cfg.allow_forward_pixel_to_be_killed = true;
+
+         /* Alpha isn't written so these are vacuous */
+         cfg.overdraw_alpha0 = true;
+         cfg.overdraw_alpha1 = true;
+      }
+   }
+
+   pan_pack(&dcd_flags1, DCD_FLAGS_1, cfg) {
+      cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
+
+      if (fs_required) {
+         /* See JM Valhall equivalent code */
+         cfg.render_target_mask =
+            (fs->info.outputs_written >> FRAG_RESULT_DATA0) & ctx->fb_rt_mask;
+      }
+   }
+
+   ceu_move32_to(b, ceu_reg32(b, 57), dcd_flags0);
+   ceu_move32_to(b, ceu_reg32(b, 58), dcd_flags1);
+
+   uint64_t primsize = 0;
+   panfrost_emit_primitive_size(ctx, info->mode == MESA_PRIM_POINTS, 0,
+                                &primsize);
+   ceu_move64_to(b, ceu_reg64(b, 60), primsize);
+
+   ceu_run_idvs(b, pan_draw_mode(info->mode),
+                panfrost_translate_index_size(info->index_size),
+                secondary_shader);
+}
+
+#define POSITION_FIFO_SIZE (64 * 1024)
+
+void
+GENX(csf_init_context)(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   struct drm_panthor_queue_create qc[] = {{
+      .priority = 1,
+      .ringbuf_size = 64 * 1024,
+   }};
+
+   struct drm_panthor_group_create gc = {
+      .compute_core_mask = dev->kmod.props.shader_present,
+      .fragment_core_mask = dev->kmod.props.shader_present,
+      .tiler_core_mask = 1,
+      .max_compute_cores = util_bitcount64(dev->kmod.props.shader_present),
+      .max_fragment_cores = util_bitcount64(dev->kmod.props.shader_present),
+      .max_tiler_cores = 1,
+      .priority = PANTHOR_GROUP_PRIORITY_MEDIUM,
+      .queues = DRM_PANTHOR_OBJ_ARRAY(ARRAY_SIZE(qc), qc),
+      .vm_id = pan_kmod_vm_handle(dev->kmod.vm),
+   };
+
+   int ret =
+      drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_CREATE, &gc);
+
+   assert(!ret);
+
+   ctx->csf.group_handle = gc.group_handle;
+
+   /* Get tiler heap */
+   struct drm_panthor_tiler_heap_create thc = {
+      .vm_id = pan_kmod_vm_handle(dev->kmod.vm),
+      .chunk_size = 2 * 1024 * 1024,
+      .initial_chunk_count = 5,
+      .max_chunks = 64 * 1024,
+      .target_in_flight = 65535,
+   };
+   ret = drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_TILER_HEAP_CREATE,
+                  &thc);
+
+   assert(!ret);
+
+   ctx->csf.heap.handle = thc.handle;
+
+   ctx->csf.heap.desc_bo =
+      panfrost_bo_create(dev, pan_size(TILER_HEAP), 0, "Tiler Heap");
+   pan_pack(ctx->csf.heap.desc_bo->ptr.cpu, TILER_HEAP, heap) {
+      heap.size = 2 * 1024 * 1024;
+      heap.base = thc.first_heap_chunk_gpu_va;
+      heap.bottom = heap.base + 64;
+      heap.top = heap.base + heap.size;
+   }
+
+   ctx->csf.tmp_geom_bo = panfrost_bo_create(
+      dev, POSITION_FIFO_SIZE, PAN_BO_INVISIBLE, "Temporary Geometry buffer");
+   assert(ctx->csf.tmp_geom_bo);
+
+   /* Setup the tiler heap */
+   struct panfrost_bo *cs_bo =
+      panfrost_bo_create(dev, 4096, 0, "Temporary CS buffer");
+   assert(cs_bo);
+
+   struct ceu_queue init_queue = {
+      .cpu = cs_bo->ptr.cpu,
+      .gpu = cs_bo->ptr.gpu,
+      .capacity = panfrost_bo_size(cs_bo) / sizeof(uint64_t),
+   };
+   ceu_builder b;
+   ceu_builder_init(&b, 96, NULL, NULL, init_queue);
+   ceu_index heap = ceu_reg64(&b, 72);
+   ceu_move64_to(&b, heap, thc.tiler_heap_ctx_gpu_va);
+   ceu_heap_set(&b, heap);
+
+   struct drm_panthor_queue_submit qsubmit;
+   struct drm_panthor_group_submit gsubmit;
+   struct drm_panthor_sync_op sync = {
+      .flags =
+         DRM_PANTHOR_SYNC_OP_SIGNAL | DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_SYNCOBJ,
+      .handle = ctx->syncobj,
+   };
+   uint32_t cs_instr_count = ceu_finish(&b);
+   uint64_t cs_start = b.root.gpu;
+   uint32_t cs_size = cs_instr_count * 8;
+
+   csf_prepare_qsubmit(ctx, &qsubmit, 0, cs_start, cs_size, &sync, 1);
+   csf_prepare_gsubmit(ctx, &gsubmit, &qsubmit, 1);
+   ret = csf_submit_gsubmit(ctx, &gsubmit);
+   assert(!ret);
+
+   /* Wait before freeing the buffer. */
+   drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                  NULL);
+   panfrost_bo_unreference(cs_bo);
+}
+
+void
+GENX(csf_cleanup_context)(struct panfrost_context *ctx)
+{
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   struct drm_panthor_tiler_heap_destroy thd = {
+      .handle = ctx->csf.heap.handle,
+   };
+   int ret = drmIoctl(panfrost_device_fd(dev),
+                      DRM_IOCTL_PANTHOR_TILER_HEAP_DESTROY, &thd);
+   assert(!ret);
+   panfrost_bo_unreference(ctx->csf.heap.desc_bo);
+
+   struct drm_panthor_group_destroy gd = {
+      .group_handle = ctx->csf.group_handle,
+   };
+
+   ret =
+      drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANTHOR_GROUP_DESTROY, &gd);
+   assert(!ret);
+
+   panfrost_bo_unreference(ctx->csf.heap.desc_bo);
+}
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.h.8~	2023-11-24 23:33:24.961610824 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_csf.h	2023-11-24 23:33:24.961610824 +0100
@@ -0,0 +1,91 @@
+/*
+ * Copyright (C) 2023 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#ifndef __PAN_CSF_H__
+#define __PAN_CSF_H__
+
+#include "compiler/shader_enums.h"
+
+#include "pan_bo.h"
+
+struct ceu_builder;
+
+struct panfrost_csf_batch {
+   /* CS related fields. */
+   struct {
+      /* CS builder. */
+      struct ceu_builder *builder;
+
+      /* CS state, written through the CS, and checked when PAN_MESA_DEBUG=sync.
+       */
+      struct panfrost_ptr state;
+   } cs;
+};
+
+struct panfrost_csf_context {
+   uint32_t group_handle;
+
+   struct {
+      uint32_t handle;
+      struct panfrost_bo *desc_bo;
+   } heap;
+
+   /* Temporary geometry buffer. Used as a FIFO by the tiler. */
+   struct panfrost_bo *tmp_geom_bo;
+};
+
+#if defined(PAN_ARCH) && PAN_ARCH >= 10
+
+#include "genxml/gen_macros.h"
+
+struct panfrost_batch;
+struct panfrost_context;
+struct pan_fb_info;
+struct pipe_draw_info;
+struct pipe_grid_info;
+struct pipe_draw_start_count_bias;
+
+void GENX(csf_init_context)(struct panfrost_context *ctx);
+void GENX(csf_cleanup_context)(struct panfrost_context *ctx);
+
+void GENX(csf_init_batch)(struct panfrost_batch *batch);
+void GENX(csf_cleanup_batch)(struct panfrost_batch *batch);
+int GENX(csf_submit_batch)(struct panfrost_batch *batch);
+
+void GENX(csf_emit_fragment_job)(struct panfrost_batch *batch,
+                                 const struct pan_fb_info *pfb);
+void GENX(csf_emit_batch_end)(struct panfrost_batch *batch);
+void GENX(csf_launch_xfb)(struct panfrost_batch *batch,
+                          const struct pipe_draw_info *info, unsigned count);
+void GENX(csf_launch_grid)(struct panfrost_batch *batch,
+                           const struct pipe_grid_info *info);
+void GENX(csf_launch_draw)(struct panfrost_batch *batch,
+                           const struct pipe_draw_info *info,
+                           unsigned drawid_offset,
+                           const struct pipe_draw_start_count_bias *draw,
+                           unsigned vertex_count);
+
+#endif /* PAN_ARCH >= 10 */
+
+#endif /* __PAN_CSF_H__ */
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_fence.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_fence.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_fence.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_fence.c	2023-11-24 23:33:24.961610824 +0100
@@ -42,7 +42,7 @@ panfrost_fence_reference(struct pipe_scr
    struct pipe_fence_handle *old = *ptr;
 
    if (pipe_reference(&old->reference, &fence->reference)) {
-      drmSyncobjDestroy(dev->fd, old->syncobj);
+      drmSyncobjDestroy(panfrost_device_fd(dev), old->syncobj);
       free(old);
    }
 
@@ -63,8 +63,8 @@ panfrost_fence_finish(struct pipe_screen
    if (abs_timeout == OS_TIMEOUT_INFINITE)
       abs_timeout = INT64_MAX;
 
-   ret = drmSyncobjWait(dev->fd, &fence->syncobj, 1, abs_timeout,
-                        DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+   ret = drmSyncobjWait(panfrost_device_fd(dev), &fence->syncobj, 1,
+                        abs_timeout, DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
 
    fence->signaled = (ret >= 0);
    return fence->signaled;
@@ -76,7 +76,7 @@ panfrost_fence_get_fd(struct pipe_screen
    struct panfrost_device *dev = pan_device(screen);
    int fd = -1;
 
-   drmSyncobjExportSyncFile(dev->fd, f->syncobj, &fd);
+   drmSyncobjExportSyncFile(panfrost_device_fd(dev), f->syncobj, &fd);
    return fd;
 }
 
@@ -92,20 +92,20 @@ panfrost_fence_from_fd(struct panfrost_c
       return NULL;
 
    if (type == PIPE_FD_TYPE_NATIVE_SYNC) {
-      ret = drmSyncobjCreate(dev->fd, 0, &f->syncobj);
+      ret = drmSyncobjCreate(panfrost_device_fd(dev), 0, &f->syncobj);
       if (ret) {
          fprintf(stderr, "create syncobj failed\n");
          goto err_free_fence;
       }
 
-      ret = drmSyncobjImportSyncFile(dev->fd, f->syncobj, fd);
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), f->syncobj, fd);
       if (ret) {
          fprintf(stderr, "import syncfile failed\n");
          goto err_destroy_syncobj;
       }
    } else {
       assert(type == PIPE_FD_TYPE_SYNCOBJ);
-      ret = drmSyncobjFDToHandle(dev->fd, fd, &f->syncobj);
+      ret = drmSyncobjFDToHandle(panfrost_device_fd(dev), fd, &f->syncobj);
       if (ret) {
          fprintf(stderr, "import syncobj FD failed\n");
          goto err_free_fence;
@@ -117,7 +117,7 @@ panfrost_fence_from_fd(struct panfrost_c
    return f;
 
 err_destroy_syncobj:
-   drmSyncobjDestroy(dev->fd, f->syncobj);
+   drmSyncobjDestroy(panfrost_device_fd(dev), f->syncobj);
 err_free_fence:
    free(f);
    return NULL;
@@ -134,7 +134,7 @@ panfrost_fence_create(struct panfrost_co
     * (HandleToFD/FDToHandle just gives you another syncobj ID for the
     * same syncobj).
     */
-   ret = drmSyncobjExportSyncFile(dev->fd, ctx->syncobj, &fd);
+   ret = drmSyncobjExportSyncFile(panfrost_device_fd(dev), ctx->syncobj, &fd);
    if (ret || fd == -1) {
       fprintf(stderr, "export failed\n");
       return NULL;
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.c.8~	2023-11-24 23:33:24.962610829 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.c	2023-11-24 23:33:24.962610829 +0100
@@ -0,0 +1,900 @@
+/*
+ * Copyright (C) 2018 Alyssa Rosenzweig
+ * Copyright (C) 2020 Collabora Ltd.
+ * Copyright Â© 2017 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include "decode.h"
+
+#include "drm-uapi/panfrost_drm.h"
+
+#include "genxml/ceu_builder.h"
+
+#include "pan_cmdstream.h"
+#include "pan_context.h"
+#include "pan_indirect_dispatch.h"
+#include "pan_jm.h"
+#include "pan_job.h"
+
+#if PAN_ARCH >= 10
+#error "JM helpers are only used for gen < 10"
+#endif
+
+void
+GENX(jm_init_batch)(struct panfrost_batch *batch)
+{
+   /* Reserve the framebuffer and local storage descriptors */
+   batch->framebuffer =
+#if PAN_ARCH == 4
+      pan_pool_alloc_desc(&batch->pool.base, FRAMEBUFFER);
+#else
+      pan_pool_alloc_desc_aggregate(
+         &batch->pool.base, PAN_DESC(FRAMEBUFFER), PAN_DESC(ZS_CRC_EXTENSION),
+         PAN_DESC_ARRAY(MAX2(batch->key.nr_cbufs, 1), RENDER_TARGET));
+#endif
+
+#if PAN_ARCH >= 6
+   batch->tls = pan_pool_alloc_desc(&batch->pool.base, LOCAL_STORAGE);
+#else
+   /* On Midgard, the TLS is embedded in the FB descriptor */
+   batch->tls = batch->framebuffer;
+
+#if PAN_ARCH == 5
+   struct mali_framebuffer_pointer_packed ptr;
+
+   pan_pack(ptr.opaque, FRAMEBUFFER_POINTER, cfg) {
+      cfg.pointer = batch->framebuffer.gpu;
+      cfg.render_target_count = 1; /* a necessary lie */
+   }
+
+   batch->tls.gpu = ptr.opaque[0];
+#endif
+#endif
+}
+
+static int
+jm_submit_jc(struct panfrost_batch *batch, mali_ptr first_job_desc,
+             uint32_t reqs)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+   bool has_frag = panfrost_has_fragment_job(batch);
+   struct drm_panfrost_submit submit = {
+      .jc = first_job_desc,
+      .requirements = reqs,
+   };
+   uint32_t in_syncs[1];
+   uint32_t *bo_handles;
+   int ret;
+
+   if ((reqs & PANFROST_JD_REQ_FS) || !has_frag ||
+       (dev->debug & (PAN_DBG_SYNC | PAN_DBG_TRACE)))
+      submit.out_sync = ctx->syncobj;
+
+   if (ctx->in_sync_fd >= 0) {
+      ret = drmSyncobjImportSyncFile(panfrost_device_fd(dev), ctx->in_sync_obj,
+                                     ctx->in_sync_fd);
+      assert(!ret);
+
+      in_syncs[submit.in_sync_count++] = ctx->in_sync_obj;
+      close(ctx->in_sync_fd);
+      ctx->in_sync_fd = -1;
+   }
+
+   if (submit.in_sync_count)
+      submit.in_syncs = (uintptr_t)in_syncs;
+
+   bo_handles = calloc(panfrost_pool_num_bos(&batch->pool) +
+                          panfrost_pool_num_bos(&batch->invisible_pool) +
+                          batch->num_bos + 2,
+                       sizeof(*bo_handles));
+   assert(bo_handles);
+
+   pan_bo_access *flags = util_dynarray_begin(&batch->bos);
+   unsigned end_bo = util_dynarray_num_elements(&batch->bos, pan_bo_access);
+
+   for (int i = 0; i < end_bo; ++i) {
+      if (!flags[i])
+         continue;
+
+      assert(submit.bo_handle_count < batch->num_bos);
+      bo_handles[submit.bo_handle_count++] = i;
+
+      /* Update the BO access flags so that panfrost_bo_wait() knows
+       * about all pending accesses.
+       * We only keep the READ/WRITE info since this is all the BO
+       * wait logic cares about.
+       * We also preserve existing flags as this batch might not
+       * be the first one to access the BO.
+       */
+      struct panfrost_bo *bo = pan_lookup_bo(dev, i);
+
+      bo->gpu_access |= flags[i] & (PAN_BO_ACCESS_RW);
+   }
+
+   panfrost_pool_get_bo_handles(&batch->pool,
+                                bo_handles + submit.bo_handle_count);
+   submit.bo_handle_count += panfrost_pool_num_bos(&batch->pool);
+   panfrost_pool_get_bo_handles(&batch->invisible_pool,
+                                bo_handles + submit.bo_handle_count);
+   submit.bo_handle_count += panfrost_pool_num_bos(&batch->invisible_pool);
+
+   /* Add the tiler heap to the list of accessed BOs if the batch has at
+    * least one tiler job. Tiler heap is written by tiler jobs and read
+    * by fragment jobs (the polygon list is coming from this heap).
+    */
+   if (batch->jm.jobs.vtc_jc.first_tiler) {
+      bo_handles[submit.bo_handle_count++] =
+         panfrost_bo_handle(dev->tiler_heap);
+   }
+
+   /* Always used on Bifrost, occassionally used on Midgard */
+   bo_handles[submit.bo_handle_count++] =
+      panfrost_bo_handle(dev->sample_positions);
+
+   submit.bo_handles = (u64)(uintptr_t)bo_handles;
+   if (ctx->is_noop)
+      ret = 0;
+   else
+      ret =
+         drmIoctl(panfrost_device_fd(dev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
+   free(bo_handles);
+
+   if (ret)
+      return errno;
+
+   /* Trace the job if we're doing that */
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      /* Wait so we can get errors reported back */
+      drmSyncobjWait(panfrost_device_fd(dev), &ctx->syncobj, 1, INT64_MAX, 0,
+                     NULL);
+
+      if (dev->debug & PAN_DBG_TRACE)
+         pandecode_jc(dev->decode_ctx, submit.jc, panfrost_device_gpu_id(dev));
+
+      if (dev->debug & PAN_DBG_DUMP)
+         pandecode_dump_mappings(dev->decode_ctx);
+
+      /* Jobs won't be complete if blackhole rendering, that's ok */
+      if (!ctx->is_noop && dev->debug & PAN_DBG_SYNC)
+         pandecode_abort_on_fault(dev->decode_ctx, submit.jc,
+                                  panfrost_device_gpu_id(dev));
+   }
+
+   return 0;
+}
+
+int
+GENX(jm_submit_batch)(struct panfrost_batch *batch)
+{
+   struct panfrost_device *dev = pan_device(batch->ctx->base.screen);
+   bool has_draws = batch->jm.jobs.vtc_jc.first_job;
+   bool has_tiler = batch->jm.jobs.vtc_jc.first_tiler;
+   bool has_frag = panfrost_has_fragment_job(batch);
+   int ret;
+
+   /* Take the submit lock to make sure no tiler jobs from other context
+    * are inserted between our tiler and fragment jobs, failing to do that
+    * might result in tiler heap corruption.
+    */
+   if (has_tiler)
+      pthread_mutex_lock(&dev->submit_lock);
+
+   if (has_draws) {
+      ret = jm_submit_jc(batch, batch->jm.jobs.vtc_jc.first_job, 0);
+      if (ret)
+         goto done;
+   }
+
+   if (has_frag) {
+      ret = jm_submit_jc(batch, batch->jm.jobs.frag, PANFROST_JD_REQ_FS);
+      if (ret)
+         goto done;
+   }
+
+done:
+   if (has_tiler)
+      pthread_mutex_unlock(&dev->submit_lock);
+
+   return ret;
+}
+
+void
+GENX(jm_emit_fragment_job)(struct panfrost_batch *batch,
+                           const struct pan_fb_info *pfb)
+{
+   struct panfrost_ptr transfer =
+      pan_pool_alloc_desc(&batch->pool.base, FRAGMENT_JOB);
+
+   GENX(pan_emit_fragment_job)(pfb, batch->framebuffer.gpu, transfer.cpu);
+
+   batch->jm.jobs.frag = transfer.gpu;
+}
+
+#if PAN_ARCH == 9
+static void
+jm_emit_shader_env(struct panfrost_batch *batch,
+                   struct MALI_SHADER_ENVIRONMENT *cfg,
+                   enum pipe_shader_type stage, mali_ptr shader_ptr)
+{
+   cfg->resources = panfrost_emit_resources(batch, stage);
+   cfg->thread_storage = batch->tls.gpu;
+   cfg->shader = shader_ptr;
+
+   /* Each entry of FAU is 64-bits */
+   cfg->fau = batch->push_uniforms[stage];
+   cfg->fau_count = DIV_ROUND_UP(batch->nr_push_uniforms[stage], 2);
+}
+#endif
+
+void
+GENX(jm_launch_grid)(struct panfrost_batch *batch,
+                     const struct pipe_grid_info *info)
+{
+   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+
+   /* Invoke according to the grid info */
+
+   unsigned num_wg[3] = {info->grid[0], info->grid[1], info->grid[2]};
+
+   if (info->indirect)
+      num_wg[0] = num_wg[1] = num_wg[2] = 1;
+
+#if PAN_ARCH <= 7
+   panfrost_pack_work_groups_compute(
+      pan_section_ptr(t.cpu, COMPUTE_JOB, INVOCATION), num_wg[0], num_wg[1],
+      num_wg[2], info->block[0], info->block[1], info->block[2], false,
+      info->indirect != NULL);
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = util_logbase2_ceil(info->block[0] + 1) +
+                           util_logbase2_ceil(info->block[1] + 1) +
+                           util_logbase2_ceil(info->block[2] + 1);
+   }
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, DRAW, cfg) {
+      cfg.state = batch->rsd[PIPE_SHADER_COMPUTE];
+      cfg.attributes = batch->attribs[PIPE_SHADER_COMPUTE];
+      cfg.attribute_buffers = batch->attrib_bufs[PIPE_SHADER_COMPUTE];
+      cfg.thread_storage = batch->tls.gpu;
+      cfg.uniform_buffers = batch->uniform_buffers[PIPE_SHADER_COMPUTE];
+      cfg.push_uniforms = batch->push_uniforms[PIPE_SHADER_COMPUTE];
+      cfg.textures = batch->textures[PIPE_SHADER_COMPUTE];
+      cfg.samplers = batch->samplers[PIPE_SHADER_COMPUTE];
+   }
+#else
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *cs = ctx->prog[PIPE_SHADER_COMPUTE];
+
+   pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
+      cfg.workgroup_size_x = info->block[0];
+      cfg.workgroup_size_y = info->block[1];
+      cfg.workgroup_size_z = info->block[2];
+
+      cfg.workgroup_count_x = num_wg[0];
+      cfg.workgroup_count_y = num_wg[1];
+      cfg.workgroup_count_z = num_wg[2];
+
+      jm_emit_shader_env(batch, &cfg.compute, PIPE_SHADER_COMPUTE,
+                         batch->rsd[PIPE_SHADER_COMPUTE]);
+
+      /* Workgroups may be merged if the shader does not use barriers
+       * or shared memory. This condition is checked against the
+       * static shared_size at compile-time. We need to check the
+       * variable shared size at launch_grid time, because the
+       * compiler doesn't know about that.
+       */
+      cfg.allow_merging_workgroups = cs->info.cs.allow_merging_workgroups &&
+                                     (info->variable_shared_mem == 0);
+
+      cfg.task_increment = 1;
+      cfg.task_axis = MALI_TASK_AXIS_Z;
+   }
+#endif
+
+   unsigned indirect_dep = 0;
+#if PAN_GPU_INDIRECTS
+   if (info->indirect) {
+      struct pan_indirect_dispatch_info indirect = {
+         .job = t.gpu,
+         .indirect_dim = pan_resource(info->indirect)->image.data.bo->ptr.gpu +
+                         info->indirect_offset,
+         .num_wg_sysval =
+            {
+               batch->num_wg_sysval[0],
+               batch->num_wg_sysval[1],
+               batch->num_wg_sysval[2],
+            },
+      };
+
+      indirect_dep = GENX(pan_indirect_dispatch_emit)(
+         &batch->pool.base, &batch->jm.jobs.vtc_jc, &indirect);
+   }
+#endif
+
+   panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                    MALI_JOB_TYPE_COMPUTE, true, false, indirect_dep, 0, &t,
+                    false);
+}
+
+#if PAN_ARCH >= 6
+static mali_ptr
+jm_get_tiler_desc(struct panfrost_batch *batch)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_device *dev = pan_device(ctx->base.screen);
+
+   if (batch->tiler_ctx.bifrost.ctx)
+      return batch->tiler_ctx.bifrost.ctx;
+
+   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, TILER_HEAP);
+   mali_ptr heap = t.gpu;
+
+   GENX(pan_emit_tiler_heap)(dev, (uint8_t *)t.cpu);
+   batch->tiler_ctx.bifrost.heap = t.gpu;
+
+   t = pan_pool_alloc_desc(&batch->pool.base, TILER_CONTEXT);
+   GENX(pan_emit_tiler_ctx)
+   (dev, batch->key.width, batch->key.height,
+    util_framebuffer_get_num_samples(&batch->key),
+    pan_tristate_get(batch->first_provoking_vertex), heap, 0, 0, t.cpu);
+
+   batch->tiler_ctx.bifrost.ctx = t.gpu;
+   return batch->tiler_ctx.bifrost.ctx;
+}
+#endif
+
+#if PAN_ARCH <= 7
+static inline void
+jm_emit_draw_descs(struct panfrost_batch *batch, struct MALI_DRAW *d,
+                   enum pipe_shader_type st)
+{
+   d->offset_start = batch->ctx->offset_start;
+   d->instance_size =
+      batch->ctx->instance_count > 1 ? batch->ctx->padded_count : 1;
+
+   d->uniform_buffers = batch->uniform_buffers[st];
+   d->push_uniforms = batch->push_uniforms[st];
+   d->textures = batch->textures[st];
+   d->samplers = batch->samplers[st];
+}
+
+static void
+jm_emit_vertex_draw(struct panfrost_batch *batch, void *section)
+{
+   pan_pack(section, DRAW, cfg) {
+      cfg.state = batch->rsd[PIPE_SHADER_VERTEX];
+      cfg.attributes = batch->attribs[PIPE_SHADER_VERTEX];
+      cfg.attribute_buffers = batch->attrib_bufs[PIPE_SHADER_VERTEX];
+      cfg.varyings = batch->varyings.vs;
+      cfg.varying_buffers = cfg.varyings ? batch->varyings.bufs : 0;
+      cfg.thread_storage = batch->tls.gpu;
+      jm_emit_draw_descs(batch, &cfg, PIPE_SHADER_VERTEX);
+   }
+}
+
+static void
+jm_emit_vertex_job(struct panfrost_batch *batch,
+                   const struct pipe_draw_info *info, void *invocation_template,
+                   void *job)
+{
+   void *section = pan_section_ptr(job, COMPUTE_JOB, INVOCATION);
+   memcpy(section, invocation_template, pan_size(INVOCATION));
+
+   pan_section_pack(job, COMPUTE_JOB, PARAMETERS, cfg) {
+      cfg.job_task_split = 5;
+   }
+
+   section = pan_section_ptr(job, COMPUTE_JOB, DRAW);
+   jm_emit_vertex_draw(batch, section);
+}
+#endif /* PAN_ARCH <= 7 */
+
+static void
+jm_emit_tiler_draw(void *out, struct panfrost_batch *batch, bool fs_required,
+                   enum mesa_prim prim)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+   bool polygon = (prim == MESA_PRIM_TRIANGLES);
+
+   pan_pack(out, DRAW, cfg) {
+      /*
+       * From the Gallium documentation,
+       * pipe_rasterizer_state::cull_face "indicates which faces of
+       * polygons to cull". Points and lines are not considered
+       * polygons and should be drawn even if all faces are culled.
+       * The hardware does not take primitive type into account when
+       * culling, so we need to do that check ourselves.
+       */
+      cfg.cull_front_face = polygon && (rast->cull_face & PIPE_FACE_FRONT);
+      cfg.cull_back_face = polygon && (rast->cull_face & PIPE_FACE_BACK);
+      cfg.front_face_ccw = rast->front_ccw;
+
+      if (ctx->occlusion_query && ctx->active_queries) {
+         if (ctx->occlusion_query->type == PIPE_QUERY_OCCLUSION_COUNTER)
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_COUNTER;
+         else
+            cfg.occlusion_query = MALI_OCCLUSION_MODE_PREDICATE;
+
+         struct panfrost_resource *rsrc =
+            pan_resource(ctx->occlusion_query->rsrc);
+         cfg.occlusion = rsrc->image.data.bo->ptr.gpu;
+         panfrost_batch_write_rsrc(ctx->batch, rsrc, PIPE_SHADER_FRAGMENT);
+      }
+
+#if PAN_ARCH >= 9
+      struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+
+      cfg.multisample_enable = rast->multisample;
+      cfg.sample_mask = rast->multisample ? ctx->sample_mask : 0xFFFF;
+
+      /* Use per-sample shading if required by API Also use it when a
+       * blend shader is used with multisampling, as this is handled
+       * by a single ST_TILE in the blend shader with the current
+       * sample ID, requiring per-sample shading.
+       */
+      cfg.evaluate_per_sample =
+         (rast->multisample &&
+          ((ctx->min_samples > 1) || ctx->valhall_has_blend_shader));
+
+      cfg.single_sampled_lines = !rast->multisample;
+
+      cfg.vertex_array.packet = true;
+
+      cfg.minimum_z = batch->minimum_z;
+      cfg.maximum_z = batch->maximum_z;
+
+      cfg.depth_stencil = batch->depth_stencil;
+
+      if (fs_required) {
+         bool has_oq = ctx->occlusion_query && ctx->active_queries;
+
+         struct pan_earlyzs_state earlyzs = pan_earlyzs_get(
+            fs->earlyzs, ctx->depth_stencil->writes_zs || has_oq,
+            ctx->blend->base.alpha_to_coverage,
+            ctx->depth_stencil->zs_always_passes);
+
+         cfg.pixel_kill_operation = earlyzs.kill;
+         cfg.zs_update_operation = earlyzs.update;
+
+         cfg.allow_forward_pixel_to_kill =
+            pan_allow_forward_pixel_to_kill(ctx, fs);
+         cfg.allow_forward_pixel_to_be_killed = !fs->info.writes_global;
+
+         /* Mask of render targets that may be written. A render
+          * target may be written if the fragment shader writes
+          * to it AND it actually exists. If the render target
+          * doesn't actually exist, the blend descriptor will be
+          * OFF so it may be omitted from the mask.
+          *
+          * Only set when there is a fragment shader, since
+          * otherwise no colour updates are possible.
+          */
+         cfg.render_target_mask =
+            (fs->info.outputs_written >> FRAG_RESULT_DATA0) & ctx->fb_rt_mask;
+
+         /* Also use per-sample shading if required by the shader
+          */
+         cfg.evaluate_per_sample |= fs->info.fs.sample_shading;
+
+         /* Unlike Bifrost, alpha-to-coverage must be included in
+          * this identically-named flag. Confusing, isn't it?
+          */
+         cfg.shader_modifies_coverage = fs->info.fs.writes_coverage ||
+                                        fs->info.fs.can_discard ||
+                                        ctx->blend->base.alpha_to_coverage;
+
+         /* Blend descriptors are only accessed by a BLEND
+          * instruction on Valhall. It follows that if the
+          * fragment shader is omitted, we may also emit the
+          * blend descriptors.
+          */
+         cfg.blend = batch->blend;
+         cfg.blend_count = MAX2(batch->key.nr_cbufs, 1);
+         cfg.alpha_to_coverage = ctx->blend->base.alpha_to_coverage;
+
+         cfg.overdraw_alpha0 = panfrost_overdraw_alpha(ctx, 0);
+         cfg.overdraw_alpha1 = panfrost_overdraw_alpha(ctx, 1);
+
+         jm_emit_shader_env(batch, &cfg.shader, PIPE_SHADER_FRAGMENT,
+                            batch->rsd[PIPE_SHADER_FRAGMENT]);
+      } else {
+         /* These operations need to be FORCE to benefit from the
+          * depth-only pass optimizations.
+          */
+         cfg.pixel_kill_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+         cfg.zs_update_operation = MALI_PIXEL_KILL_FORCE_EARLY;
+
+         /* No shader and no blend => no shader or blend
+          * reasons to disable FPK. The only FPK-related state
+          * not covered is alpha-to-coverage which we don't set
+          * without blend.
+          */
+         cfg.allow_forward_pixel_to_kill = true;
+
+         /* No shader => no shader side effects */
+         cfg.allow_forward_pixel_to_be_killed = true;
+
+         /* Alpha isn't written so these are vacuous */
+         cfg.overdraw_alpha0 = true;
+         cfg.overdraw_alpha1 = true;
+      }
+#else
+      cfg.position = batch->varyings.pos;
+      cfg.state = batch->rsd[PIPE_SHADER_FRAGMENT];
+      cfg.attributes = batch->attribs[PIPE_SHADER_FRAGMENT];
+      cfg.attribute_buffers = batch->attrib_bufs[PIPE_SHADER_FRAGMENT];
+      cfg.viewport = batch->viewport;
+      cfg.varyings = batch->varyings.fs;
+      cfg.varying_buffers = cfg.varyings ? batch->varyings.bufs : 0;
+      cfg.thread_storage = batch->tls.gpu;
+
+      /* For all primitives but lines DRAW.flat_shading_vertex must
+       * be set to 0 and the provoking vertex is selected with the
+       * PRIMITIVE.first_provoking_vertex field.
+       */
+      if (prim == MESA_PRIM_LINES) {
+         /* The logic is inverted across arches. */
+         cfg.flat_shading_vertex = rast->flatshade_first ^ (PAN_ARCH <= 5);
+      }
+
+      jm_emit_draw_descs(batch, &cfg, PIPE_SHADER_FRAGMENT);
+#endif
+   }
+}
+
+/* Packs a primitive descriptor, mostly common between Midgard/Bifrost tiler
+ * jobs and Valhall IDVS jobs
+ */
+static void
+jm_emit_primitive(struct panfrost_context *ctx,
+                  const struct pipe_draw_info *info,
+                  const struct pipe_draw_start_count_bias *draw,
+                  mali_ptr indices, bool secondary_shader, void *out)
+{
+   UNUSED struct pipe_rasterizer_state *rast = &ctx->rasterizer->base;
+
+   pan_pack(out, PRIMITIVE, cfg) {
+      cfg.draw_mode = pan_draw_mode(info->mode);
+      if (panfrost_writes_point_size(ctx))
+         cfg.point_size_array_format = MALI_POINT_SIZE_ARRAY_FORMAT_FP16;
+
+#if PAN_ARCH <= 8
+      bool lines =
+         (info->mode == MESA_PRIM_LINES || info->mode == MESA_PRIM_LINE_LOOP ||
+          info->mode == MESA_PRIM_LINE_STRIP);
+
+      /* For line primitives, PRIMITIVE.first_provoking_vertex must
+       * be set to true and the provoking vertex is selected with
+       * DRAW.flat_shading_vertex.
+       */
+      if (lines)
+         cfg.first_provoking_vertex = true;
+      else
+         cfg.first_provoking_vertex = rast->flatshade_first;
+
+      if (panfrost_is_implicit_prim_restart(info)) {
+         cfg.primitive_restart = MALI_PRIMITIVE_RESTART_IMPLICIT;
+      } else if (info->primitive_restart) {
+         cfg.primitive_restart = MALI_PRIMITIVE_RESTART_EXPLICIT;
+         cfg.primitive_restart_index = info->restart_index;
+      }
+
+      cfg.job_task_split = 6;
+#else
+      struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+
+      cfg.allow_rotating_primitives = pan_allow_rotating_primitives(fs, info);
+      cfg.primitive_restart = info->primitive_restart;
+
+      /* Non-fixed restart indices should have been lowered */
+      assert(!cfg.primitive_restart || panfrost_is_implicit_prim_restart(info));
+#endif
+
+      cfg.index_count = draw->count;
+      cfg.index_type = panfrost_translate_index_size(info->index_size);
+
+      if (PAN_ARCH >= 9) {
+         /* Base vertex offset on Valhall is used for both
+          * indexed and non-indexed draws, in a simple way for
+          * either. Handle both cases.
+          */
+         if (cfg.index_type)
+            cfg.base_vertex_offset = draw->index_bias;
+         else
+            cfg.base_vertex_offset = draw->start;
+
+         /* Indices are moved outside the primitive descriptor
+          * on Valhall, so we don't need to set that here
+          */
+      } else if (cfg.index_type) {
+         cfg.base_vertex_offset = draw->index_bias - ctx->offset_start;
+
+#if PAN_ARCH <= 7
+         cfg.indices = indices;
+#endif
+      }
+
+#if PAN_ARCH >= 6
+      cfg.secondary_shader = secondary_shader;
+#endif
+   }
+}
+
+#if PAN_ARCH == 9
+static void
+jm_emit_malloc_vertex_job(struct panfrost_batch *batch,
+                          const struct pipe_draw_info *info,
+                          const struct pipe_draw_start_count_bias *draw,
+                          bool secondary_shader, void *job)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   struct panfrost_compiled_shader *fs = ctx->prog[PIPE_SHADER_FRAGMENT];
+
+   bool fs_required = panfrost_fs_required(
+      fs, ctx->blend, &ctx->pipe_framebuffer, ctx->depth_stencil);
+
+   /* Varying shaders only feed data to the fragment shader, so if we omit
+    * the fragment shader, we should omit the varying shader too.
+    */
+   secondary_shader &= fs_required;
+
+   jm_emit_primitive(ctx, info, draw, batch->indices, secondary_shader,
+                     pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE));
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, INSTANCE_COUNT, cfg) {
+      cfg.count = info->instance_count;
+   }
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, ALLOCATION, cfg) {
+      if (secondary_shader) {
+         unsigned sz = panfrost_vertex_attribute_stride(vs, fs);
+         cfg.vertex_packet_stride = sz + 16;
+         cfg.vertex_attribute_stride = sz;
+      } else {
+         /* Hardware requirement for "no varyings" */
+         cfg.vertex_packet_stride = 16;
+         cfg.vertex_attribute_stride = 0;
+      }
+   }
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, TILER, cfg) {
+      cfg.address = jm_get_tiler_desc(batch);
+   }
+
+   STATIC_ASSERT(sizeof(batch->scissor) == pan_size(SCISSOR));
+   memcpy(pan_section_ptr(job, MALLOC_VERTEX_JOB, SCISSOR), &batch->scissor,
+          pan_size(SCISSOR));
+
+   panfrost_emit_primitive_size(
+      ctx, info->mode == MESA_PRIM_POINTS, 0,
+      pan_section_ptr(job, MALLOC_VERTEX_JOB, PRIMITIVE_SIZE));
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, INDICES, cfg) {
+      cfg.address = batch->indices;
+   }
+
+   jm_emit_tiler_draw(pan_section_ptr(job, MALLOC_VERTEX_JOB, DRAW), batch,
+                      fs_required, u_reduced_prim(info->mode));
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, POSITION, cfg) {
+      jm_emit_shader_env(batch, &cfg, PIPE_SHADER_VERTEX,
+                         panfrost_get_position_shader(batch, info));
+   }
+
+   pan_section_pack(job, MALLOC_VERTEX_JOB, VARYING, cfg) {
+      /* If a varying shader is used, we configure it with the same
+       * state as the position shader for backwards compatible
+       * behaviour with Bifrost. This could be optimized.
+       */
+      if (!secondary_shader)
+         continue;
+
+      jm_emit_shader_env(batch, &cfg, PIPE_SHADER_VERTEX,
+                         panfrost_get_varying_shader(batch));
+   }
+}
+#endif
+
+#if PAN_ARCH <= 7
+static void
+jm_emit_tiler_job(struct panfrost_batch *batch,
+                  const struct pipe_draw_info *info,
+                  const struct pipe_draw_start_count_bias *draw,
+                  void *invocation_template, bool secondary_shader, void *job)
+{
+   struct panfrost_context *ctx = batch->ctx;
+
+   void *section = pan_section_ptr(job, TILER_JOB, INVOCATION);
+   memcpy(section, invocation_template, pan_size(INVOCATION));
+
+   jm_emit_primitive(ctx, info, draw, batch->indices, secondary_shader,
+                     pan_section_ptr(job, TILER_JOB, PRIMITIVE));
+
+   void *prim_size = pan_section_ptr(job, TILER_JOB, PRIMITIVE_SIZE);
+   enum mesa_prim prim = u_reduced_prim(info->mode);
+
+#if PAN_ARCH >= 6
+   pan_section_pack(job, TILER_JOB, TILER, cfg) {
+      cfg.address = jm_get_tiler_desc(batch);
+   }
+
+   pan_section_pack(job, TILER_JOB, PADDING, cfg)
+      ;
+#endif
+
+   jm_emit_tiler_draw(pan_section_ptr(job, TILER_JOB, DRAW), batch, true, prim);
+
+   panfrost_emit_primitive_size(ctx, prim == MESA_PRIM_POINTS,
+                                batch->varyings.psiz, prim_size);
+}
+#endif
+
+void
+GENX(jm_launch_xfb)(struct panfrost_batch *batch,
+                    const struct pipe_draw_info *info, unsigned count)
+{
+   struct panfrost_ptr t = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+
+#if PAN_ARCH == 9
+   pan_section_pack(t.cpu, COMPUTE_JOB, PAYLOAD, cfg) {
+      cfg.workgroup_size_x = 1;
+      cfg.workgroup_size_y = 1;
+      cfg.workgroup_size_z = 1;
+
+      cfg.workgroup_count_x = count;
+      cfg.workgroup_count_y = info->instance_count;
+      cfg.workgroup_count_z = 1;
+
+      jm_emit_shader_env(batch, &cfg.compute, PIPE_SHADER_VERTEX,
+                         batch->rsd[PIPE_SHADER_VERTEX]);
+
+      /* TODO: Indexing. Also, this is a legacy feature... */
+      cfg.compute.attribute_offset = batch->ctx->offset_start;
+
+      /* Transform feedback shaders do not use barriers or shared
+       * memory, so we may merge workgroups.
+       */
+      cfg.allow_merging_workgroups = true;
+      cfg.task_increment = 1;
+      cfg.task_axis = MALI_TASK_AXIS_Z;
+   }
+#else
+   struct mali_invocation_packed invocation;
+
+   panfrost_pack_work_groups_compute(&invocation, 1, count,
+                                     info->instance_count, 1, 1, 1,
+                                     PAN_ARCH <= 5, false);
+
+   /* No varyings on XFB compute jobs. */
+   mali_ptr saved_vs_varyings = batch->varyings.vs;
+
+   batch->varyings.vs = 0;
+   jm_emit_vertex_job(batch, info, &invocation, t.cpu);
+   batch->varyings.vs = saved_vs_varyings;
+
+#endif
+   enum mali_job_type job_type = MALI_JOB_TYPE_COMPUTE;
+#if PAN_ARCH <= 5
+   job_type = MALI_JOB_TYPE_VERTEX;
+#endif
+   panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc, job_type, true,
+                    false, 0, 0, &t, false);
+}
+
+#if PAN_ARCH < 9
+/*
+ * Push jobs required for the rasterization pipeline. If there are side effects
+ * from the vertex shader, these are handled ahead-of-time with a compute
+ * shader. This function should not be called if rasterization is skipped.
+ */
+static void
+jm_push_vertex_tiler_jobs(struct panfrost_batch *batch,
+                          const struct panfrost_ptr *vertex_job,
+                          const struct panfrost_ptr *tiler_job)
+{
+   unsigned vertex = panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                                      MALI_JOB_TYPE_VERTEX, false, false, 0, 0,
+                                      vertex_job, false);
+
+   panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                    MALI_JOB_TYPE_TILER, false, false, vertex, 0, tiler_job,
+                    false);
+}
+#endif
+
+void
+GENX(jm_launch_draw)(struct panfrost_batch *batch,
+                     const struct pipe_draw_info *info, unsigned drawid_offset,
+                     const struct pipe_draw_start_count_bias *draw,
+                     unsigned vertex_count)
+{
+   struct panfrost_context *ctx = batch->ctx;
+   struct panfrost_compiled_shader *vs = ctx->prog[PIPE_SHADER_VERTEX];
+   bool secondary_shader = vs->info.vs.secondary_enable;
+   bool idvs = vs->info.vs.idvs;
+
+#if PAN_ARCH <= 7
+   struct mali_invocation_packed invocation;
+   if (info->instance_count > 1) {
+      panfrost_pack_work_groups_compute(&invocation, 1, vertex_count,
+                                        info->instance_count, 1, 1, 1, true,
+                                        false);
+   } else {
+      pan_pack(&invocation, INVOCATION, cfg) {
+         cfg.invocations = vertex_count - 1;
+         cfg.size_y_shift = 0;
+         cfg.size_z_shift = 0;
+         cfg.workgroups_x_shift = 0;
+         cfg.workgroups_y_shift = 0;
+         cfg.workgroups_z_shift = 32;
+         cfg.thread_group_split = MALI_SPLIT_MIN_EFFICIENT;
+      }
+   }
+
+   /* Emit all sort of descriptors. */
+#endif
+
+   UNUSED struct panfrost_ptr tiler, vertex;
+
+   if (idvs) {
+#if PAN_ARCH == 9
+      tiler = pan_pool_alloc_desc(&batch->pool.base, MALLOC_VERTEX_JOB);
+#elif PAN_ARCH >= 6
+      tiler = pan_pool_alloc_desc(&batch->pool.base, INDEXED_VERTEX_JOB);
+#else
+      unreachable("IDVS is unsupported on Midgard");
+#endif
+   } else {
+      vertex = pan_pool_alloc_desc(&batch->pool.base, COMPUTE_JOB);
+      tiler = pan_pool_alloc_desc(&batch->pool.base, TILER_JOB);
+   }
+
+#if PAN_ARCH == 9
+   assert(idvs && "Memory allocated IDVS required on Valhall");
+
+   jm_emit_malloc_vertex_job(batch, info, draw, secondary_shader, tiler.cpu);
+
+   panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                    MALI_JOB_TYPE_MALLOC_VERTEX, false, false, 0, 0, &tiler,
+                    false);
+#else
+   /* Fire off the draw itself */
+   jm_emit_tiler_job(batch, info, draw, &invocation, secondary_shader,
+                     tiler.cpu);
+   if (idvs) {
+#if PAN_ARCH >= 6
+      jm_emit_vertex_draw(
+         batch, pan_section_ptr(tiler.cpu, INDEXED_VERTEX_JOB, VERTEX_DRAW));
+
+      panfrost_add_job(&batch->pool.base, &batch->jm.jobs.vtc_jc,
+                       MALI_JOB_TYPE_INDEXED_VERTEX, false, false, 0, 0, &tiler,
+                       false);
+#endif
+   } else {
+      jm_emit_vertex_job(batch, info, &invocation, vertex.cpu);
+      jm_push_vertex_tiler_jobs(batch, &vertex, &tiler);
+   }
+#endif
+}
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.h.8~	2023-11-24 23:33:24.962610829 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_jm.h	2023-11-24 23:33:24.962610829 +0100
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 2023 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#ifndef __PAN_JM_H__
+#define __PAN_JM_H__
+
+#include "pan_scoreboard.h"
+
+struct panfrost_jm_batch {
+   /* Job related fields. */
+   struct {
+      /* Vertex/tiler/compute job chain. */
+      struct pan_scoreboard vtc_jc;
+
+      /* Fragment job, only one per batch. */
+      mali_ptr frag;
+   } jobs;
+};
+
+struct panfrost_jm_context {};
+
+#if defined(PAN_ARCH) && PAN_ARCH < 10
+
+#include "genxml/gen_macros.h"
+
+struct panfrost_batch;
+struct panfrost_context;
+struct pan_fb_info;
+struct pipe_draw_info;
+struct pipe_grid_info;
+struct pipe_draw_start_count_bias;
+
+static inline void
+GENX(jm_init_context)(struct panfrost_context *ctx)
+{
+}
+
+static inline void
+GENX(jm_cleanup_context)(struct panfrost_context *ctx)
+{
+}
+
+void GENX(jm_init_batch)(struct panfrost_batch *batch);
+
+static inline void
+GENX(jm_cleanup_batch)(struct panfrost_batch *batch)
+{
+}
+
+static inline void
+GENX(jm_emit_batch_end)(struct panfrost_batch *batch)
+{
+}
+
+int GENX(jm_submit_batch)(struct panfrost_batch *batch);
+
+void GENX(jm_emit_fragment_job)(struct panfrost_batch *batch,
+                                const struct pan_fb_info *pfb);
+
+void GENX(jm_launch_xfb)(struct panfrost_batch *batch,
+                         const struct pipe_draw_info *info, unsigned count);
+
+void GENX(jm_launch_grid)(struct panfrost_batch *batch,
+                          const struct pipe_grid_info *info);
+
+void GENX(jm_launch_draw)(struct panfrost_batch *batch,
+                          const struct pipe_draw_info *info,
+                          unsigned drawid_offset,
+                          const struct pipe_draw_start_count_bias *draw,
+                          unsigned vertex_count);
+
+#endif /* PAN_ARCH < 10 */
+
+#endif
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.c	2023-11-24 23:33:24.962610829 +0100
@@ -26,15 +26,12 @@
 
 #include <assert.h>
 
-#include "drm-uapi/panfrost_drm.h"
-
 #include "util/format/u_format.h"
 #include "util/hash_table.h"
 #include "util/ralloc.h"
 #include "util/rounding.h"
 #include "util/u_framebuffer.h"
 #include "util/u_pack_color.h"
-#include "decode.h"
 #include "pan_bo.h"
 #include "pan_context.h"
 #include "pan_util.h"
@@ -68,7 +65,6 @@ panfrost_batch_add_surface(struct panfro
 {
    if (surf) {
       struct panfrost_resource *rsrc = pan_resource(surf->texture);
-      pan_legalize_afbc_format(batch->ctx, rsrc, surf->format, true);
       panfrost_batch_write_rsrc(batch, rsrc, PIPE_SHADER_FRAGMENT);
    }
 }
@@ -116,6 +112,7 @@ static void
 panfrost_batch_cleanup(struct panfrost_context *ctx,
                        struct panfrost_batch *batch)
 {
+   struct panfrost_screen *screen = pan_screen(ctx->base.screen);
    struct panfrost_device *dev = pan_device(ctx->base.screen);
 
    assert(batch->seqnum);
@@ -123,6 +120,8 @@ panfrost_batch_cleanup(struct panfrost_c
    if (ctx->batch == batch)
       ctx->batch = NULL;
 
+   screen->vtbl.cleanup_batch(batch);
+
    unsigned batch_idx = panfrost_batch_idx(batch);
 
    pan_bo_access *flags = util_dynarray_begin(&batch->bos);
@@ -229,7 +228,7 @@ panfrost_get_fresh_batch_for_fbo(struct
    /* We only need to submit and get a fresh batch if there is no
     * draw/clear queued. Otherwise we may reuse the batch. */
 
-   if (batch->scoreboard.first_job) {
+   if (batch->draw_count) {
       perf_debug_ctx(ctx, "Flushing the current FBO due to: %s", reason);
       panfrost_batch_submit(ctx, batch);
       batch = panfrost_get_batch(ctx, &ctx->pipe_framebuffer);
@@ -303,7 +302,7 @@ panfrost_batch_uses_resource(struct panf
                              struct panfrost_resource *rsrc)
 {
    /* A resource is used iff its current BO is used */
-   uint32_t handle = rsrc->image.data.bo->gem_handle;
+   uint32_t handle = panfrost_bo_handle(rsrc->image.data.bo);
    unsigned size = util_dynarray_num_elements(&batch->bos, pan_bo_access);
 
    /* If out of bounds, certainly not used */
@@ -321,7 +320,8 @@ panfrost_batch_add_bo_old(struct panfros
    if (!bo)
       return;
 
-   pan_bo_access *entry = panfrost_batch_get_bo_access(batch, bo->gem_handle);
+   pan_bo_access *entry =
+      panfrost_batch_get_bo_access(batch, panfrost_bo_handle(bo));
    pan_bo_access old_flags = *entry;
 
    if (!old_flags) {
@@ -420,7 +420,7 @@ panfrost_batch_get_scratchpad(struct pan
       size_per_thread, thread_tls_alloc, core_id_range);
 
    if (batch->scratchpad) {
-      assert(batch->scratchpad->size >= size);
+      assert(panfrost_bo_size(batch->scratchpad) >= size);
    } else {
       batch->scratchpad =
          panfrost_batch_create_bo(batch, size, PAN_BO_INVISIBLE,
@@ -437,7 +437,7 @@ panfrost_batch_get_shared_memory(struct
                                  unsigned workgroup_count)
 {
    if (batch->shared_memory) {
-      assert(batch->shared_memory->size >= size);
+      assert(panfrost_bo_size(batch->shared_memory) >= size);
    } else {
       batch->shared_memory = panfrost_batch_create_bo(
          batch, size, PAN_BO_INVISIBLE, PIPE_SHADER_VERTEX,
@@ -590,206 +590,20 @@ panfrost_batch_to_fb_info(const struct p
    }
 }
 
-static int
-panfrost_batch_submit_ioctl(struct panfrost_batch *batch,
-                            mali_ptr first_job_desc, uint32_t reqs,
-                            uint32_t in_sync, uint32_t out_sync)
-{
-   struct panfrost_context *ctx = batch->ctx;
-   struct pipe_context *gallium = (struct pipe_context *)ctx;
-   struct panfrost_device *dev = pan_device(gallium->screen);
-   struct drm_panfrost_submit submit = {
-      0,
-   };
-   uint32_t in_syncs[2];
-   uint32_t *bo_handles;
-   int ret;
-
-   /* If we trace, we always need a syncobj, so make one of our own if we
-    * weren't given one to use. Remember that we did so, so we can free it
-    * after we're done but preventing double-frees if we were given a
-    * syncobj */
-
-   if (!out_sync && dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-      out_sync = ctx->syncobj;
-
-   submit.out_sync = out_sync;
-   submit.jc = first_job_desc;
-   submit.requirements = reqs;
-
-   if (in_sync)
-      in_syncs[submit.in_sync_count++] = in_sync;
-
-   if (ctx->in_sync_fd >= 0) {
-      ret =
-         drmSyncobjImportSyncFile(dev->fd, ctx->in_sync_obj, ctx->in_sync_fd);
-      assert(!ret);
-
-      in_syncs[submit.in_sync_count++] = ctx->in_sync_obj;
-      close(ctx->in_sync_fd);
-      ctx->in_sync_fd = -1;
-   }
-
-   if (submit.in_sync_count)
-      submit.in_syncs = (uintptr_t)in_syncs;
-
-   bo_handles = calloc(panfrost_pool_num_bos(&batch->pool) +
-                          panfrost_pool_num_bos(&batch->invisible_pool) +
-                          batch->num_bos + 2,
-                       sizeof(*bo_handles));
-   assert(bo_handles);
-
-   pan_bo_access *flags = util_dynarray_begin(&batch->bos);
-   unsigned end_bo = util_dynarray_num_elements(&batch->bos, pan_bo_access);
-
-   for (int i = 0; i < end_bo; ++i) {
-      if (!flags[i])
-         continue;
-
-      assert(submit.bo_handle_count < batch->num_bos);
-      bo_handles[submit.bo_handle_count++] = i;
-
-      /* Update the BO access flags so that panfrost_bo_wait() knows
-       * about all pending accesses.
-       * We only keep the READ/WRITE info since this is all the BO
-       * wait logic cares about.
-       * We also preserve existing flags as this batch might not
-       * be the first one to access the BO.
-       */
-      struct panfrost_bo *bo = pan_lookup_bo(dev, i);
-
-      bo->gpu_access |= flags[i] & (PAN_BO_ACCESS_RW);
-   }
-
-   panfrost_pool_get_bo_handles(&batch->pool,
-                                bo_handles + submit.bo_handle_count);
-   submit.bo_handle_count += panfrost_pool_num_bos(&batch->pool);
-   panfrost_pool_get_bo_handles(&batch->invisible_pool,
-                                bo_handles + submit.bo_handle_count);
-   submit.bo_handle_count += panfrost_pool_num_bos(&batch->invisible_pool);
-
-   /* Add the tiler heap to the list of accessed BOs if the batch has at
-    * least one tiler job. Tiler heap is written by tiler jobs and read
-    * by fragment jobs (the polygon list is coming from this heap).
-    */
-   if (batch->scoreboard.first_tiler)
-      bo_handles[submit.bo_handle_count++] = dev->tiler_heap->gem_handle;
-
-   /* Always used on Bifrost, occassionally used on Midgard */
-   bo_handles[submit.bo_handle_count++] = dev->sample_positions->gem_handle;
-
-   submit.bo_handles = (u64)(uintptr_t)bo_handles;
-   if (ctx->is_noop)
-      ret = 0;
-   else
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
-   free(bo_handles);
-
-   if (ret)
-      return errno;
-
-   /* Trace the job if we're doing that */
-   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
-      /* Wait so we can get errors reported back */
-      drmSyncobjWait(dev->fd, &out_sync, 1, INT64_MAX, 0, NULL);
-
-      if (dev->debug & PAN_DBG_TRACE)
-         pandecode_jc(dev->decode_ctx, submit.jc, dev->gpu_id);
-
-      if (dev->debug & PAN_DBG_DUMP)
-         pandecode_dump_mappings(dev->decode_ctx);
-
-      /* Jobs won't be complete if blackhole rendering, that's ok */
-      if (!ctx->is_noop && dev->debug & PAN_DBG_SYNC)
-         pandecode_abort_on_fault(dev->decode_ctx, submit.jc, dev->gpu_id);
-   }
-
-   return 0;
-}
-
-static bool
-panfrost_has_fragment_job(struct panfrost_batch *batch)
-{
-   return batch->scoreboard.first_tiler || batch->clear;
-}
-
-/* Submit both vertex/tiler and fragment jobs for a batch, possibly with an
- * outsync corresponding to the later of the two (since there will be an
- * implicit dep between them) */
-
-static int
-panfrost_batch_submit_jobs(struct panfrost_batch *batch,
-                           const struct pan_fb_info *fb, uint32_t in_sync,
-                           uint32_t out_sync)
-{
-   struct pipe_screen *pscreen = batch->ctx->base.screen;
-   struct panfrost_screen *screen = pan_screen(pscreen);
-   struct panfrost_device *dev = pan_device(pscreen);
-   bool has_draws = batch->scoreboard.first_job;
-   bool has_tiler = batch->scoreboard.first_tiler;
-   bool has_frag = panfrost_has_fragment_job(batch);
-   int ret = 0;
-
-   /* Take the submit lock to make sure no tiler jobs from other context
-    * are inserted between our tiler and fragment jobs, failing to do that
-    * might result in tiler heap corruption.
-    */
-   if (has_tiler)
-      pthread_mutex_lock(&dev->submit_lock);
-
-   if (has_draws) {
-      ret = panfrost_batch_submit_ioctl(batch, batch->scoreboard.first_job, 0,
-                                        in_sync, has_frag ? 0 : out_sync);
-
-      if (ret)
-         goto done;
-   }
-
-   if (has_frag) {
-      mali_ptr fragjob = screen->vtbl.emit_fragment_job(batch, fb);
-      ret = panfrost_batch_submit_ioctl(batch, fragjob, PANFROST_JD_REQ_FS, 0,
-                                        out_sync);
-      if (ret)
-         goto done;
-   }
-
-done:
-   if (has_tiler)
-      pthread_mutex_unlock(&dev->submit_lock);
-
-   return ret;
-}
-
-static void
-panfrost_emit_tile_map(struct panfrost_batch *batch, struct pan_fb_info *fb)
-{
-   if (batch->key.nr_cbufs < 1 || !batch->key.cbufs[0])
-      return;
-
-   struct pipe_surface *surf = batch->key.cbufs[0];
-   struct panfrost_resource *pres = surf ? pan_resource(surf->texture) : NULL;
-
-   if (pres && pres->damage.tile_map.enable) {
-      fb->tile_map.base =
-         pan_pool_upload_aligned(&batch->pool.base, pres->damage.tile_map.data,
-                                 pres->damage.tile_map.size, 64);
-      fb->tile_map.stride = pres->damage.tile_map.stride;
-   }
-}
-
 static void
 panfrost_batch_submit(struct panfrost_context *ctx,
                       struct panfrost_batch *batch)
 {
    struct pipe_screen *pscreen = ctx->base.screen;
    struct panfrost_screen *screen = pan_screen(pscreen);
+   bool has_frag = panfrost_has_fragment_job(batch);
    int ret;
 
    /* Nothing to do! */
-   if (!batch->scoreboard.first_job && !batch->clear)
+   if (!has_frag && !batch->any_compute)
       goto out;
 
-   if (batch->key.zsbuf && panfrost_has_fragment_job(batch)) {
+   if (batch->key.zsbuf && has_frag) {
       struct pipe_surface *surf = batch->key.zsbuf;
       struct panfrost_resource *z_rsrc = pan_resource(surf->texture);
 
@@ -814,20 +628,7 @@ panfrost_batch_submit(struct panfrost_co
 
    panfrost_batch_to_fb_info(batch, &fb, rts, &zs, &s, false);
 
-   screen->vtbl.preload(batch, &fb);
-   screen->vtbl.init_polygon_list(batch);
-
-   /* Now that all draws are in, we can finally prepare the
-    * FBD for the batch (if there is one). */
-
-   screen->vtbl.emit_tls(batch);
-   panfrost_emit_tile_map(batch, &fb);
-
-   if (batch->scoreboard.first_tiler || batch->clear)
-      screen->vtbl.emit_fbd(batch, &fb);
-
-   ret = panfrost_batch_submit_jobs(batch, &fb, 0, ctx->syncobj);
-
+   ret = screen->vtbl.submit_batch(batch, &fb);
    if (ret)
       fprintf(stderr, "panfrost_batch_submit failed: %d\n", ret);
 
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_job.h	2023-11-24 23:33:24.962610829 +0100
@@ -29,6 +29,8 @@
 #include "pipe/p_state.h"
 #include "util/u_dynarray.h"
 #include "pan_cs.h"
+#include "pan_csf.h"
+#include "pan_jm.h"
 #include "pan_mempool.h"
 #include "pan_resource.h"
 #include "pan_scoreboard.h"
@@ -82,6 +84,8 @@ pan_tristate_get(struct pan_tristate sta
 /* A panfrost_batch corresponds to a bound FBO we're rendering to,
  * collecting over multiple draws. */
 
+struct ceu_builder;
+
 struct panfrost_batch {
    struct panfrost_context *ctx;
    struct pipe_framebuffer_state key;
@@ -102,6 +106,8 @@ struct panfrost_batch {
    /* Buffers needing resolve to memory */
    unsigned resolve;
 
+   bool any_compute;
+
    /* Packed clear values, indexed by both render target as well as word.
     * Essentially, a single pixel is packed, with some padding to bring it
     * up to a 32-bit interval; that pixel is then duplicated over to fill
@@ -139,12 +145,6 @@ struct panfrost_batch {
     * varyings */
    struct panfrost_pool invisible_pool;
 
-   /* Job scoreboarding state */
-   struct pan_scoreboard scoreboard;
-
-   /* Polygon list bound to the batch, or NULL if none bound yet */
-   struct panfrost_bo *polygon_list;
-
    /* Scratchpad BO bound to the batch, or NULL if none bound yet */
    struct panfrost_bo *scratchpad;
 
@@ -178,6 +178,19 @@ struct panfrost_batch {
    unsigned nr_push_uniforms[PIPE_SHADER_TYPES];
    unsigned nr_uniform_buffers[PIPE_SHADER_TYPES];
 
+   /* Varying related pointers */
+   struct {
+      mali_ptr bufs;
+      unsigned nr_bufs;
+      mali_ptr vs;
+      mali_ptr fs;
+      mali_ptr pos;
+      mali_ptr psiz;
+   } varyings;
+
+   /* Index array */
+   mali_ptr indices;
+
    /* Valhall: struct mali_scissor_packed */
    unsigned scissor[2];
    float minimum_z, maximum_z;
@@ -192,6 +205,14 @@ struct panfrost_batch {
     */
    struct pan_tristate sprite_coord_origin;
    struct pan_tristate first_provoking_vertex;
+
+   uint32_t draw_count;
+
+   /* Job frontend specific fields. */
+   union {
+      struct panfrost_jm_batch jm;
+      struct panfrost_csf_batch csf;
+   };
 };
 
 /* Functions for managing the above */
@@ -258,4 +279,10 @@ void panfrost_batch_union_scissor(struct
 
 bool panfrost_batch_skip_rasterization(struct panfrost_batch *batch);
 
+static inline bool
+panfrost_has_fragment_job(struct panfrost_batch *batch)
+{
+   return batch->draw_count > 0 || batch->clear;
+}
+
 #endif
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_mempool.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_mempool.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_mempool.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_mempool.c	2023-11-24 23:33:24.962610829 +0100
@@ -104,8 +104,8 @@ panfrost_pool_get_bo_handles(struct panf
 
    unsigned idx = 0;
    util_dynarray_foreach(&pool->bos, struct panfrost_bo *, bo) {
-      assert((*bo)->gem_handle > 0);
-      handles[idx++] = (*bo)->gem_handle;
+      assert(panfrost_bo_handle(*bo) > 0);
+      handles[idx++] = panfrost_bo_handle(*bo);
 
       /* Update the BO access flags so that panfrost_bo_wait() knows
        * about all pending accesses.
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.c	2023-11-24 23:33:24.962610829 +0100
@@ -4,7 +4,6 @@
  * Copyright (C) 2014-2017 Broadcom
  * Copyright (C) 2018-2019 Alyssa Rosenzweig
  * Copyright (C) 2019 Collabora, Ltd.
- * Copyright (C) 2023 Amazon.com, Inc. or its affiliates
  *
  * Permission is hereby granted, free of charge, to any person obtaining a
  * copy of this software and associated documentation files (the "Software"),
@@ -37,7 +36,6 @@
 
 #include "frontend/winsys_handle.h"
 #include "util/format/u_format.h"
-#include "util/u_debug_image.h"
 #include "util/u_drm.h"
 #include "util/u_gen_mipmap.h"
 #include "util/u_memory.h"
@@ -194,7 +192,7 @@ panfrost_resource_get_handle(struct pipe
    if (handle->type == WINSYS_HANDLE_TYPE_KMS && dev->ro) {
       return renderonly_get_handle(scanout, handle);
    } else if (handle->type == WINSYS_HANDLE_TYPE_KMS) {
-      handle->handle = rsrc->image.data.bo->gem_handle;
+      handle->handle = panfrost_bo_handle(rsrc->image.data.bo);
    } else if (handle->type == WINSYS_HANDLE_TYPE_FD) {
       int fd = panfrost_bo_export(rsrc->image.data.bo);
 
@@ -251,8 +249,11 @@ static struct pipe_surface *
 panfrost_create_surface(struct pipe_context *pipe, struct pipe_resource *pt,
                         const struct pipe_surface *surf_tmpl)
 {
+   struct panfrost_context *ctx = pan_context(pipe);
    struct pipe_surface *ps = NULL;
 
+   pan_legalize_afbc_format(ctx, pan_resource(pt), surf_tmpl->format);
+
    ps = CALLOC_STRUCT(pipe_surface);
 
    if (ps) {
@@ -381,23 +382,7 @@ panfrost_should_tile_afbc(const struct p
                           const struct panfrost_resource *pres)
 {
    return panfrost_afbc_can_tile(dev) && pres->base.width0 >= 128 &&
-          pres->base.height0 >= 128 && !(dev->debug & PAN_DBG_FORCE_PACK);
-}
-
-bool
-panfrost_should_pack_afbc(struct panfrost_device *dev,
-                          const struct panfrost_resource *prsrc)
-{
-   const unsigned valid_binding = PIPE_BIND_DEPTH_STENCIL |
-                                  PIPE_BIND_RENDER_TARGET |
-                                  PIPE_BIND_SAMPLER_VIEW;
-
-   return panfrost_afbc_can_pack(prsrc->base.format) && panfrost_is_2d(prsrc) &&
-          drm_is_afbc(prsrc->image.layout.modifier) &&
-          (prsrc->image.layout.modifier & AFBC_FORMAT_MOD_SPARSE) &&
-          (prsrc->base.bind & ~valid_binding) == 0 &&
-          !prsrc->modifier_constant && prsrc->base.width0 >= 32 &&
-          prsrc->base.height0 >= 32;
+          pres->base.height0 >= 128;
 }
 
 static bool
@@ -676,7 +661,12 @@ panfrost_resource_create_with_modifier(s
                        : (bind & PIPE_BIND_SHADER_IMAGE)    ? "Shader image"
                                                             : "Other resource";
 
-   if (dev->ro && (template->bind & PIPE_BIND_SCANOUT)) {
+   /* Set up the "scanout resource". If the buffer might ever have
+    * resource_get_handle(WINSYS_HANDLE_TYPE_KMS) called on it.
+    * create_with_modifiers() doesn't give us usage flags, so we have to
+    * assume that all calls with PIPE_BIND_SHARED are scanout-possible.
+    */
+   if (dev->ro && (template->bind & PAN_BIND_SHARED_MASK)) {
       struct winsys_handle handle;
       struct pan_block_size blocksize =
          panfrost_block_size(modifier, template->format);
@@ -737,9 +727,14 @@ panfrost_resource_create_with_modifier(s
    } else {
       /* We create a BO immediately but don't bother mapping, since we don't
        * care to map e.g. FBOs which the CPU probably won't touch */
+      uint32_t flags = PAN_BO_DELAY_MMAP;
 
-      so->image.data.bo = panfrost_bo_create(dev, so->image.layout.data_size,
-                                             PAN_BO_DELAY_MMAP, label);
+      /* If the resource is never exported, we can make the BO private. */
+      if (template->bind & PAN_BIND_SHARED_MASK)
+         flags |= PAN_BO_SHAREABLE;
+
+      so->image.data.bo =
+         panfrost_bo_create(dev, so->image.layout.data_size, flags, label);
 
       so->constant_stencil = true;
    }
@@ -914,131 +909,6 @@ panfrost_load_tiled_images(struct panfro
    }
 }
 
-#ifdef DEBUG
-
-static unsigned
-get_superblock_size(uint32_t *hdr, unsigned uncompressed_size)
-{
-   /* AFBC superblock layout 0 */
-   unsigned body_base_ptr_len = 32;
-   unsigned nr_subblocks = 16;
-   unsigned sz_len = 6; /* bits */
-   unsigned mask = (1 << sz_len) - 1;
-   unsigned size = 0;
-
-   /* Sum up all of the subblock sizes */
-   for (int i = 0; i < nr_subblocks; i++) {
-      unsigned bitoffset = body_base_ptr_len + (i * sz_len);
-      unsigned start = bitoffset / 32;
-      unsigned end = (bitoffset + (sz_len - 1)) / 32;
-      unsigned offset = bitoffset % 32;
-      unsigned subblock_size;
-
-      if (start != end)
-         subblock_size = (hdr[start] >> offset) | (hdr[end] << (32 - offset));
-      else
-         subblock_size = hdr[start] >> offset;
-      subblock_size = (subblock_size == 1) ? uncompressed_size : subblock_size;
-      size += subblock_size & mask;
-
-      if (i == 0 && size == 0)
-         return 0;
-   }
-
-   return size;
-}
-
-static void
-dump_block(struct panfrost_resource *rsrc, uint32_t idx)
-{
-   panfrost_bo_wait(rsrc->image.data.bo, INT64_MAX, false);
-
-   uint8_t *ptr = rsrc->image.data.bo->ptr.cpu;
-   uint32_t *header = (uint32_t *)(ptr + (idx * AFBC_HEADER_BYTES_PER_TILE));
-   uint32_t body_base_ptr = header[0];
-   uint32_t *body = (uint32_t *)(ptr + body_base_ptr);
-   struct pan_block_size block_sz =
-      panfrost_afbc_subblock_size(rsrc->image.layout.modifier);
-   unsigned pixel_sz = util_format_get_blocksize(rsrc->base.format);
-   unsigned uncompressed_size = pixel_sz * block_sz.width * block_sz.height;
-   unsigned size = get_superblock_size(header, uncompressed_size);
-
-   fprintf(stderr, "  Header: %08x %08x %08x %08x (size: %u bytes)\n",
-           header[0], header[1], header[2], header[3], size);
-   if (size > 0) {
-      fprintf(stderr, "  Body:   %08x %08x %08x %08x\n", body[0], body[1],
-              body[2], body[3]);
-   } else {
-      uint8_t *comp = (uint8_t *)(header + 2);
-      fprintf(stderr, "  Color:  0x%02x%02x%02x%02x\n", comp[0], comp[1],
-              comp[2], comp[3]);
-   }
-   fprintf(stderr, "\n");
-}
-
-void
-pan_dump_resource(struct panfrost_context *ctx, struct panfrost_resource *rsc)
-{
-   struct pipe_context *pctx = &ctx->base;
-   struct pipe_resource tmpl = rsc->base;
-   struct pipe_resource *plinear = NULL;
-   struct panfrost_resource *linear = rsc;
-   struct pipe_blit_info blit = {0};
-   struct pipe_box box;
-   char buffer[1024];
-
-   if (rsc->image.layout.modifier != DRM_FORMAT_MOD_LINEAR) {
-      tmpl.bind |= PIPE_BIND_LINEAR;
-      tmpl.bind &= ~PAN_BIND_SHARED_MASK;
-
-      plinear = pctx->screen->resource_create(pctx->screen, &tmpl);
-      u_box_2d(0, 0, rsc->base.width0, rsc->base.height0, &box);
-
-      blit.src.resource = &rsc->base;
-      blit.src.format = rsc->base.format;
-      blit.src.level = 0;
-      blit.src.box = box;
-      blit.dst.resource = plinear;
-      blit.dst.format = rsc->base.format;
-      blit.dst.level = 0;
-      blit.dst.box = box;
-      blit.mask = util_format_get_mask(blit.dst.format);
-      blit.filter = PIPE_TEX_FILTER_NEAREST;
-
-      panfrost_blit(pctx, &blit);
-
-      linear = pan_resource(plinear);
-   }
-
-   panfrost_flush_writer(ctx, linear, "dump image");
-   panfrost_bo_wait(linear->image.data.bo, INT64_MAX, false);
-   panfrost_bo_mmap(linear->image.data.bo);
-
-   static unsigned frame_count = 0;
-   frame_count++;
-   snprintf(buffer, sizeof(buffer), "dump_image.%04d", frame_count);
-
-   debug_dump_image(buffer, rsc->base.format, 0 /* UNUSED */, rsc->base.width0,
-                    rsc->base.height0,
-                    linear->image.layout.slices[0].row_stride,
-                    linear->image.data.bo->ptr.cpu);
-
-   if (plinear)
-      pipe_resource_reference(&plinear, NULL);
-}
-
-#endif
-
-/* Get scan-order index from (x, y) position when blocks are
- * arranged in z-order in 8x8 tiles */
-static unsigned
-get_morton_index(unsigned x, unsigned y, unsigned stride)
-{
-   unsigned i = ((x << 0) & 1) | ((y << 1) & 2) | ((x << 1) & 4) |
-                ((y << 2) & 8) | ((x << 2) & 16) | ((y << 3) & 32);
-   return (((y & ~7) * stride) + ((x & ~7) << 3)) + i;
-}
-
 static void
 panfrost_store_tiled_images(struct panfrost_transfer *transfer,
                             struct panfrost_resource *rsrc)
@@ -1140,9 +1010,10 @@ panfrost_ptr_map(struct pipe_context *pc
    /* If we haven't already mmaped, now's the time */
    panfrost_bo_mmap(bo);
 
-   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-      pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu, bo->size,
-                            NULL);
+   if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
+      pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu,
+                            panfrost_bo_size(bo), NULL);
+   }
 
    /* Upgrade writes to uninitialized ranges to UNSYNCHRONIZED */
    if ((usage & PIPE_MAP_WRITE) && resource->target == PIPE_BUFFER &&
@@ -1208,12 +1079,16 @@ panfrost_ptr_map(struct pipe_context *pc
           * importer/exporter wouldn't see the change we're
           * doing to it.
           */
-         if (!(bo->flags & PAN_BO_SHARED))
-            newbo = panfrost_bo_create(dev, bo->size, flags, bo->label);
+         if (!(bo->flags & PAN_BO_SHARED)) {
+            newbo =
+               panfrost_bo_create(dev, panfrost_bo_size(bo), flags, bo->label);
+         }
 
          if (newbo) {
-            if (copy_resource)
-               memcpy(newbo->ptr.cpu, rsrc->image.data.bo->ptr.cpu, bo->size);
+            if (copy_resource) {
+               memcpy(newbo->ptr.cpu, rsrc->image.data.bo->ptr.cpu,
+                      panfrost_bo_size(bo));
+            }
 
             /* Swap the pointers, dropping a reference to
              * the old BO which is no long referenced from
@@ -1302,8 +1177,7 @@ pan_resource_modifier_convert(struct pan
 {
    assert(!rsrc->modifier_constant);
 
-   perf_debug_ctx(ctx, "%s AFBC with a blit. Reason: %s",
-                  drm_is_afbc(modifier) ? "Unpacking" : "Disabling", reason);
+   perf_debug_ctx(ctx, "Disabling AFBC with a blit. Reason: %s", reason);
 
    struct pipe_resource *tmp_prsrc = panfrost_resource_create_with_modifier(
       ctx->base.screen, &rsrc->base, modifier);
@@ -1355,25 +1229,20 @@ pan_resource_modifier_convert(struct pan
 void
 pan_legalize_afbc_format(struct panfrost_context *ctx,
                          struct panfrost_resource *rsrc,
-                         enum pipe_format format, bool write)
+                         enum pipe_format format)
 {
    struct panfrost_device *dev = pan_device(ctx->base.screen);
 
    if (!drm_is_afbc(rsrc->image.layout.modifier))
       return;
 
-   if (panfrost_afbc_format(dev->arch, rsrc->base.format) !=
-       panfrost_afbc_format(dev->arch, format)) {
-      pan_resource_modifier_convert(
-         ctx, rsrc, DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED,
-         "Reinterpreting AFBC surface as incompatible format");
+   if (panfrost_afbc_format(dev->arch, rsrc->base.format) ==
+       panfrost_afbc_format(dev->arch, format))
       return;
-   }
 
-   if (write && (rsrc->image.layout.modifier & AFBC_FORMAT_MOD_SPARSE) == 0)
-      pan_resource_modifier_convert(
-         ctx, rsrc, rsrc->image.layout.modifier | AFBC_FORMAT_MOD_SPARSE,
-         "Legalizing resource to allow writing");
+   pan_resource_modifier_convert(
+      ctx, rsrc, DRM_FORMAT_MOD_ARM_16X16_BLOCK_U_INTERLEAVED,
+      "Reinterpreting AFBC surface as incompatible format");
 }
 
 static bool
@@ -1412,148 +1281,11 @@ panfrost_should_linear_convert(struct pa
    }
 }
 
-struct panfrost_bo *
-panfrost_get_afbc_superblock_sizes(struct panfrost_context *ctx,
-                                   struct panfrost_resource *rsrc,
-                                   unsigned first_level, unsigned last_level,
-                                   unsigned *out_offsets)
-{
-   struct panfrost_screen *screen = pan_screen(ctx->base.screen);
-   struct panfrost_device *dev = pan_device(ctx->base.screen);
-   struct panfrost_batch *batch;
-   struct panfrost_bo *bo;
-   unsigned metadata_size = 0;
-
-   for (int level = first_level; level <= last_level; ++level) {
-      struct pan_image_slice_layout *slice = &rsrc->image.layout.slices[level];
-      unsigned sz = slice->afbc.nr_blocks * sizeof(struct pan_afbc_block_info);
-      out_offsets[level - first_level] = metadata_size;
-      metadata_size += sz;
-   }
-
-   panfrost_flush_batches_accessing_rsrc(ctx, rsrc, "AFBC before size flush");
-   batch = panfrost_get_fresh_batch_for_fbo(ctx, "AFBC superblock sizes");
-   bo = panfrost_bo_create(dev, metadata_size, 0, "AFBC superblock sizes");
-
-   for (int level = first_level; level <= last_level; ++level) {
-      unsigned offset = out_offsets[level - first_level];
-      screen->vtbl.afbc_size(batch, rsrc, bo, offset, level);
-   }
-
-   panfrost_flush_batches_accessing_rsrc(ctx, rsrc, "AFBC after size flush");
-
-   return bo;
-}
-
-void
-panfrost_pack_afbc(struct panfrost_context *ctx,
-                   struct panfrost_resource *prsrc)
-{
-   struct panfrost_screen *screen = pan_screen(ctx->base.screen);
-   struct panfrost_device *dev = pan_device(ctx->base.screen);
-   struct panfrost_bo *metadata_bo;
-   unsigned metadata_offsets[PIPE_MAX_TEXTURE_LEVELS];
-
-   uint64_t src_modifier = prsrc->image.layout.modifier;
-   uint64_t dst_modifier =
-      src_modifier & ~(AFBC_FORMAT_MOD_TILED | AFBC_FORMAT_MOD_SPARSE);
-   bool is_tiled = src_modifier & AFBC_FORMAT_MOD_TILED;
-   unsigned last_level = prsrc->base.last_level;
-   struct pan_image_slice_layout slice_infos[PIPE_MAX_TEXTURE_LEVELS] = {0};
-   unsigned total_size = 0;
-
-   /* It doesn't make sense to pack everything if we need to unpack right
-    * away to upload data to another level */
-   for (int i = 0; i <= last_level; i++) {
-      if (!BITSET_TEST(prsrc->valid.data, i))
-         return;
-   }
-
-   metadata_bo = panfrost_get_afbc_superblock_sizes(ctx, prsrc, 0, last_level,
-                                                    metadata_offsets);
-   panfrost_bo_wait(metadata_bo, INT64_MAX, false);
-
-   for (unsigned level = 0; level <= last_level; ++level) {
-      struct pan_image_slice_layout *src_slice =
-         &prsrc->image.layout.slices[level];
-      struct pan_image_slice_layout *dst_slice = &slice_infos[level];
-
-      unsigned width = u_minify(prsrc->base.width0, level);
-      unsigned height = u_minify(prsrc->base.height0, level);
-      unsigned src_stride =
-         pan_afbc_stride_blocks(src_modifier, src_slice->row_stride);
-      unsigned dst_stride =
-         DIV_ROUND_UP(width, panfrost_afbc_superblock_width(dst_modifier));
-      unsigned dst_height =
-         DIV_ROUND_UP(height, panfrost_afbc_superblock_height(dst_modifier));
-
-      uint32_t offset = 0;
-      struct pan_afbc_block_info *meta =
-         metadata_bo->ptr.cpu + metadata_offsets[level];
-
-      for (unsigned y = 0, i = 0; y < dst_height; ++y) {
-         for (unsigned x = 0; x < dst_stride; ++x, ++i) {
-            unsigned idx = is_tiled ? get_morton_index(x, y, src_stride) : i;
-            uint32_t size = meta[idx].size;
-            meta[idx].offset = offset; /* write the start offset */
-            offset += size;
-         }
-      }
-
-      total_size = ALIGN_POT(total_size, pan_slice_align(dst_modifier));
-      {
-         dst_slice->afbc.stride = dst_stride;
-         dst_slice->afbc.nr_blocks = dst_stride * dst_height;
-         dst_slice->afbc.header_size =
-            ALIGN_POT(dst_stride * dst_height * AFBC_HEADER_BYTES_PER_TILE,
-                      pan_afbc_body_align(dst_modifier));
-         dst_slice->afbc.body_size = offset;
-         dst_slice->afbc.surface_stride = dst_slice->afbc.header_size + offset;
-
-         dst_slice->offset = total_size;
-         dst_slice->row_stride = dst_stride * AFBC_HEADER_BYTES_PER_TILE;
-         dst_slice->surface_stride = dst_slice->afbc.surface_stride;
-         dst_slice->size = dst_slice->afbc.surface_stride;
-      }
-      total_size += dst_slice->afbc.surface_stride;
-   }
-
-   unsigned new_size = ALIGN_POT(total_size, 4096); // FIXME
-   unsigned old_size = prsrc->image.data.bo->size;
-   unsigned ratio = 100 * new_size / old_size;
-
-   if (ratio > screen->max_afbc_packing_ratio)
-      return;
-
-   perf_debug(dev, "%i%%: %i KB -> %i KB\n", ratio, old_size / 1024,
-              new_size / 1024);
-
-   struct panfrost_bo *dst =
-      panfrost_bo_create(dev, new_size, 0, "AFBC compact texture");
-   struct panfrost_batch *batch =
-      panfrost_get_fresh_batch_for_fbo(ctx, "AFBC compaction");
-
-   for (unsigned level = 0; level <= last_level; ++level) {
-      struct pan_image_slice_layout *slice = &slice_infos[level];
-      screen->vtbl.afbc_pack(batch, prsrc, dst, slice, metadata_bo,
-                             metadata_offsets[level], level);
-      prsrc->image.layout.slices[level] = *slice;
-   }
-
-   panfrost_flush_batches_accessing_rsrc(ctx, prsrc, "AFBC compaction flush");
-
-   prsrc->image.layout.modifier = dst_modifier;
-   panfrost_bo_unreference(prsrc->image.data.bo);
-   prsrc->image.data.bo = dst;
-   panfrost_bo_unreference(metadata_bo);
-}
-
 static void
 panfrost_ptr_unmap(struct pipe_context *pctx, struct pipe_transfer *transfer)
 {
    /* Gallium expects writeback here, so we tile */
 
-   struct panfrost_context *ctx = pan_context(pctx);
    struct panfrost_transfer *trans = pan_transfer(transfer);
    struct panfrost_resource *prsrc =
       (struct panfrost_resource *)transfer->resource;
@@ -1582,13 +1314,8 @@ panfrost_ptr_unmap(struct pipe_context *
          } else {
             pan_blit_from_staging(pctx, trans);
             panfrost_flush_batches_accessing_rsrc(
-               ctx, pan_resource(trans->staging.rsrc),
+               pan_context(pctx), pan_resource(trans->staging.rsrc),
                "AFBC write staging blit");
-
-            if (dev->debug & PAN_DBG_FORCE_PACK) {
-               if (panfrost_should_pack_afbc(dev, prsrc))
-                  panfrost_pack_afbc(ctx, prsrc);
-            }
          }
       }
 
@@ -1607,11 +1334,13 @@ panfrost_ptr_unmap(struct pipe_context *
             if (panfrost_should_linear_convert(dev, prsrc, transfer)) {
                panfrost_resource_setup(dev, prsrc, DRM_FORMAT_MOD_LINEAR,
                                        prsrc->image.layout.format);
-               if (prsrc->image.layout.data_size > bo->size) {
+               if (prsrc->image.layout.data_size > panfrost_bo_size(bo)) {
+                  uint32_t flags = bo->flags & ~PAN_BO_DELAY_MMAP;
                   const char *label = bo->label;
+
                   panfrost_bo_unreference(bo);
                   bo = prsrc->image.data.bo = panfrost_bo_create(
-                     dev, prsrc->image.layout.data_size, 0, label);
+                     dev, prsrc->image.layout.data_size, flags, label);
                   assert(bo);
                }
 
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_resource.h	2023-11-24 23:33:24.962610829 +0100
@@ -177,24 +177,12 @@ panfrost_resource_create_with_modifier(s
                                        const struct pipe_resource *template,
                                        uint64_t modifier);
 
-struct panfrost_bo *panfrost_get_afbc_superblock_sizes(
-   struct panfrost_context *ctx, struct panfrost_resource *rsrc,
-   unsigned first_level, unsigned last_level, unsigned *out_offsets);
-
-bool panfrost_should_pack_afbc(struct panfrost_device *dev,
-                               const struct panfrost_resource *rsrc);
-
-void panfrost_pack_afbc(struct panfrost_context *ctx,
-                        struct panfrost_resource *prsrc);
-
 void pan_resource_modifier_convert(struct panfrost_context *ctx,
                                    struct panfrost_resource *rsrc,
                                    uint64_t modifier, const char *reason);
 
 void pan_legalize_afbc_format(struct panfrost_context *ctx,
                               struct panfrost_resource *rsrc,
-                              enum pipe_format format, bool write);
-void pan_dump_resource(struct panfrost_context *ctx,
-                       struct panfrost_resource *rsc);
+                              enum pipe_format format);
 
 #endif /* PAN_RESOURCE_H */
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.c.8~	2023-11-24 23:33:24.937610687 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.c	2023-11-24 23:33:24.962610829 +0100
@@ -43,6 +43,8 @@
 #include "drm-uapi/drm_fourcc.h"
 #include "drm-uapi/panfrost_drm.h"
 
+#include "genxml/ceu_builder.h"
+
 #include "decode.h"
 #include "pan_bo.h"
 #include "pan_fence.h"
@@ -54,8 +56,6 @@
 
 #include "pan_context.h"
 
-#define DEFAULT_MAX_AFBC_PACKING_RATIO 90
-
 /* clang-format off */
 static const struct debug_named_value panfrost_debug_options[] = {
    {"perf",       PAN_DBG_PERF,     "Enable performance warnings"},
@@ -74,7 +74,6 @@ static const struct debug_named_value pa
    {"overflow",   PAN_DBG_OVERFLOW, "Check for buffer overflows in pool uploads"},
 #endif
    {"yuv",        PAN_DBG_YUV,      "Tint YUV textures with blue for 1-plane and green for 2-plane"},
-   {"forcepack",  PAN_DBG_FORCE_PACK,  "Force packing of AFBC textures on upload"},
    DEBUG_NAMED_VALUE_END
 };
 /* clang-format on */
@@ -109,8 +108,8 @@ panfrost_get_param(struct pipe_screen *s
    bool has_mrt = (dev->arch >= 5);
 
    /* Only kernel drivers >= 1.1 can allocate HEAP BOs */
-   bool has_heap = dev->kernel_version->version_major > 1 ||
-                   dev->kernel_version->version_minor >= 1;
+   bool has_heap = panfrost_device_kmod_version_major(dev) > 1 ||
+                   panfrost_device_kmod_version_minor(dev) >= 1;
 
    switch (param) {
    case PIPE_CAP_NPOT_TEXTURES:
@@ -143,7 +142,7 @@ panfrost_get_param(struct pipe_screen *s
       return true;
 
    case PIPE_CAP_ANISOTROPIC_FILTER:
-      return dev->revision >= dev->model->min_rev_anisotropic;
+      return panfrost_device_gpu_rev(dev) >= dev->model->min_rev_anisotropic;
 
    /* Compile side is done for Bifrost, Midgard TODO. Needs some kernel
     * work to turn on, since CYCLE_COUNT_START needs to be issued. In
@@ -807,7 +806,7 @@ panfrost_get_disk_shader_cache(struct pi
 static int
 panfrost_get_screen_fd(struct pipe_screen *pscreen)
 {
-   return pan_device(pscreen)->fd;
+   return panfrost_device_fd(pan_device(pscreen));
 }
 
 int
@@ -842,8 +841,6 @@ panfrost_create_screen(int fd, const str
    /* Debug must be set first for pandecode to work correctly */
    dev->debug =
       debug_get_flags_option("PAN_MESA_DEBUG", panfrost_debug_options, 0);
-   screen->max_afbc_packing_ratio = debug_get_num_option(
-      "PAN_MAX_AFBC_PACKING_RATIO", DEFAULT_MAX_AFBC_PACKING_RATIO);
    panfrost_open_device(screen, fd, dev);
 
    if (dev->debug & PAN_DBG_NO_AFBC)
@@ -851,7 +848,8 @@ panfrost_create_screen(int fd, const str
 
    /* Bail early on unsupported hardware */
    if (dev->model == NULL) {
-      debug_printf("panfrost: Unsupported model %X", dev->gpu_id);
+      debug_printf("panfrost: Unsupported model %X",
+                   panfrost_device_gpu_id(dev));
       panfrost_destroy_screen(&(screen->base));
       return NULL;
    }
@@ -901,6 +899,8 @@ panfrost_create_screen(int fd, const str
       panfrost_cmdstream_screen_init_v7(screen);
    else if (dev->arch == 9)
       panfrost_cmdstream_screen_init_v9(screen);
+   else if (dev->arch == 10)
+      panfrost_cmdstream_screen_init_v10(screen);
    else
       unreachable("Unhandled architecture major");
 
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.h.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.h
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_screen.h	2023-11-24 23:33:24.962610829 +0100
@@ -41,7 +41,6 @@
 
 #include "pan_device.h"
 #include "pan_mempool.h"
-#include "pan_texture.h"
 
 #define PAN_QUERY_DRAW_CALLS (PIPE_QUERY_DRIVER_SPECIFIC + 0)
 
@@ -64,54 +63,33 @@ struct panfrost_vtable {
    void (*prepare_shader)(struct panfrost_compiled_shader *,
                           struct panfrost_pool *, bool);
 
-   /* Emits a thread local storage descriptor */
-   void (*emit_tls)(struct panfrost_batch *);
-
-   /* Emits a framebuffer descriptor */
-   void (*emit_fbd)(struct panfrost_batch *, const struct pan_fb_info *);
-
-   /* Emits a fragment job */
-   mali_ptr (*emit_fragment_job)(struct panfrost_batch *,
-                                 const struct pan_fb_info *);
-
    /* General destructor */
    void (*screen_destroy)(struct pipe_screen *);
 
-   /* Preload framebuffer */
-   void (*preload)(struct panfrost_batch *, struct pan_fb_info *);
+   /* Populate context vtable */
+   void (*context_populate_vtbl)(struct pipe_context *pipe);
 
-   /* Initialize a Gallium context */
-   void (*context_init)(struct pipe_context *pipe);
+   /* Initialize/cleanup a Gallium context */
+   void (*context_init)(struct panfrost_context *ctx);
+   void (*context_cleanup)(struct panfrost_context *ctx);
 
-   /* Device-dependent initialization of a panfrost_batch */
+   /* Device-dependent initialization/cleanup of a panfrost_batch */
    void (*init_batch)(struct panfrost_batch *batch);
+   void (*cleanup_batch)(struct panfrost_batch *batch);
+
+   /* Device-dependent submission of a panfrost_batch */
+   int (*submit_batch)(struct panfrost_batch *batch, struct pan_fb_info *fb);
 
    /* Get blend shader */
    struct pan_blend_shader_variant *(*get_blend_shader)(
       const struct panfrost_device *, const struct pan_blend_state *,
       nir_alu_type, nir_alu_type, unsigned rt);
 
-   /* Initialize the polygon list */
-   void (*init_polygon_list)(struct panfrost_batch *);
-
    /* Shader compilation methods */
    const nir_shader_compiler_options *(*get_compiler_options)(void);
    void (*compile_shader)(nir_shader *s, struct panfrost_compile_inputs *inputs,
                           struct util_dynarray *binary,
                           struct pan_shader_info *info);
-
-   /* Run a compute shader to get the compressed size of each superblock */
-   void (*afbc_size)(struct panfrost_batch *batch,
-                     struct panfrost_resource *src,
-                     struct panfrost_bo *metadata, unsigned offset,
-                     unsigned level);
-
-   /* Run a compute shader to compact a sparse layout afbc resource */
-   void (*afbc_pack)(struct panfrost_batch *batch,
-                     struct panfrost_resource *src, struct panfrost_bo *dst,
-                     struct pan_image_slice_layout *slice,
-                     struct panfrost_bo *metadata, unsigned metadata_offset,
-                     unsigned level);
 };
 
 struct panfrost_screen {
@@ -124,7 +102,6 @@ struct panfrost_screen {
 
    struct panfrost_vtable vtbl;
    struct disk_cache *disk_cache;
-   unsigned max_afbc_packing_ratio;
 };
 
 static inline struct panfrost_screen *
@@ -147,6 +124,7 @@ void panfrost_cmdstream_screen_init_v5(s
 void panfrost_cmdstream_screen_init_v6(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v7(struct panfrost_screen *screen);
 void panfrost_cmdstream_screen_init_v9(struct panfrost_screen *screen);
+void panfrost_cmdstream_screen_init_v10(struct panfrost_screen *screen);
 
 #define perf_debug(dev, ...)                                                   \
    do {                                                                        \
diff -up mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_shader.c.8~ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_shader.c
--- mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_shader.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/drivers/panfrost/pan_shader.c	2023-11-24 23:33:24.962610829 +0100
@@ -84,11 +84,11 @@ panfrost_shader_compile(struct panfrost_
     * happens at CSO create time regardless.
     */
    if (gl_shader_stage_is_compute(s->info.stage))
-      pan_shader_preprocess(s, dev->gpu_id);
+      pan_shader_preprocess(s, panfrost_device_gpu_id(dev));
 
    struct panfrost_compile_inputs inputs = {
       .debug = dbg,
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
    };
 
    /* Lower this early so the backends don't have to worry about it */
@@ -130,7 +130,7 @@ panfrost_shader_compile(struct panfrost_
    if (dev->arch <= 5 && s->info.stage == MESA_SHADER_FRAGMENT) {
       NIR_PASS_V(s, pan_lower_framebuffer, key->fs.rt_formats,
                  pan_raw_format_mask_midgard(key->fs.rt_formats), 0,
-                 dev->gpu_id < 0x700);
+                 panfrost_device_gpu_id(dev) < 0x700);
    }
 
    NIR_PASS_V(s, panfrost_nir_lower_sysvals, &out->sysvals);
@@ -375,7 +375,7 @@ panfrost_create_shader_state(struct pipe
 
    /* Then run the suite of lowering and optimization, including I/O lowering */
    struct panfrost_device *dev = pan_device(pctx->screen);
-   pan_shader_preprocess(nir, dev->gpu_id);
+   pan_shader_preprocess(nir, panfrost_device_gpu_id(dev));
 
    /* If this shader uses transform feedback, compile the transform
     * feedback program. This is a special shader variant.
diff -up mesa-23.3.0-rc5/src/gallium/targets/dri/meson.build.8~ mesa-23.3.0-rc5/src/gallium/targets/dri/meson.build
--- mesa-23.3.0-rc5/src/gallium/targets/dri/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/targets/dri/meson.build	2023-11-24 23:33:24.962610829 +0100
@@ -101,7 +101,7 @@ foreach d : [[with_gallium_kmsro, [
              [with_gallium_softpipe and with_gallium_drisw_kms, 'kms_swrast_dri.so'],
              [with_gallium_v3d, 'v3d_dri.so'],
              [with_gallium_vc4, 'vc4_dri.so'],
-             [with_gallium_panfrost, 'panfrost_dri.so'],
+             [with_gallium_panfrost, ['panfrost_dri.so', 'panthor_dri.so']],
              [with_gallium_etnaviv, 'etnaviv_dri.so'],
              [with_gallium_tegra, 'tegra_dri.so'],
              [with_gallium_crocus, 'crocus_dri.so'],
diff -up mesa-23.3.0-rc5/src/gallium/targets/dri/target.c.8~ mesa-23.3.0-rc5/src/gallium/targets/dri/target.c
--- mesa-23.3.0-rc5/src/gallium/targets/dri/target.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/targets/dri/target.c	2023-11-24 23:33:24.962610829 +0100
@@ -82,6 +82,7 @@ DEFINE_LOADER_DRM_ENTRYPOINT(vc4)
 
 #if defined(GALLIUM_PANFROST)
 DEFINE_LOADER_DRM_ENTRYPOINT(panfrost)
+DEFINE_LOADER_DRM_ENTRYPOINT(panthor)
 #endif
 
 #if defined(GALLIUM_ASAHI)
diff -up mesa-23.3.0-rc5/src/gallium/targets/pipe-loader/meson.build.8~ mesa-23.3.0-rc5/src/gallium/targets/pipe-loader/meson.build
--- mesa-23.3.0-rc5/src/gallium/targets/pipe-loader/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/targets/pipe-loader/meson.build	2023-11-24 23:33:24.962610829 +0100
@@ -64,7 +64,7 @@ pipe_loaders = [
   [with_gallium_r600, 'r600', driver_r600, []],
   [with_gallium_radeonsi, 'radeonsi', [driver_radeonsi, idep_xmlconfig], []],
   [with_gallium_freedreno, 'msm', driver_freedreno, []],
-  [with_gallium_kmsro, 'kmsro', _kmsro_targets, [libpipe_loader_dynamic]],
+  [with_gallium_kmsro, 'kmsro', _kmsro_targets, []],
   [with_gallium_svga, 'vmwgfx', driver_svga, []],
   [with_gallium_softpipe, 'swrast', driver_swrast, [libwsw, libws_null, libswdri, libswkmsdri]],
 ]
diff -up mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_public.h.8~ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_public.h
--- mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_public.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_public.h	2023-11-24 23:33:24.962610829 +0100
@@ -30,7 +30,7 @@
 struct pipe_screen;
 struct pipe_screen_config;
 
-struct pipe_screen *kmsro_drm_screen_create(int kms_fd,
+struct pipe_screen *kmsro_drm_screen_create(int fd,
                                             const struct pipe_screen_config *config);
 
 #endif /* __KMSRO_DRM_PUBLIC_H__ */
diff -up mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c.8~ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c
--- mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/kmsro_drm_winsys.c	2023-11-24 23:33:24.962610829 +0100
@@ -36,12 +36,9 @@
 #include "xf86drm.h"
 
 #include "pipe/p_screen.h"
-#include "pipe-loader/pipe_loader.h"
 #include "renderonly/renderonly.h"
 #include "util/u_memory.h"
 
-#include "loader.h"
-
 static void kmsro_ro_destroy(struct renderonly *ro)
 {
    if (ro->gpu_fd >= 0)
@@ -52,77 +49,112 @@ static void kmsro_ro_destroy(struct rend
    FREE(ro);
 }
 
-struct pipe_screen *kmsro_drm_screen_create(int kms_fd,
+struct pipe_screen *kmsro_drm_screen_create(int fd,
                                             const struct pipe_screen_config *config)
 {
    struct pipe_screen *screen = NULL;
    struct renderonly *ro = CALLOC_STRUCT(renderonly);
-   char *render_dev_name = NULL;
 
    if (!ro)
       return NULL;
 
-   ro->kms_fd = kms_fd;
-   ro->gpu_fd = pipe_loader_get_compatible_render_capable_device_fd(kms_fd);
-   if (ro->gpu_fd < 0) {
-      FREE(ro);
-      return NULL;
-   }
-
-   render_dev_name = loader_get_kernel_driver_name(ro->gpu_fd);
-   if (!render_dev_name) {
-      close(ro->gpu_fd);
-      FREE(ro);
-      return NULL;
-   }
-
+   ro->kms_fd = fd;
+   ro->gpu_fd = -1;
    ro->destroy = kmsro_ro_destroy;
    util_sparse_array_init(&ro->bo_map, sizeof(struct renderonly_scanout), 64);
    simple_mtx_init(&ro->bo_map_lock, mtx_plain);
 
-   if (strcmp(render_dev_name, "asahi") == 0) {
+   const struct {
+      const char *name;
+      struct pipe_screen *(*create_screen)(int, struct renderonly *,
+                                           const struct pipe_screen_config *);
+      struct renderonly_scanout *(*create_for_resource)(struct pipe_resource *,
+                                                        struct renderonly *,
+                                                        struct winsys_handle *);
+   } renderonly_drivers[] = {
 #if defined(GALLIUM_ASAHI)
-      ro->create_for_resource = renderonly_create_gpu_import_for_resource;
-      screen = asahi_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "asahi",
+         .create_screen = asahi_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_gpu_import_for_resource,
+      },
 #endif
-   }
-   else if (strcmp(render_dev_name, "etnaviv") == 0) {
+
 #if defined(GALLIUM_ETNAVIV)
-      ro->create_for_resource = renderonly_create_kms_dumb_buffer_for_resource;
-      screen = etna_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "etnaviv",
+         .create_screen = etna_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_kms_dumb_buffer_for_resource,
+      },
 #endif
-   } else if (strcmp(render_dev_name, "msm") == 0) {
+
 #if defined(GALLIUM_FREEDRENO)
-      ro->create_for_resource = renderonly_create_kms_dumb_buffer_for_resource;
-      screen = fd_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "msm",
+         .create_screen = fd_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_kms_dumb_buffer_for_resource,
+      },
 #endif
-   } else if (strcmp(render_dev_name, "lima") == 0) {
+
 #if defined(GALLIUM_LIMA)
-      ro->create_for_resource = renderonly_create_kms_dumb_buffer_for_resource;
-      screen = lima_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "lima",
+         .create_screen = lima_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_kms_dumb_buffer_for_resource,
+      },
 #endif
-   } else if (strcmp(render_dev_name, "panfrost") == 0) {
+
 #if defined(GALLIUM_PANFROST)
-      ro->create_for_resource = panfrost_create_kms_dumb_buffer_for_resource;
-      screen = panfrost_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "panfrost",
+         .create_screen = panfrost_drm_screen_create_renderonly,
+         .create_for_resource = panfrost_create_kms_dumb_buffer_for_resource,
+      },
+      {
+         .name = "panthor",
+         .create_screen = panfrost_drm_screen_create_renderonly,
+         .create_for_resource = panfrost_create_kms_dumb_buffer_for_resource,
+      },
 #endif
-   } else if (strcmp(render_dev_name, "v3d") == 0) {
+
 #if defined(GALLIUM_V3D)
-      ro->create_for_resource = renderonly_create_kms_dumb_buffer_for_resource;
-      screen = v3d_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
+      {
+         .name = "v3d",
+         .create_screen = v3d_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_kms_dumb_buffer_for_resource,
+      },
 #endif
-   } else if (strcmp(render_dev_name, "vc4") == 0) {
+
 #if defined(GALLIUM_VC4)
       /* Passes the vc4-allocated BO through to the KMS-only DRM device using
        * PRIME buffer sharing.  The VC4 BO must be linear, which the SCANOUT
        * flag on allocation will have ensured.
        */
-      ro->create_for_resource = renderonly_create_gpu_import_for_resource;
-      screen = vc4_drm_screen_create_renderonly(ro->gpu_fd, ro, config);
-#endif
+      {
+         .name = "vc4",
+         .create_screen = vc4_drm_screen_create_renderonly,
+         .create_for_resource = renderonly_create_gpu_import_for_resource,
+      },
+#endif
+   };
+
+   for (int i = 0; i < ARRAY_SIZE(renderonly_drivers); i++) {
+      ro->gpu_fd = drmOpenWithType(renderonly_drivers[i].name, NULL, DRM_NODE_RENDER);
+      if (ro->gpu_fd >= 0) {
+         ro->create_for_resource = renderonly_drivers[i].create_for_resource;
+         screen = renderonly_drivers[i].create_screen(ro->gpu_fd, ro, config);
+         if (!screen)
+            goto out_free;
+         return screen;
+      }
    }
 
-   free(render_dev_name);
-
    return screen;
+
+out_free:
+   if (ro->gpu_fd >= 0)
+      close(ro->gpu_fd);
+   FREE(ro);
+
+   return NULL;
 }
diff -up mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/meson.build.8~ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/meson.build
--- mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/gallium/winsys/kmsro/drm/meson.build	2023-11-24 23:33:24.963610835 +0100
@@ -22,10 +22,9 @@ libkmsrowinsys = static_library(
   'kmsrowinsys',
   files('kmsro_drm_winsys.c'),
   include_directories : [
-    inc_src, inc_include, inc_loader,
+    inc_src, inc_include,
     inc_gallium, inc_gallium_aux, inc_gallium_winsys,
   ],
-  link_with : libloader,
   c_args : [renderonly_drivers_c_args],
   gnu_symbol_visibility : 'hidden',
   dependencies: [dep_libdrm, idep_mesautil],
diff -up mesa-23.3.0-rc5/src/loader/loader.c.8~ mesa-23.3.0-rc5/src/loader/loader.c
--- mesa-23.3.0-rc5/src/loader/loader.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/loader/loader.c	2023-11-24 23:33:24.963610835 +0100
@@ -103,8 +103,7 @@ loader_open_device(const char *device_na
    return fd;
 }
 
-char *
-loader_get_kernel_driver_name(int fd)
+static char *loader_get_kernel_driver_name(int fd)
 {
    char *driver;
    drmVersionPtr version = drmGetVersion(fd);
diff -up mesa-23.3.0-rc5/src/loader/loader.h.8~ mesa-23.3.0-rc5/src/loader/loader.h
--- mesa-23.3.0-rc5/src/loader/loader.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/loader/loader.h	2023-11-24 23:33:24.963610835 +0100
@@ -43,9 +43,6 @@ struct __DRIextensionRec;
 int
 loader_open_device(const char *);
 
-char *
-loader_get_kernel_driver_name(int fd);
-
 int
 loader_open_render_node_platform_device(const char * const drivers[],
                                         unsigned int n_drivers);
diff -up mesa-23.3.0-rc5/src/panfrost/ci/gitlab-ci.yml.8~ mesa-23.3.0-rc5/src/panfrost/ci/gitlab-ci.yml
--- mesa-23.3.0-rc5/src/panfrost/ci/gitlab-ci.yml.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/ci/gitlab-ci.yml	2023-11-24 23:33:24.963610835 +0100
@@ -11,6 +11,7 @@
       - src/panfrost/ci/*
       - src/panfrost/include/*
       - src/panfrost/lib/*
+      - src/panfrost/lib/kmod/*
       - src/panfrost/shared/*
       - src/panfrost/util/*
       when: on_success
diff -up mesa-23.3.0-rc5/src/panfrost/ci/panfrost-g52-fails.txt.8~ mesa-23.3.0-rc5/src/panfrost/ci/panfrost-g52-fails.txt
--- mesa-23.3.0-rc5/src/panfrost/ci/panfrost-g52-fails.txt.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/ci/panfrost-g52-fails.txt	2023-11-24 23:33:24.963610835 +0100
@@ -160,6 +160,7 @@ spec@arb_texture_view@sampling-2d-array-
 spec@arb_transform_feedback_instanced@draw-auto instanced,Fail
 spec@arb_uniform_buffer_object@rendering-dsa-offset,Fail
 spec@arb_uniform_buffer_object@rendering-offset,Fail
+spec@egl 1.4@egl-ext_egl_image_storage,Fail
 spec@egl 1.4@eglterminate then unbind context,Fail
 spec@egl_chromium_sync_control@conformance@eglGetSyncValuesCHROMIUM_msc_and_sbc_test,Fail
 spec@egl_chromium_sync_control@conformance,Fail
diff -up mesa-23.3.0-rc5/src/panfrost/lib/genxml/ceu_builder.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/genxml/ceu_builder.h
--- mesa-23.3.0-rc5/src/panfrost/lib/genxml/ceu_builder.h.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/genxml/ceu_builder.h	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,564 @@
+/*
+ * Copyright (C) 2022 Collabora Ltd.
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#pragma once
+
+#include "gen_macros.h"
+
+/*
+ * ceu_builder implements a builder for CSF command queues. It manages the
+ * allocation and overflow behaviour of queues and provides helpers for emitting
+ * commands to run on the CEU.
+ *
+ * Users must implement the ceu_alloc_queue method performing the physical
+ * memory allocation and queue binding. Users must initialize a queue with
+ * ceu_builder_init.
+ */
+
+struct ceu_queue {
+   /* CPU pointer */
+   uint64_t *cpu;
+
+   /* GPU pointer */
+   uint64_t gpu;
+
+   /* Capacity */
+   size_t capacity;
+};
+
+typedef struct ceu_builder {
+   /* Initial (root) queue */
+   struct ceu_queue root;
+
+   /* Number of instructions emitted into the root queue */
+   uint32_t root_size;
+
+   /* Current queue */
+   struct ceu_queue queue;
+
+   /* Number of instructions emitted into the current queue so far */
+   uint32_t queue_size;
+
+   /* Number of 32-bit registers in the hardware register file */
+   uint8_t nr_registers;
+
+   /* Move immediate instruction at the end of the last queue that needs to
+    * be patched with the final length of the current queue in order to
+    * facilitate correct overflow behaviour.
+    */
+   uint32_t *length_patch;
+
+   /* Cookie passed back to ceu_alloc_queue for caller use */
+   void *cookie;
+
+   /* CS chunk allocator. */
+   struct ceu_queue (*alloc)(void *cookie);
+} ceu_builder;
+
+static void
+ceu_builder_init(struct ceu_builder *b, uint8_t nr_registers, void *cookie,
+                 struct ceu_queue (*alloc)(void *cookie), struct ceu_queue root)
+{
+   *b = (struct ceu_builder){
+      .nr_registers = nr_registers,
+      .cookie = cookie,
+      .queue = root,
+      .root = root,
+      .alloc = alloc,
+   };
+}
+
+/*
+ * Finish building a queue. External users should call this once when they are
+ * done with the builder. Returns the number of instructions of the emitted
+ * command queue.
+ *
+ * Internally, this is also used to finalize internal subqueues when allocating
+ * new subqueues. See ceu_alloc for details.
+ *
+ * This notably requires patching the previous queue with the length we ended up
+ * emitting for this queue.
+ */
+static unsigned
+ceu_finish(ceu_builder *b)
+{
+   if (b->length_patch) {
+      *b->length_patch = (b->queue_size * 8);
+      b->length_patch = NULL;
+   }
+
+   if (b->root.gpu == b->queue.gpu)
+      b->root_size = b->queue_size;
+
+   return b->root_size;
+}
+
+#if PAN_ARCH >= 10
+enum ceu_index_type { CEU_INDEX_REGISTER = 0, CEU_INDEX_IMMEDIATE = 1 };
+
+typedef struct ceu_index {
+   enum ceu_index_type type;
+
+   /* Number of 32-bit words in the index, must be nonzero */
+   uint8_t size;
+
+   union {
+      uint64_t imm;
+      uint8_t reg;
+   };
+} ceu_index;
+
+static uint8_t
+ceu_to_reg_tuple(ceu_index idx, ASSERTED uint8_t expected_size)
+{
+   assert(idx.type == CEU_INDEX_REGISTER);
+   assert(idx.size == expected_size);
+
+   return idx.reg;
+}
+
+static uint8_t
+ceu_to_reg32(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 1);
+}
+
+static uint8_t
+ceu_to_reg64(ceu_index idx)
+{
+   return ceu_to_reg_tuple(idx, 2);
+}
+
+static ceu_index
+ceu_reg_tuple(ASSERTED ceu_builder *b, uint8_t reg, uint8_t size)
+{
+   assert(reg + size <= b->nr_registers && "overflowed register file");
+   assert(size < 16 && "unsupported");
+
+   return (
+      struct ceu_index){.type = CEU_INDEX_REGISTER, .size = size, .reg = reg};
+}
+
+static inline ceu_index
+ceu_reg32(ceu_builder *b, uint8_t reg)
+{
+   return ceu_reg_tuple(b, reg, 1);
+}
+
+static inline ceu_index
+ceu_reg64(ceu_builder *b, uint8_t reg)
+{
+   assert((reg % 2) == 0 && "unaligned 64-bit reg");
+   return ceu_reg_tuple(b, reg, 2);
+}
+
+/*
+ * The top of the register file is reserved for ceu_builder internal use. We
+ * need 3 spare registers for handling command queue overflow. These are
+ * available here.
+ */
+static inline ceu_index
+ceu_overflow_address(ceu_builder *b)
+{
+   return ceu_reg64(b, b->nr_registers - 2);
+}
+
+static inline ceu_index
+ceu_overflow_length(ceu_builder *b)
+{
+   return ceu_reg32(b, b->nr_registers - 3);
+}
+
+static ceu_index
+ceu_extract32(ceu_builder *b, ceu_index idx, uint8_t word)
+{
+   assert(idx.type == CEU_INDEX_REGISTER && "unsupported");
+   assert(word < idx.size && "overrun");
+
+   return ceu_reg32(b, idx.reg + word);
+}
+
+static inline void *
+ceu_alloc(ceu_builder *b)
+{
+   /* If the current command queue runs out of space, allocate a new one
+    * and jump to it. We actually do this a few instructions before running
+    * out, because the sequence to jump to a new queue takes multiple
+    * instructions.
+    */
+   if (unlikely((b->queue_size + 4) > b->queue.capacity)) {
+      /* Now, allocate a new queue */
+      struct ceu_queue newq = b->alloc(b->cookie);
+
+      uint64_t *ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE, I) {
+         I.destination = ceu_to_reg64(ceu_overflow_address(b));
+         I.immediate = newq.gpu;
+      }
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_MOVE32, I) {
+         I.destination = ceu_to_reg32(ceu_overflow_length(b));
+      }
+
+      /* The length will be patched in later */
+      uint32_t *length_patch = (uint32_t *)ptr;
+
+      ptr = b->queue.cpu + (b->queue_size++);
+
+      pan_pack(ptr, CEU_JUMP, I) {
+         I.length = ceu_to_reg32(ceu_overflow_length(b));
+         I.address = ceu_to_reg64(ceu_overflow_address(b));
+      }
+
+      /* Now that we've emitted everything, finish up the previous queue */
+      ceu_finish(b);
+
+      /* And make this one current */
+      b->length_patch = length_patch;
+      b->queue = newq;
+      b->queue_size = 0;
+   }
+
+   assert(b->queue_size < b->queue.capacity);
+   return b->queue.cpu + (b->queue_size++);
+}
+
+/*
+ * Helper to emit a new instruction into the command queue. The allocation needs
+ * to be separated out being pan_pack can evaluate its argument multiple times,
+ * yet ceu_alloc has side effects.
+ */
+#define ceu_emit(b, T, cfg)                                                    \
+   void *_dest = ceu_alloc(b);                                                 \
+   pan_pack(_dest, CEU_##T, cfg)
+
+static inline void
+ceu_move32_to(ceu_builder *b, ceu_index dest, uint32_t imm)
+{
+   ceu_emit(b, MOVE32, I) {
+      I.destination = ceu_to_reg32(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_move48_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   ceu_emit(b, MOVE, I) {
+      I.destination = ceu_to_reg64(dest);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_wait_slots(ceu_builder *b, uint8_t slots)
+{
+   ceu_emit(b, WAIT, I) {
+      I.slots = slots;
+   }
+}
+
+static inline void
+ceu_branch(ceu_builder *b, int16_t offset, enum mali_ceu_condition cond,
+           ceu_index val)
+{
+   ceu_emit(b, BRANCH, I) {
+      I.offset = offset;
+      I.condition = cond;
+      I.value = ceu_to_reg32(val);
+   }
+}
+
+static inline void
+ceu_run_compute(ceu_builder *b, unsigned task_increment,
+                enum mali_task_axis task_axis)
+{
+   ceu_emit(b, RUN_COMPUTE, I) {
+      I.task_increment = task_increment;
+      I.task_axis = task_axis;
+
+      /* We always use the first table for compute jobs */
+   }
+}
+
+static inline void
+ceu_run_idvs(ceu_builder *b, enum mali_draw_mode draw_mode,
+             enum mali_index_type index_type, bool secondary_shader)
+{
+   ceu_emit(b, RUN_IDVS, I) {
+      /* We do not have a use case for traditional IDVS */
+      I.malloc_enable = true;
+
+      /* We hardcode these settings for now, we can revisit this if we
+       * rework how we emit state later.
+       */
+      I.fragment_srt_select = true;
+
+      /* Pack the override we use */
+      pan_pack(&I.flags_override, PRIMITIVE_FLAGS, cfg) {
+         cfg.draw_mode = draw_mode;
+         cfg.index_type = index_type;
+         cfg.secondary_shader = secondary_shader;
+      }
+   }
+}
+
+static inline void
+ceu_run_fragment(ceu_builder *b, bool enable_tem)
+{
+   ceu_emit(b, RUN_FRAGMENT, I) {
+      I.enable_tem = enable_tem;
+   }
+}
+
+static inline void
+ceu_finish_tiling(ceu_builder *b)
+{
+   ceu_emit(b, FINISH_TILING, _)
+      ;
+}
+
+static inline void
+ceu_finish_fragment(ceu_builder *b, bool increment_frag_completed,
+                    ceu_index first_free_heap_chunk,
+                    ceu_index last_free_heap_chunk, uint16_t scoreboard_mask,
+                    uint8_t signal_slot)
+{
+   ceu_emit(b, FINISH_FRAGMENT, I) {
+      I.increment_fragment_completed = increment_frag_completed;
+      I.wait_mask = scoreboard_mask;
+      I.first_heap_chunk = ceu_to_reg64(first_free_heap_chunk);
+      I.last_heap_chunk = ceu_to_reg64(last_free_heap_chunk);
+      I.scoreboard_entry = signal_slot;
+   }
+}
+
+static inline void
+ceu_heap_set(ceu_builder *b, ceu_index address)
+{
+   ceu_emit(b, HEAP_SET, I) {
+      I.address = ceu_to_reg64(address);
+   }
+}
+
+static inline void
+ceu_load_to(ceu_builder *b, ceu_index dest, ceu_index address, uint16_t mask,
+            int16_t offset)
+{
+   ceu_emit(b, LOAD_MULTIPLE, I) {
+      I.base = ceu_to_reg_tuple(dest, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+static inline void
+ceu_store(ceu_builder *b, ceu_index data, ceu_index address, uint16_t mask,
+          int16_t offset)
+{
+   ceu_emit(b, STORE_MULTIPLE, I) {
+      I.base = ceu_to_reg_tuple(data, util_bitcount(mask));
+      I.address = ceu_to_reg64(address);
+      I.mask = mask;
+      I.offset = offset;
+   }
+}
+
+/*
+ * Select which scoreboard entry will track endpoint tasks and other tasks
+ * respectively. Pass to ceu_wait to wait later.
+ */
+static inline void
+ceu_set_scoreboard_entry(ceu_builder *b, uint8_t ep, uint8_t other)
+{
+   assert(ep < 8 && "invalid slot");
+   assert(other < 8 && "invalid slot");
+
+   ceu_emit(b, SET_SB_ENTRY, I) {
+      I.endpoint_entry = ep;
+      I.other_entry = other;
+   }
+}
+
+static inline void
+ceu_require_all(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+      I.fragment = true;
+   }
+}
+
+static inline void
+ceu_require_compute(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+      I.compute = true;
+}
+
+static inline void
+ceu_require_fragment(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I)
+      I.fragment = true;
+}
+
+static inline void
+ceu_require_idvs(ceu_builder *b)
+{
+   ceu_emit(b, REQ_RESOURCE, I) {
+      I.compute = true;
+      I.tiler = true;
+      I.idvs = true;
+   }
+}
+
+static inline void
+ceu_heap_operation(ceu_builder *b, enum mali_ceu_heap_operation operation)
+{
+   ceu_emit(b, HEAP_OPERATION, I)
+      I.operation = operation;
+}
+
+static inline void
+ceu_vt_start(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_STARTED);
+}
+
+static inline void
+ceu_vt_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_VERTEX_TILER_COMPLETED);
+}
+
+static inline void
+ceu_frag_end(ceu_builder *b)
+{
+   ceu_heap_operation(b, MALI_CEU_HEAP_OPERATION_FRAGMENT_COMPLETED);
+}
+
+static inline void
+ceu_flush_caches(ceu_builder *b, enum mali_ceu_flush_mode l2,
+                 enum mali_ceu_flush_mode lsc, bool other_inv,
+                 ceu_index flush_id, uint16_t scoreboard_mask,
+                 uint8_t signal_slot)
+{
+   ceu_emit(b, FLUSH_CACHE2, I) {
+      I.l2_flush_mode = l2;
+      I.lsc_flush_mode = lsc;
+      I.other_invalidate = other_inv;
+      I.scoreboard_mask = scoreboard_mask;
+      I.latest_flush_id = ceu_to_reg32(flush_id);
+      I.scoreboard_entry = signal_slot;
+   }
+}
+
+/* Pseudoinstructions follow */
+
+static inline void
+ceu_move64_to(ceu_builder *b, ceu_index dest, uint64_t imm)
+{
+   if (imm < (1ull << 48)) {
+      /* Zero extends */
+      ceu_move48_to(b, dest, imm);
+   } else {
+      ceu_move32_to(b, ceu_extract32(b, dest, 0), imm);
+      ceu_move32_to(b, ceu_extract32(b, dest, 1), imm >> 32);
+   }
+}
+
+static inline void
+ceu_load32_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_load64_to(ceu_builder *b, ceu_index dest, ceu_index address, int16_t offset)
+{
+   ceu_load_to(b, dest, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_store32(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(1), offset);
+}
+
+static inline void
+ceu_store64(ceu_builder *b, ceu_index data, ceu_index address, int16_t offset)
+{
+   ceu_store(b, data, address, BITFIELD_MASK(2), offset);
+}
+
+static inline void
+ceu_wait_slot(ceu_builder *b, uint8_t slot)
+{
+   assert(slot < 8 && "invalid slot");
+
+   ceu_wait_slots(b, BITFIELD_BIT(slot));
+}
+
+static inline void
+ceu_store_state(ceu_builder *b, uint8_t signal_slot, ceu_index address,
+                enum mali_ceu_state state, uint16_t wait_mask, int16_t offset)
+{
+   ceu_emit(b, STORE_STATE, I) {
+      I.offset = offset;
+      I.wait_mask = wait_mask;
+      I.state = state;
+      I.address = ceu_to_reg64(address);
+      I.scoreboard_slot = signal_slot;
+   }
+}
+
+static inline void
+ceu_add64(ceu_builder *b, ceu_index dest, ceu_index src, uint32_t imm)
+{
+   ceu_emit(b, ADD_IMMEDIATE64, I)
+   {
+      I.destination = ceu_to_reg64(dest);
+      I.source = ceu_to_reg64(src);
+      I.immediate = imm;
+   }
+}
+
+static inline void
+ceu_add32(ceu_builder *b, ceu_index dest, ceu_index src, uint32_t imm)
+{
+   ceu_emit(b, ADD_IMMEDIATE32, I)
+   {
+      I.destination = ceu_to_reg32(dest);
+      I.source = ceu_to_reg32(src);
+      I.immediate = imm;
+   }
+}
+
+#endif /* PAN_ARCH >= 10 */
diff -up mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode.c
--- mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode.c	2023-11-24 23:33:24.963610835 +0100
@@ -434,6 +434,8 @@ GENX(pandecode_resource_tables)(struct p
    const uint8_t *cl =
       pandecode_fetch_gpu_mem(ctx, addr, MALI_RESOURCE_LENGTH * count);
 
+   pandecode_log(ctx, "%s resource table @%" PRIx64 "\n", label, addr);
+   ctx->indent += 2;
    for (unsigned i = 0; i < count; ++i) {
       pan_unpack(cl + i * MALI_RESOURCE_LENGTH, RESOURCE, entry);
       DUMP_UNPACKED(ctx, RESOURCE, entry, "Entry %u @%" PRIx64 ":\n", i,
@@ -444,6 +446,7 @@ GENX(pandecode_resource_tables)(struct p
          pandecode_resources(ctx, entry.address, entry.size);
       ctx->indent -= 2;
    }
+   ctx->indent -= 2;
 }
 
 void
diff -up mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode_csf.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode_csf.c
--- mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode_csf.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/genxml/decode_csf.c	2023-11-24 23:33:24.963610835 +0100
@@ -147,9 +147,14 @@ pandecode_run_idvs(struct pandecode_cont
    uint64_t vary_srt = cs_get_u64(qctx, reg_vary_srt);
    uint64_t frag_srt = cs_get_u64(qctx, reg_frag_srt);
 
-   GENX(pandecode_resource_tables)(ctx, position_srt, "Position resources");
-   GENX(pandecode_resource_tables)(ctx, vary_srt, "Varying resources");
-   GENX(pandecode_resource_tables)(ctx, frag_srt, "Fragment resources");
+   if (position_srt)
+      GENX(pandecode_resource_tables)(ctx, position_srt, "Position resources");
+
+   if (vary_srt)
+      GENX(pandecode_resource_tables)(ctx, vary_srt, "Varying resources");
+
+   if (frag_srt)
+      GENX(pandecode_resource_tables)(ctx, frag_srt, "Fragment resources");
 
    mali_ptr position_fau = cs_get_u64(qctx, reg_position_fau);
    mali_ptr vary_fau = cs_get_u64(qctx, reg_vary_fau);
@@ -176,8 +181,10 @@ pandecode_run_idvs(struct pandecode_cont
       GENX(pandecode_fau)(ctx, lo, hi, "Fragment FAU");
    }
 
-   GENX(pandecode_shader)
-   (ctx, cs_get_u64(qctx, 16), "Position shader", qctx->gpu_id);
+   if (cs_get_u64(qctx, 16)) {
+      GENX(pandecode_shader)
+      (ctx, cs_get_u64(qctx, 16), "Position shader", qctx->gpu_id);
+   }
 
    if (tiler_flags.secondary_shader) {
       uint64_t ptr = cs_get_u64(qctx, 18);
@@ -185,8 +192,10 @@ pandecode_run_idvs(struct pandecode_cont
       GENX(pandecode_shader)(ctx, ptr, "Varying shader", qctx->gpu_id);
    }
 
-   GENX(pandecode_shader)
-   (ctx, cs_get_u64(qctx, 20), "Fragment shader", qctx->gpu_id);
+   if (cs_get_u64(qctx, 20)) {
+      GENX(pandecode_shader)
+      (ctx, cs_get_u64(qctx, 20), "Fragment shader", qctx->gpu_id);
+   }
 
    DUMP_ADDR(ctx, LOCAL_STORAGE, cs_get_u64(qctx, 24),
              "Position Local Storage @%" PRIx64 ":\n",
@@ -247,7 +256,8 @@ pandecode_run_fragment(struct pandecode_
    DUMP_CL(ctx, SCISSOR, &qctx->regs[42], "Scissor\n");
 
    /* TODO: Tile enable map */
-   GENX(pandecode_fbd)(ctx, cs_get_u64(qctx, 40), true, qctx->gpu_id);
+   GENX(pandecode_fbd)
+   (ctx, cs_get_u64(qctx, 40) & ~0x3full, true, qctx->gpu_id);
 
    ctx->indent--;
 }
@@ -697,12 +707,7 @@ interpret_ceu_instr(struct pandecode_con
    }
 
    case MALI_CEU_OPCODE_JUMP: {
-      pan_unpack(bytes, CEU_CALL, I);
-
-      if (qctx->call_stack_depth == 0) {
-         fprintf(stderr, "Cannot jump from the entrypoint\n");
-         return false;
-      }
+      pan_unpack(bytes, CEU_JUMP, I);
 
       return interpret_ceu_jump(ctx, qctx, I.address, I.length);
    }
diff -up mesa-23.3.0-rc5/src/panfrost/lib/genxml/v10.xml.8~ mesa-23.3.0-rc5/src/panfrost/lib/genxml/v10.xml
--- mesa-23.3.0-rc5/src/panfrost/lib/genxml/v10.xml.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/genxml/v10.xml	2023-11-24 23:33:24.963610835 +0100
@@ -570,6 +570,7 @@
   <struct name="CEU RUN_COMPUTE" size="2">
     <field name="Task increment" size="14" start="0" type="uint"/>
     <field name="Task axis" size="2" start="14" type="Task Axis"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="SRT select" size="2" start="40" type="uint"/>
     <field name="SPD select" size="2" start="42" type="uint"/>
     <field name="TSD select" size="2" start="44" type="uint"/>
@@ -579,6 +580,7 @@
 
   <struct name="CEU RUN_IDVS" size="2">
     <field name="Flags override" size="32" start="0" type="hex"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Malloc enable" size="1" start="33" type="bool"/>
     <field name="Draw ID register enable" size="1" start="34" type="bool"/>
     <field name="Varying SRT select" size="1" start="35" type="bool"/>
@@ -592,18 +594,20 @@
 
   <struct name="CEU RUN_FRAGMENT" size="2">
     <field name="Enable TEM" size="1" start="0" type="bool"/>
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Opcode" size="8" start="56" type="CEU Opcode" default="RUN_FRAGMENT"/>
   </struct>
 
   <struct name="CEU FINISH_TILING" size="2">
+    <field name="Progress increment" size="1" start="32" type="bool" default="true"/>
     <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FINISH_TILING"/>
   </struct>
 
   <struct name="CEU FINISH_FRAGMENT" size="2">
     <field name="Increment Fragment Completed" size="1" start="0" type="bool"/>
     <field name="Wait mask" size="16" start="16" type="hex"/>
-    <field name="First Heap Chunk" size="8" start="32" type="hex"/>
-    <field name="Last Heap Chunk" size="8" start="40" type="hex"/>
+    <field name="Last Heap Chunk" size="8" start="32" type="hex"/>
+    <field name="First Heap Chunk" size="8" start="40" type="hex"/>
     <field name="Scoreboard entry" size="4" start="48" type="hex"/>
     <field name="Opcode" size="8" start="56" type="CEU Opcode" default="FINISH_FRAGMENT"/>
   </struct>
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/meson.build.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/meson.build
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/meson.build.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/meson.build	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,40 @@
+# Copyright Â© 2023 Collabora
+
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+libpankmod_lib_files = files(
+  'pan_kmod.c',
+  'panfrost_kmod.c',
+  'panthor_kmod.c',
+)
+
+libpankmod_lib = static_library(
+  'pankmod_lib',
+  [libpankmod_lib_files],
+  include_directories : [inc_include, inc_src, inc_panfrost],
+  c_args : [no_override_init_args],
+  gnu_symbol_visibility : 'hidden',
+  dependencies: [dep_libdrm, idep_mesautil],
+  build_by_default : false,
+)
+
+libpankmod_dep = declare_dependency(
+  include_directories: [inc_include, inc_src],
+  dependencies: [dep_libdrm],
+)
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.c
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.c.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.c	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,91 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/macros.h"
+#include "pan_kmod.h"
+
+extern const struct pan_kmod_ops panfrost_kmod_ops;
+extern const struct pan_kmod_ops panthor_kmod_ops;
+
+static const struct {
+   const char *name;
+   const struct pan_kmod_ops *ops;
+} drivers[] = {
+   {"panfrost", &panfrost_kmod_ops},
+   {"panthor", &panthor_kmod_ops},
+};
+
+static void *
+default_zalloc(const struct pan_kmod_allocator *allocator, size_t size)
+{
+   return rzalloc_size(allocator, size);
+}
+
+static void
+default_free(const struct pan_kmod_allocator *allocator, void *data)
+{
+   return ralloc_free(data);
+}
+
+static const struct pan_kmod_allocator *
+create_default_allocator(void)
+{
+   struct pan_kmod_allocator *allocator =
+      rzalloc(NULL, struct pan_kmod_allocator);
+
+   if (allocator) {
+      allocator->zalloc = default_zalloc;
+      allocator->free = default_free;
+   }
+
+   return allocator;
+}
+
+struct pan_kmod_dev *
+pan_kmod_dev_create(int fd, const struct pan_kmod_allocator *allocator)
+{
+   drmVersionPtr version = drmGetVersion(fd);
+
+   if (!version)
+      return NULL;
+
+   if (!allocator) {
+      allocator = create_default_allocator();
+      if (!allocator)
+         return NULL;
+   }
+
+   for (unsigned i = 0; i < ARRAY_SIZE(drivers); i++) {
+      if (!strcmp(drivers[i].name, version->name)) {
+         const struct pan_kmod_ops *ops = drivers[i].ops;
+         struct pan_kmod_dev *dev;
+
+         dev = ops->dev_create(fd, version, allocator);
+         if (dev)
+            return dev;
+         break;
+      }
+   }
+
+   if (allocator->zalloc == default_zalloc)
+      ralloc_free((void *)allocator);
+
+   return NULL;
+}
+
+void
+pan_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   const struct pan_kmod_allocator *allocator = dev->allocator;
+
+   dev->ops->dev_destroy(dev);
+
+   if (allocator->zalloc == default_zalloc)
+      ralloc_free((void *)allocator);
+}
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.h
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.h.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod.h	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,223 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#pragma once
+
+#include <unistd.h>
+#include <xf86drm.h>
+
+#include "util/macros.h"
+#include "util/os_file.h"
+#include "util/os_mman.h"
+#include "util/ralloc.h"
+
+#include "kmod/panthor_kmod.h"
+
+struct pan_kmod_dev;
+
+enum pan_kmod_vm_flags {
+   PAN_KMOD_VM_FLAG_AUTO_VA = BITFIELD_BIT(0),
+};
+
+struct pan_kmod_vm {
+   uint32_t flags;
+   uint32_t handle;
+   struct pan_kmod_dev *dev;
+};
+
+enum pan_kmod_bo_flags {
+   PAN_KMOD_BO_FLAG_EXECUTABLE = BITFIELD_BIT(0),
+   PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT = BITFIELD_BIT(1),
+   PAN_KMOD_BO_FLAG_NO_MMAP = BITFIELD_BIT(2),
+   PAN_KMOD_BO_FLAG_EXPORTED = BITFIELD_BIT(3),
+   PAN_KMOD_BO_FLAG_IMPORTED = BITFIELD_BIT(4),
+   PAN_KMOD_BO_FLAG_GPU_UNCACHED = BITFIELD_BIT(5),
+};
+
+struct pan_kmod_bo {
+   size_t size;
+   uint32_t handle;
+   uint32_t flags;
+   struct pan_kmod_vm *exclusive_vm;
+   struct pan_kmod_dev *dev;
+};
+
+struct pan_kmod_dev_props {
+   uint32_t gpu_prod_id;
+   uint32_t gpu_revision;
+   uint64_t shader_present;
+   uint32_t tiler_features;
+   uint32_t mem_features;
+   uint32_t mmu_features;
+   uint32_t texture_features[4];
+   uint32_t thread_tls_alloc;
+   uint32_t afbc_features;
+};
+
+struct pan_kmod_allocator {
+   void *(*zalloc)(const struct pan_kmod_allocator *allocator, size_t size);
+   void (*free)(const struct pan_kmod_allocator *allocator, void *data);
+   void *priv;
+};
+
+#define PAN_KMOD_VM_MAP_AUTO_VA ~0ull
+#define PAN_KMOD_VM_MAP_FAILED  ~0ull
+
+struct pan_kmod_ops {
+   struct pan_kmod_dev *(*dev_create)(
+      int fd, const drmVersionPtr version,
+      const struct pan_kmod_allocator *allocator);
+   void (*dev_destroy)(struct pan_kmod_dev *dev);
+   void (*dev_query_props)(struct pan_kmod_dev *dev,
+                           struct pan_kmod_dev_props *props);
+   struct pan_kmod_bo *(*bo_alloc)(struct pan_kmod_dev *dev,
+                                   struct pan_kmod_vm *exclusive_vm,
+                                   size_t size, uint32_t flags);
+   void (*bo_free)(struct pan_kmod_bo *bo);
+   struct pan_kmod_bo *(*bo_import)(struct pan_kmod_dev *dev, int fd);
+   int (*bo_export)(struct pan_kmod_bo *bo);
+   off_t (*bo_get_mmap_offset)(struct pan_kmod_bo *bo);
+   bool (*bo_wait)(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                   bool for_read_only_access);
+   void (*bo_make_evictable)(struct pan_kmod_bo *bo);
+   bool (*bo_make_unevictable)(struct pan_kmod_bo *bo);
+   struct pan_kmod_vm *(*vm_create)(struct pan_kmod_dev *dev, uint32_t flags,
+                                    uint64_t va_start, uint64_t va_range);
+   void (*vm_destroy)(struct pan_kmod_vm *vm);
+   uint64_t (*vm_map)(struct pan_kmod_vm *vm, struct pan_kmod_bo *bo,
+                      uint64_t va, off_t offset, size_t size);
+   void (*vm_unmap)(struct pan_kmod_vm *vm, uint64_t va, size_t size);
+};
+
+struct pan_kmod_driver {
+   struct {
+      uint32_t major;
+      uint32_t minor;
+   } version;
+};
+
+struct pan_kmod_dev {
+   int fd;
+   struct pan_kmod_driver driver;
+   const struct pan_kmod_ops *ops;
+   const struct pan_kmod_allocator *allocator;
+};
+
+struct pan_kmod_dev *
+pan_kmod_dev_create(int fd, const struct pan_kmod_allocator *allocator);
+
+void pan_kmod_dev_destroy(struct pan_kmod_dev *dev);
+
+static inline void
+pan_kmod_dev_query_props(struct pan_kmod_dev *dev,
+                         struct pan_kmod_dev_props *props)
+{
+   dev->ops->dev_query_props(dev, props);
+}
+
+static inline struct pan_kmod_bo *
+pan_kmod_bo_alloc(struct pan_kmod_dev *dev, struct pan_kmod_vm *exclusive_vm,
+                  size_t size, uint32_t flags)
+{
+   return dev->ops->bo_alloc(dev, exclusive_vm, size, flags);
+}
+
+static inline void
+pan_kmod_bo_free(struct pan_kmod_bo *bo)
+{
+   bo->dev->ops->bo_free(bo);
+}
+
+static inline struct pan_kmod_bo *
+pan_kmod_bo_import(struct pan_kmod_dev *dev, int fd)
+{
+   return dev->ops->bo_import(dev, fd);
+}
+
+static inline int
+pan_kmod_bo_export(struct pan_kmod_bo *bo)
+{
+   if (bo->exclusive_vm)
+      return -1;
+
+   return bo->dev->ops->bo_export(bo);
+}
+
+static inline bool
+pan_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                 bool for_read_only_access)
+{
+   return bo->dev->ops->bo_wait(bo, timeout_ns, for_read_only_access);
+}
+
+static inline void
+pan_kmod_bo_make_evictable(struct pan_kmod_bo *bo)
+{
+   if (bo->dev->ops->bo_make_evictable)
+      bo->dev->ops->bo_make_evictable(bo);
+}
+
+static inline bool
+pan_kmod_bo_make_unevictable(struct pan_kmod_bo *bo)
+{
+   if (bo->dev->ops->bo_make_unevictable)
+      return bo->dev->ops->bo_make_unevictable(bo);
+
+   return true;
+}
+
+static inline void *
+pan_kmod_bo_mmap(struct pan_kmod_bo *bo, off_t bo_offset, size_t size, int prot,
+                 int flags)
+{
+   off_t mmap_offset;
+
+   if (bo_offset + size > bo->size)
+      return MAP_FAILED;
+
+   mmap_offset = bo->dev->ops->bo_get_mmap_offset(bo);
+   if (mmap_offset < 0)
+      return MAP_FAILED;
+
+   return os_mmap(NULL, size, prot, flags, bo->dev->fd,
+                  mmap_offset + bo_offset);
+}
+
+static inline struct pan_kmod_vm *
+pan_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags, uint64_t va_start,
+                   uint64_t va_range)
+{
+   return dev->ops->vm_create(dev, flags, va_start, va_range);
+}
+
+static inline void
+pan_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   vm->dev->ops->vm_destroy(vm);
+}
+
+static inline uint64_t
+pan_kmod_vm_map(struct pan_kmod_vm *vm, struct pan_kmod_bo *bo, uint64_t va,
+                off_t offset, size_t size)
+{
+   if (!!(vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA) !=
+       (va == PAN_KMOD_VM_MAP_AUTO_VA))
+      return PAN_KMOD_VM_MAP_FAILED;
+
+   return vm->dev->ops->vm_map(vm, bo, va, offset, size);
+}
+
+static inline void
+pan_kmod_vm_unmap(struct pan_kmod_vm *vm, uint64_t va, size_t size)
+{
+   vm->dev->ops->vm_unmap(vm, va, size);
+}
+
+static inline uint32_t
+pan_kmod_vm_handle(struct pan_kmod_vm *vm)
+{
+   return vm->handle;
+}
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod_backend.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod_backend.h
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod_backend.h.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/pan_kmod_backend.h	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,72 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#pragma once
+
+#include "pan_kmod.h"
+
+static inline void
+pan_kmod_dev_init(struct pan_kmod_dev *dev, int fd, drmVersionPtr version,
+                  const struct pan_kmod_ops *ops,
+                  const struct pan_kmod_allocator *allocator)
+{
+   dev->driver.version.major = version->version_major;
+   dev->driver.version.minor = version->version_minor;
+   dev->fd = fd;
+   dev->ops = ops;
+   dev->allocator = allocator;
+}
+
+static inline void
+pan_kmod_dev_cleanup(struct pan_kmod_dev *dev)
+{
+   close(dev->fd);
+}
+
+static inline void *
+pan_kmod_alloc(const struct pan_kmod_allocator *allocator, size_t size)
+{
+   return allocator->zalloc(allocator, size);
+}
+
+static inline void
+pan_kmod_free(const struct pan_kmod_allocator *allocator, void *data)
+{
+   return allocator->free(allocator, data);
+}
+
+static inline void *
+pan_kmod_dev_alloc(struct pan_kmod_dev *dev, size_t size)
+{
+   return pan_kmod_alloc(dev->allocator, size);
+}
+
+static inline void
+pan_kmod_dev_free(const struct pan_kmod_dev *dev, void *data)
+{
+   return pan_kmod_free(dev->allocator, data);
+}
+
+static inline void
+pan_kmod_bo_init(struct pan_kmod_bo *bo, struct pan_kmod_dev *dev,
+                 struct pan_kmod_vm *exclusive_vm, size_t size, uint32_t flags,
+                 uint32_t handle)
+{
+   bo->dev = dev;
+   bo->exclusive_vm = exclusive_vm;
+   bo->size = size;
+   bo->flags = flags;
+   bo->handle = handle;
+}
+
+static inline void
+pan_kmod_vm_init(struct pan_kmod_vm *vm, struct pan_kmod_dev *dev,
+                 uint32_t handle, uint32_t flags)
+{
+   vm->dev = dev;
+   vm->handle = handle;
+   vm->flags = flags;
+}
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/panfrost_kmod.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panfrost_kmod.c
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/panfrost_kmod.c.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panfrost_kmod.c	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,408 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/hash_table.h"
+#include "util/macros.h"
+#include "util/simple_mtx.h"
+
+#include "drm-uapi/panfrost_drm.h"
+
+#include "pan_kmod_backend.h"
+
+const struct pan_kmod_ops panfrost_kmod_ops;
+
+struct panfrost_kmod_vm {
+   struct pan_kmod_vm base;
+#ifndef NDEBUG
+   struct {
+      struct hash_table_u64 *ht;
+      simple_mtx_t lock;
+   } va_to_bo;
+#endif
+};
+
+struct panfrost_kmod_dev {
+   struct pan_kmod_dev base;
+   struct panfrost_kmod_vm *vm;
+};
+
+struct panfrost_kmod_bo {
+   struct pan_kmod_bo base;
+   uint64_t offset;
+};
+
+static struct pan_kmod_dev *
+panfrost_kmod_dev_create(int fd, drmVersionPtr version,
+                         const struct pan_kmod_allocator *allocator)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      pan_kmod_alloc(allocator, sizeof(*panfrost_dev));
+   if (!panfrost_dev)
+      return NULL;
+
+   pan_kmod_dev_init(&panfrost_dev->base, fd, version, &panfrost_kmod_ops,
+                     allocator);
+   return &panfrost_dev->base;
+}
+
+static void
+panfrost_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(dev, struct panfrost_kmod_dev, base);
+
+   pan_kmod_dev_cleanup(dev);
+   pan_kmod_free(dev->allocator, panfrost_dev);
+}
+
+/* Abstraction over the raw drm_panfrost_get_param ioctl for fetching
+ * information about devices */
+
+static __u64
+panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
+                   unsigned default_value)
+{
+   struct drm_panfrost_get_param get_param = {};
+   ASSERTED int ret;
+
+   get_param.param = param;
+   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
+
+   if (ret) {
+      assert(!required);
+      return default_value;
+   }
+
+   return get_param.value;
+}
+
+static void
+panfrost_dev_query_props(struct pan_kmod_dev *dev,
+                         struct pan_kmod_dev_props *props)
+{
+   int fd = dev->fd;
+
+   memset(props, 0, sizeof(*props));
+   props->gpu_prod_id =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
+   props->gpu_revision =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
+   props->shader_present =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
+   props->tiler_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
+   props->mem_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
+   props->mmu_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_MEM_FEATURES, false, 0);
+
+   for (unsigned i = 0; i < ARRAY_SIZE(props->texture_features); i++) {
+      /* If unspecified, assume ASTC/ETC only. Factory default for Juno, and
+       * should exist on any Mali configuration. All hardware should report
+       * these texture formats but the kernel might not be new enough. */
+      static const uint32_t default_tex_features[4] = {0xfe001e, 0, 0, 0};
+
+      props->texture_features[i] =
+         panfrost_query_raw(fd, DRM_PANFROST_PARAM_TEXTURE_FEATURES0 + i, false,
+                            default_tex_features[i]);
+   }
+
+   props->thread_tls_alloc =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
+   props->afbc_features =
+      panfrost_query_raw(fd, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
+}
+
+static uint32_t
+to_panfrost_bo_flags(struct pan_kmod_dev *dev, uint32_t flags)
+{
+   uint32_t panfrost_flags = 0;
+
+   if (dev->driver.version.major > 1 || dev->driver.version.minor >= 1) {
+      if (flags & PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT)
+         panfrost_flags |= PANFROST_BO_HEAP;
+      if (!(flags & PAN_KMOD_BO_FLAG_EXECUTABLE))
+         panfrost_flags |= PANFROST_BO_NOEXEC;
+   }
+
+   return panfrost_flags;
+}
+
+static struct pan_kmod_bo *
+panfrost_kmod_bo_alloc(struct pan_kmod_dev *dev,
+                       struct pan_kmod_vm *exclusive_vm, size_t size,
+                       uint32_t flags)
+{
+   /* We can't map GPU uncached. */
+   if (flags & PAN_KMOD_BO_FLAG_GPU_UNCACHED)
+      return NULL;
+
+   struct panfrost_kmod_bo *bo = pan_kmod_dev_alloc(dev, sizeof(*bo));
+   if (!bo)
+      return NULL;
+
+   struct drm_panfrost_create_bo req = {
+      .size = size,
+      .flags = to_panfrost_bo_flags(dev, flags),
+   };
+
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &req);
+   if (ret)
+      goto err_free_bo;
+
+   pan_kmod_bo_init(&bo->base, dev, exclusive_vm, req.size, flags, req.handle);
+   bo->offset = req.offset;
+   return &bo->base;
+
+err_free_bo:
+   pan_kmod_dev_free(dev, bo);
+   return NULL;
+}
+
+static void
+panfrost_kmod_bo_free(struct pan_kmod_bo *bo)
+{
+   drmCloseBufferHandle(bo->dev->fd, bo->handle);
+   pan_kmod_dev_free(bo->dev, bo);
+}
+
+static struct pan_kmod_bo *
+panfrost_kmod_bo_import(struct pan_kmod_dev *dev, int fd)
+{
+   struct panfrost_kmod_bo *panfrost_bo =
+      pan_kmod_dev_alloc(dev, sizeof(*panfrost_bo));
+   if (!panfrost_bo)
+      return NULL;
+
+   uint32_t handle;
+   int ret = drmPrimeFDToHandle(dev->fd, fd, &handle);
+   if (ret)
+      goto err_free_bo;
+
+   struct drm_panfrost_get_bo_offset get_bo_offset = {.handle = handle, 0};
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
+   if (ret)
+      goto err_close_handle;
+
+   panfrost_bo->offset = get_bo_offset.offset;
+
+   size_t size = lseek(fd, 0, SEEK_END);
+   if (size == 0 || size == (size_t)-1)
+      goto err_close_handle;
+
+   pan_kmod_bo_init(&panfrost_bo->base, dev, NULL, size,
+                    PAN_KMOD_BO_FLAG_IMPORTED, handle);
+   return &panfrost_bo->base;
+
+err_close_handle:
+   drmCloseBufferHandle(dev->fd, handle);
+
+err_free_bo:
+   pan_kmod_dev_free(dev, panfrost_bo);
+   return NULL;
+}
+
+static inline int
+panfrost_kmod_bo_export(struct pan_kmod_bo *bo)
+{
+   struct drm_prime_handle args = {
+      .handle = bo->handle,
+      .flags = DRM_CLOEXEC,
+   };
+   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PRIME_HANDLE_TO_FD, &args);
+   if (ret == -1)
+      return -1;
+
+   bo->flags |= PAN_KMOD_BO_FLAG_EXPORTED;
+   return args.fd;
+}
+
+static off_t
+panfrost_kmod_bo_get_mmap_offset(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->handle};
+   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
+   if (ret) {
+      fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
+      assert(0);
+   }
+
+   return mmap_bo.offset;
+}
+
+static bool
+panfrost_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                      bool for_read_only_access)
+{
+   struct drm_panfrost_wait_bo req = {
+      .handle = bo->handle,
+      .timeout_ns = timeout_ns,
+   };
+
+   /* The ioctl returns >= 0 value when the BO we are waiting for is ready
+    * -1 otherwise.
+    */
+   if (drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req) != -1)
+      return true;
+
+   assert(errno == ETIMEDOUT || errno == EBUSY);
+   return false;
+}
+
+static void
+panfrost_kmod_bo_make_evictable(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_madvise req = {
+      .handle = bo->handle,
+      .madv = PANFROST_MADV_DONTNEED,
+   };
+
+   drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MADVISE, &req);
+}
+
+static bool
+panfrost_kmod_bo_make_unevictable(struct pan_kmod_bo *bo)
+{
+   struct drm_panfrost_madvise req = {
+      .handle = bo->handle,
+      .madv = PANFROST_MADV_WILLNEED,
+   };
+
+   if (drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MADVISE, &req) == 0 &&
+       req.retained == 0)
+      return false;
+
+   return true;
+}
+
+static struct pan_kmod_vm *
+panfrost_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags,
+                        uint64_t va_start, uint64_t va_range)
+{
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(dev, struct panfrost_kmod_dev, base);
+
+   /* Only one VM per device. */
+   if (panfrost_dev->vm)
+      return NULL;
+
+   /* Panfrost kernel driver doesn't support userspace VA management. */
+   if (!(flags & PAN_KMOD_VM_FLAG_AUTO_VA))
+      return NULL;
+
+   /* 32-bit address space, with the lower 32MB reserved. */
+   if (va_start != 0x2000000 || va_start + va_range != 1ull << 32)
+      return NULL;
+
+   struct panfrost_kmod_vm *vm = pan_kmod_dev_alloc(dev, sizeof(*vm));
+   if (!vm)
+      return NULL;
+
+   pan_kmod_vm_init(&vm->base, dev, 0, flags);
+
+#ifndef NDEBUG
+   vm->va_to_bo.ht = _mesa_hash_table_u64_create(NULL);
+   if (!vm->va_to_bo.ht) {
+      pan_kmod_dev_free(dev, vm);
+      return NULL;
+   }
+   simple_mtx_init(&vm->va_to_bo.lock, mtx_plain);
+#endif
+
+   panfrost_dev->vm = vm;
+   return &vm->base;
+}
+
+static void
+panfrost_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   struct panfrost_kmod_vm *panfrost_vm =
+      container_of(vm, struct panfrost_kmod_vm, base);
+   struct panfrost_kmod_dev *panfrost_dev =
+      container_of(vm->dev, struct panfrost_kmod_dev, base);
+
+   panfrost_dev->vm = NULL;
+
+#ifndef NDEBUG
+   _mesa_hash_table_u64_destroy(panfrost_vm->va_to_bo.ht);
+   simple_mtx_destroy(&panfrost_vm->va_to_bo.lock);
+#endif
+
+   pan_kmod_dev_free(vm->dev, vm);
+}
+
+static uint64_t
+panfrost_kmod_vm_map(struct pan_kmod_vm *vm, struct pan_kmod_bo *bo,
+                     uint64_t va, off_t offset, size_t size)
+{
+   struct panfrost_kmod_vm *panfrost_vm =
+      container_of(vm, struct panfrost_kmod_vm, base);
+   struct panfrost_kmod_bo *panfrost_bo =
+      container_of(bo, struct panfrost_kmod_bo, base);
+
+   /* Panfrost kernel driver doesn't support userspace VA management. */
+   if (va != PAN_KMOD_VM_MAP_AUTO_VA)
+      return PAN_KMOD_VM_MAP_FAILED;
+
+   /* Panfrost kernel driver only support full BO mapping. */
+   if (offset != 0 || size != bo->size)
+      return PAN_KMOD_VM_MAP_FAILED;
+
+   va = panfrost_bo->offset;
+
+   /* Make sure we don't have a BO mapped at this address. */
+#ifndef NDEBUG
+   simple_mtx_lock(&panfrost_vm->va_to_bo.lock);
+   assert(_mesa_hash_table_u64_search(panfrost_vm->va_to_bo.ht, va) == NULL);
+
+   _mesa_hash_table_u64_insert(panfrost_vm->va_to_bo.ht, va, bo);
+   simple_mtx_unlock(&panfrost_vm->va_to_bo.lock);
+#endif
+
+   return va;
+}
+
+static void
+panfrost_kmod_vm_unmap(struct pan_kmod_vm *vm, uint64_t va, size_t size)
+{
+   struct panfrost_kmod_vm *panfrost_vm =
+      container_of(vm, struct panfrost_kmod_vm, base);
+
+#ifndef NDEBUG
+   simple_mtx_lock(&panfrost_vm->va_to_bo.lock);
+   ASSERTED struct panfrost_kmod_bo *panfrost_bo =
+      _mesa_hash_table_u64_search(panfrost_vm->va_to_bo.ht, va);
+
+   assert(panfrost_bo && panfrost_bo->base.size == size &&
+          panfrost_bo->offset == va);
+
+   _mesa_hash_table_u64_remove(panfrost_vm->va_to_bo.ht, va);
+   simple_mtx_unlock(&panfrost_vm->va_to_bo.lock);
+#endif
+}
+
+const struct pan_kmod_ops panfrost_kmod_ops = {
+   .dev_create = panfrost_kmod_dev_create,
+   .dev_destroy = panfrost_kmod_dev_destroy,
+   .dev_query_props = panfrost_dev_query_props,
+   .bo_alloc = panfrost_kmod_bo_alloc,
+   .bo_free = panfrost_kmod_bo_free,
+   .bo_import = panfrost_kmod_bo_import,
+   .bo_export = panfrost_kmod_bo_export,
+   .bo_get_mmap_offset = panfrost_kmod_bo_get_mmap_offset,
+   .bo_wait = panfrost_kmod_bo_wait,
+   .bo_make_evictable = panfrost_kmod_bo_make_evictable,
+   .bo_make_unevictable = panfrost_kmod_bo_make_unevictable,
+   .vm_create = panfrost_kmod_vm_create,
+   .vm_destroy = panfrost_kmod_vm_destroy,
+   .vm_map = panfrost_kmod_vm_map,
+   .vm_unmap = panfrost_kmod_vm_unmap,
+};
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.c
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.c.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.c	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,654 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include <errno.h>
+#include <fcntl.h>
+#include <string.h>
+#include <xf86drm.h>
+
+#include "util/hash_table.h"
+#include "util/libsync.h"
+#include "util/macros.h"
+#include "util/os_time.h"
+#include "util/u_debug.h"
+#include "util/vma.h"
+
+#include "drm-uapi/dma-buf.h"
+#include "drm-uapi/panthor_drm.h"
+
+#include "pan_kmod_backend.h"
+
+const struct pan_kmod_ops panthor_kmod_ops;
+
+struct panthor_kmod_async_unmap {
+   struct list_head node;
+   uint64_t sync_point;
+   uint64_t va;
+   size_t size;
+};
+
+struct panthor_kmod_vm {
+   struct pan_kmod_vm base;
+   struct util_vma_heap vma;
+   struct list_head async_unmaps;
+   struct {
+      uint32_t handle;
+      uint64_t point;
+   } sync;
+};
+
+struct panthor_kmod_dev {
+   struct pan_kmod_dev base;
+   uint32_t *flush_id;
+};
+
+struct panthor_kmod_bo {
+   struct pan_kmod_bo base;
+   struct {
+      uint32_t handle;
+      uint64_t read_point;
+      uint64_t write_point;
+   } sync;
+};
+
+static struct pan_kmod_dev *
+panthor_kmod_dev_create(int fd, drmVersionPtr version,
+                        const struct pan_kmod_allocator *allocator)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      pan_kmod_alloc(allocator, sizeof(*panthor_dev));
+   if (!panthor_dev)
+      return NULL;
+
+   panthor_dev->flush_id = os_mmap(0, getpagesize(), PROT_READ, MAP_SHARED, fd,
+                                   DRM_PANTHOR_USER_FLUSH_ID_MMIO_OFFSET);
+   if (panthor_dev->flush_id == MAP_FAILED)
+      goto err_free_dev;
+
+   pan_kmod_dev_init(&panthor_dev->base, fd, version, &panthor_kmod_ops,
+                     allocator);
+   return &panthor_dev->base;
+
+err_free_dev:
+   pan_kmod_free(allocator, panthor_dev);
+   return NULL;
+}
+
+static void
+panthor_kmod_dev_destroy(struct pan_kmod_dev *dev)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   os_munmap(panthor_dev->flush_id, getpagesize());
+   pan_kmod_dev_cleanup(dev);
+   pan_kmod_free(dev->allocator, panthor_dev);
+}
+
+static void
+panthor_dev_query_props(struct pan_kmod_dev *dev,
+                        struct pan_kmod_dev_props *props)
+{
+   struct drm_panthor_gpu_info gpu_info = {};
+   struct drm_panthor_dev_query query = {
+      .type = DRM_PANTHOR_DEV_QUERY_GPU_INFO,
+      .size = sizeof(gpu_info),
+      .pointer = (uint64_t)(uintptr_t)&gpu_info,
+   };
+
+   ASSERTED int ret = drmIoctl(dev->fd, DRM_IOCTL_PANTHOR_DEV_QUERY, &query);
+   assert(!ret);
+
+   *props = (struct pan_kmod_dev_props){
+      .gpu_prod_id = gpu_info.gpu_id >> 16,
+      .gpu_revision = gpu_info.gpu_id & 0xffff,
+      .shader_present = gpu_info.shader_present,
+      .tiler_features = gpu_info.tiler_features,
+      .mem_features = gpu_info.mem_features,
+      .mmu_features = gpu_info.mmu_features,
+      .thread_tls_alloc = 0,
+      .afbc_features = 0,
+   };
+
+   static_assert(
+      sizeof(props->texture_features) == sizeof(gpu_info.texture_features),
+      "Mismatch in texture_features array size");
+
+   memcpy(props->texture_features, gpu_info.texture_features,
+          sizeof(props->texture_features));
+}
+
+static uint32_t
+to_panthor_bo_flags(uint32_t flags)
+{
+   uint32_t panthor_flags = 0;
+
+   if (flags & PAN_KMOD_BO_FLAG_NO_MMAP)
+      panthor_flags |= DRM_PANTHOR_BO_NO_MMAP;
+
+   return panthor_flags;
+}
+
+static struct pan_kmod_bo *
+panthor_kmod_bo_alloc(struct pan_kmod_dev *dev,
+                      struct pan_kmod_vm *exclusive_vm, size_t size,
+                      uint32_t flags)
+{
+   /* We don't support allocating on-fault. */
+   if (flags & PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT)
+      return NULL;
+
+   struct panthor_kmod_vm *panthor_vm =
+      exclusive_vm ? container_of(exclusive_vm, struct panthor_kmod_vm, base)
+                   : NULL;
+   struct panthor_kmod_bo *bo = pan_kmod_dev_alloc(dev, sizeof(*bo));
+   if (!bo)
+      return NULL;
+
+   struct drm_panthor_bo_create req = {
+      .size = size,
+      .flags = to_panthor_bo_flags(flags),
+      .exclusive_vm_id = panthor_vm ? panthor_vm->base.handle : 0,
+   };
+
+   int ret = drmIoctl(dev->fd, DRM_IOCTL_PANTHOR_BO_CREATE, &req);
+   if (ret)
+      goto err_free_bo;
+
+   if (!exclusive_vm) {
+      int ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED,
+                                 &bo->sync.handle);
+      if (ret)
+         goto err_destroy_bo;
+   } else {
+      bo->sync.handle = panthor_vm->sync.handle;
+   }
+
+   bo->sync.read_point = bo->sync.write_point = 0;
+
+   pan_kmod_bo_init(&bo->base, dev, exclusive_vm, req.size, flags, req.handle);
+   return &bo->base;
+
+err_destroy_bo:
+   drmCloseBufferHandle(dev->fd, bo->base.handle);
+err_free_bo:
+   pan_kmod_dev_free(dev, bo);
+   return NULL;
+}
+
+static void
+panthor_kmod_bo_free(struct pan_kmod_bo *bo)
+{
+   drmCloseBufferHandle(bo->dev->fd, bo->handle);
+   pan_kmod_dev_free(bo->dev, bo);
+}
+
+static struct pan_kmod_bo *
+panthor_kmod_bo_import(struct pan_kmod_dev *dev, int fd)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      pan_kmod_dev_alloc(dev, sizeof(*panthor_bo));
+   if (!panthor_bo)
+      return NULL;
+
+   uint32_t handle;
+   int ret = drmPrimeFDToHandle(dev->fd, fd, &handle);
+   if (ret)
+      goto err_free_bo;
+
+   size_t size = lseek(fd, 0, SEEK_END);
+   if (size == 0 || size == (size_t)-1)
+      goto err_close_handle;
+
+   ret = drmSyncobjCreate(dev->fd, 0, &panthor_bo->sync.handle);
+   if (ret)
+      goto err_close_handle;
+
+   pan_kmod_bo_init(&panthor_bo->base, dev, NULL, size,
+                    PAN_KMOD_BO_FLAG_IMPORTED, handle);
+   return &panthor_bo->base;
+
+err_close_handle:
+   drmCloseBufferHandle(dev->fd, handle);
+
+err_free_bo:
+   pan_kmod_dev_free(dev, panthor_bo);
+   return NULL;
+}
+
+static int
+panthor_kmod_bo_export(struct pan_kmod_bo *bo)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   int dmabuf_fd;
+
+   int ret =
+      drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+   if (ret == -1)
+      return -1;
+
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (!shared) {
+      if (panthor_bo->sync.read_point || panthor_bo->sync.write_point) {
+         struct dma_buf_import_sync_file isync = {
+            .flags = DMA_BUF_SYNC_RW,
+         };
+         int ret = drmSyncobjExportSyncFile(bo->dev->fd,
+                                            panthor_bo->sync.handle, &isync.fd);
+         assert(!ret);
+
+         ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &isync);
+         assert(!ret);
+         close(isync.fd);
+      }
+
+      /* Make sure we reset the syncobj on export. We will use it as a
+       * temporary binary syncobj to import sync_file FD from now on.
+       */
+      ret = drmSyncobjReset(bo->dev->fd, &panthor_bo->sync.handle, 1);
+      assert(!ret);
+      panthor_bo->sync.read_point = 0;
+      panthor_bo->sync.write_point = 0;
+   }
+
+   bo->flags |= PAN_KMOD_BO_FLAG_EXPORTED;
+   return dmabuf_fd;
+}
+
+static off_t
+panthor_kmod_bo_get_mmap_offset(struct pan_kmod_bo *bo)
+{
+   struct drm_panthor_bo_mmap_offset req = {.handle = bo->handle};
+   ASSERTED int ret =
+      drmIoctl(bo->dev->fd, DRM_IOCTL_PANTHOR_BO_MMAP_OFFSET, &req);
+
+   assert(!ret);
+
+   return req.offset;
+}
+
+static bool
+panthor_kmod_bo_wait(struct pan_kmod_bo *bo, int64_t timeout_ns,
+                     bool for_read_only_access)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      int dmabuf_fd;
+      int ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+
+      if (ret)
+         return false;
+
+      struct dma_buf_export_sync_file esync = {
+         .flags = for_read_only_access ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &esync);
+      close(dmabuf_fd);
+
+      if (ret)
+         return false;
+
+      ret = sync_wait(esync.fd, timeout_ns / 1000000);
+      close(esync.fd);
+      return ret == 0;
+   } else {
+      uint64_t sync_point =
+         for_read_only_access
+            ? panthor_bo->sync.write_point
+            : MAX2(panthor_bo->sync.write_point, panthor_bo->sync.read_point);
+
+      if (!sync_point)
+         return true;
+
+      int64_t abs_timeout_ns = timeout_ns < INT64_MAX - os_time_get_nano()
+                                  ? timeout_ns + os_time_get_nano()
+                                  : INT64_MAX;
+      int ret = drmSyncobjTimelineWait(bo->dev->fd, &panthor_bo->sync.handle,
+                                       &sync_point, 1, abs_timeout_ns,
+                                       DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+      if (ret >= 0)
+         return true;
+
+      assert(ret == -ETIME);
+      return false;
+   }
+}
+
+void
+panthor_kmod_bo_attach_sync_point(struct pan_kmod_bo *bo, uint32_t sync_handle,
+                                  uint64_t sync_point, bool read_only)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   struct panthor_kmod_vm *panthor_vm =
+      bo->exclusive_vm
+         ? container_of(bo->exclusive_vm, struct panthor_kmod_vm, base)
+         : NULL;
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      struct dma_buf_import_sync_file isync = {
+         .flags = read_only ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+      int dmabuf_fd;
+      int ret = drmSyncobjExportSyncFile(bo->dev->fd, sync_handle, &isync.fd);
+      assert(!ret);
+
+      ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+      assert(!ret);
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_IMPORT_SYNC_FILE, &isync);
+      assert(!ret);
+      close(dmabuf_fd);
+      close(isync.fd);
+   } else if (panthor_vm) {
+      /* Private BOs should be passed the VM syncobj. */
+      assert(sync_handle == panthor_vm->sync.handle);
+
+      panthor_bo->sync.write_point =
+         MAX2(sync_point, panthor_bo->sync.write_point);
+      if (!read_only) {
+         panthor_bo->sync.read_point =
+            MAX2(sync_point, panthor_bo->sync.read_point);
+      }
+   } else {
+      uint32_t new_sync_point =
+         MAX2(panthor_bo->sync.write_point, panthor_bo->sync.read_point) + 1;
+
+      int ret = drmSyncobjTransfer(bo->dev->fd, panthor_bo->sync.handle,
+                                   new_sync_point, sync_handle, sync_point, 0);
+      assert(!ret);
+
+      panthor_bo->sync.write_point = new_sync_point;
+      if (!read_only)
+         panthor_bo->sync.read_point = new_sync_point;
+   }
+}
+
+int
+panthor_kmod_bo_get_sync_point(struct pan_kmod_bo *bo, uint32_t *sync_handle,
+                               uint64_t *sync_point, bool for_read_only_access)
+{
+   struct panthor_kmod_bo *panthor_bo =
+      container_of(bo, struct panthor_kmod_bo, base);
+   bool shared =
+      bo->flags & (PAN_KMOD_BO_FLAG_EXPORTED | PAN_KMOD_BO_FLAG_IMPORTED);
+
+   if (shared) {
+      int dmabuf_fd;
+      int ret =
+         drmPrimeHandleToFD(bo->dev->fd, bo->handle, DRM_CLOEXEC, &dmabuf_fd);
+
+      if (ret) {
+         debug_printf("drmPrimeHandleToFD() failed: %d\n", ret);
+         return -1;
+      }
+
+      struct dma_buf_export_sync_file esync = {
+         .flags = for_read_only_access ? DMA_BUF_SYNC_READ : DMA_BUF_SYNC_RW,
+      };
+
+      ret = drmIoctl(dmabuf_fd, DMA_BUF_IOCTL_EXPORT_SYNC_FILE, &esync);
+      close(dmabuf_fd);
+      if (ret) {
+         debug_printf("drmIoctl(..., DMA_BUF_IOCTL_EXPORT_SYNC_FILE, ...) "
+                      "failed: %d\n",
+                      ret);
+         return -1;
+      }
+
+      ret = drmSyncobjImportSyncFile(bo->dev->fd, panthor_bo->sync.handle,
+                                     esync.fd);
+      close(esync.fd);
+      if (ret) {
+         debug_printf("drmSyncobjImportSyncFile() failed: %d\n", ret);
+         return -1;
+      }
+
+      *sync_handle = panthor_bo->sync.handle;
+      *sync_point = 0;
+   } else {
+      *sync_handle = panthor_bo->sync.handle;
+      *sync_point = for_read_only_access ? panthor_bo->sync.write_point
+                                         : MAX2(panthor_bo->sync.read_point,
+                                                panthor_bo->sync.write_point);
+   }
+   return 0;
+}
+
+static struct pan_kmod_vm *
+panthor_kmod_vm_create(struct pan_kmod_dev *dev, uint32_t flags,
+                       uint64_t user_va_start, uint64_t user_va_range)
+{
+   struct pan_kmod_dev_props props;
+
+   panthor_dev_query_props(dev, &props);
+
+   struct panthor_kmod_vm *panthor_vm =
+      pan_kmod_dev_alloc(dev, sizeof(*panthor_vm));
+   if (!panthor_vm)
+      return NULL;
+
+   list_inithead(&panthor_vm->async_unmaps);
+   if (flags & PAN_KMOD_VM_FLAG_AUTO_VA)
+      util_vma_heap_init(&panthor_vm->vma, user_va_start, user_va_range);
+
+   panthor_vm->sync.point = 0;
+   int ret = drmSyncobjCreate(dev->fd, DRM_SYNCOBJ_CREATE_SIGNALED,
+                              &panthor_vm->sync.handle);
+   if (ret)
+      goto err_free_vm;
+
+   uint64_t full_va_range = 1ull << DRM_PANTHOR_MMU_VA_BITS(props.mmu_features);
+   struct drm_panthor_vm_create req = {
+      .kernel_va_range = MIN2(full_va_range - user_va_start - user_va_range,
+                              full_va_range >> 1),
+   };
+
+   ret = drmIoctl(dev->fd, DRM_IOCTL_PANTHOR_VM_CREATE, &req);
+   if (ret)
+      goto err_destroy_sync;
+
+   pan_kmod_vm_init(&panthor_vm->base, dev, req.id, flags);
+   return &panthor_vm->base;
+
+err_destroy_sync:
+   drmSyncobjDestroy(dev->fd, panthor_vm->sync.handle);
+
+err_free_vm:
+   if (flags & PAN_KMOD_VM_FLAG_AUTO_VA)
+      util_vma_heap_finish(&panthor_vm->vma);
+
+   pan_kmod_dev_free(dev, panthor_vm);
+   return NULL;
+}
+
+static void
+panthor_kmod_vm_collect_async_unmaps(struct panthor_kmod_vm *vm)
+{
+   bool done = false;
+
+   list_for_each_entry_safe_rev(struct panthor_kmod_async_unmap, req,
+                                &vm->async_unmaps, node)
+   {
+      if (!done) {
+         int ret = drmSyncobjTimelineWait(
+            vm->base.dev->fd, &vm->sync.handle, &req->sync_point, 1, 0,
+            DRM_SYNCOBJ_WAIT_FLAGS_WAIT_ALL, NULL);
+         if (ret >= 0)
+            done = true;
+         else
+            continue;
+      }
+
+      list_del(&req->node);
+      util_vma_heap_free(&vm->vma, req->va, req->size);
+      pan_kmod_dev_free(vm->base.dev, req);
+   }
+}
+
+static void
+panthor_kmod_vm_destroy(struct pan_kmod_vm *vm)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+   struct drm_panthor_vm_destroy req = {.id = vm->handle};
+   ASSERTED int ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_DESTROY, &req);
+   assert(!ret);
+
+   drmSyncobjDestroy(vm->dev->fd, panthor_vm->sync.handle);
+
+   if (panthor_vm->base.flags & PAN_KMOD_VM_FLAG_AUTO_VA) {
+      list_for_each_entry_safe(struct panthor_kmod_async_unmap, req,
+                               &panthor_vm->async_unmaps, node) {
+         list_del(&req->node);
+         util_vma_heap_free(&panthor_vm->vma, req->va, req->size);
+         pan_kmod_dev_free(vm->dev, req);
+      }
+      util_vma_heap_finish(&panthor_vm->vma);
+   }
+
+   pan_kmod_dev_free(vm->dev, panthor_vm);
+}
+
+static uint64_t
+panthor_kmod_vm_map(struct pan_kmod_vm *vm, struct pan_kmod_bo *bo, uint64_t va,
+                    off_t offset, size_t size)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+
+   if (vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA) {
+      panthor_kmod_vm_collect_async_unmaps(panthor_vm);
+      va = util_vma_heap_alloc(&panthor_vm->vma, size,
+                               size > 0x200000 ? 0x200000 : 0x1000);
+   }
+
+   struct drm_panthor_vm_bind_op bind_op = {
+      .flags = DRM_PANTHOR_VM_BIND_OP_TYPE_MAP,
+      .bo_handle = bo->handle,
+      .bo_offset = offset,
+      .va = va,
+      .size = size,
+   };
+   struct drm_panthor_vm_bind req = {
+      .vm_id = vm->handle,
+      .flags = 0,
+      .ops = DRM_PANTHOR_OBJ_ARRAY(1, &bind_op),
+   };
+
+   if (bo->flags & PAN_KMOD_BO_FLAG_EXECUTABLE)
+      bind_op.flags |= DRM_PANTHOR_VM_BIND_OP_MAP_READONLY;
+   else
+      bind_op.flags |= DRM_PANTHOR_VM_BIND_OP_MAP_NOEXEC;
+
+   if (bo->flags & PAN_KMOD_BO_FLAG_GPU_UNCACHED)
+      bind_op.flags |= DRM_PANTHOR_VM_BIND_OP_MAP_UNCACHED;
+
+   int ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_BIND, &req);
+   if (ret && (vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA)) {
+      util_vma_heap_free(&panthor_vm->vma, va, size);
+      va = PAN_KMOD_VM_MAP_FAILED;
+   }
+
+   assert(offset == 0);
+   assert(size == bo->size);
+   return va;
+}
+
+static void
+panthor_kmod_vm_unmap(struct pan_kmod_vm *vm, uint64_t va, size_t size)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+
+   struct drm_panthor_sync_op syncs[2] = {
+      {
+         .flags = DRM_PANTHOR_SYNC_OP_WAIT |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = panthor_vm->sync.handle,
+         .timeline_value = panthor_vm->sync.point,
+      },
+      {
+         .flags = DRM_PANTHOR_SYNC_OP_SIGNAL |
+                  DRM_PANTHOR_SYNC_OP_HANDLE_TYPE_TIMELINE_SYNCOBJ,
+         .handle = panthor_vm->sync.handle,
+         .timeline_value = ++panthor_vm->sync.point,
+      },
+   };
+   struct drm_panthor_vm_bind_op bind_op = {
+      .flags = DRM_PANTHOR_VM_BIND_OP_TYPE_UNMAP,
+      .va = va,
+      .size = size,
+      .syncs = DRM_PANTHOR_OBJ_ARRAY(ARRAY_SIZE(syncs), syncs),
+   };
+   struct drm_panthor_vm_bind req = {
+      .vm_id = vm->handle,
+      .flags = DRM_PANTHOR_VM_BIND_ASYNC,
+      .ops = DRM_PANTHOR_OBJ_ARRAY(1, &bind_op),
+   };
+
+   ASSERTED int ret = drmIoctl(vm->dev->fd, DRM_IOCTL_PANTHOR_VM_BIND, &req);
+   assert(!ret);
+
+   if (vm->flags & PAN_KMOD_VM_FLAG_AUTO_VA) {
+      struct panthor_kmod_async_unmap *req =
+         pan_kmod_dev_alloc(vm->dev, sizeof(*req));
+
+      assert(req);
+      req->va = va;
+      req->size = size;
+      req->sync_point = panthor_vm->sync.point;
+      list_addtail(&req->node, &panthor_vm->async_unmaps);
+   }
+}
+
+void
+panthor_kmod_vm_new_sync_point(struct pan_kmod_vm *vm, uint32_t *sync_handle,
+                               uint64_t *sync_point)
+{
+   struct panthor_kmod_vm *panthor_vm =
+      container_of(vm, struct panthor_kmod_vm, base);
+
+   *sync_handle = panthor_vm->sync.handle;
+   *sync_point = ++panthor_vm->sync.point;
+}
+
+uint32_t
+panthor_kmod_get_flush_id(const struct pan_kmod_dev *dev)
+{
+   struct panthor_kmod_dev *panthor_dev =
+      container_of(dev, struct panthor_kmod_dev, base);
+
+   return *(panthor_dev->flush_id);
+}
+
+const struct pan_kmod_ops panthor_kmod_ops = {
+   .dev_create = panthor_kmod_dev_create,
+   .dev_destroy = panthor_kmod_dev_destroy,
+   .dev_query_props = panthor_dev_query_props,
+   .bo_alloc = panthor_kmod_bo_alloc,
+   .bo_free = panthor_kmod_bo_free,
+   .bo_import = panthor_kmod_bo_import,
+   .bo_export = panthor_kmod_bo_export,
+   .bo_get_mmap_offset = panthor_kmod_bo_get_mmap_offset,
+   .bo_wait = panthor_kmod_bo_wait,
+   .vm_create = panthor_kmod_vm_create,
+   .vm_destroy = panthor_kmod_vm_destroy,
+   .vm_map = panthor_kmod_vm_map,
+   .vm_unmap = panthor_kmod_vm_unmap,
+};
diff -up mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.h
--- mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.h.8~	2023-11-24 23:33:24.963610835 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/kmod/panthor_kmod.h	2023-11-24 23:33:24.963610835 +0100
@@ -0,0 +1,25 @@
+/*
+ * Copyright Â© 2023 Collabora, Ltd.
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#pragma once
+
+#include <stdint.h>
+
+struct pan_kmod_bo;
+struct pan_kmod_dev;
+struct pan_kmod_vm;
+
+void panthor_kmod_bo_attach_sync_point(struct pan_kmod_bo *bo,
+                                       uint32_t sync_handle,
+                                       uint64_t sync_point, bool read_only);
+int panthor_kmod_bo_get_sync_point(struct pan_kmod_bo *bo,
+                                   uint32_t *sync_handle, uint64_t *sync_point,
+                                   bool read_only);
+uint32_t panthor_kmod_vm_handle(struct pan_kmod_vm *vm);
+void panthor_kmod_vm_new_sync_point(struct pan_kmod_vm *vm,
+                                    uint32_t *sync_handle,
+                                    uint64_t *sync_point);
+uint32_t panthor_kmod_get_flush_id(const struct pan_kmod_dev *dev);
diff -up mesa-23.3.0-rc5/src/panfrost/lib/meson.build.8~ mesa-23.3.0-rc5/src/panfrost/lib/meson.build
--- mesa-23.3.0-rc5/src/panfrost/lib/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/meson.build	2023-11-24 23:33:24.963610835 +0100
@@ -20,6 +20,7 @@
 # SOFTWARE.
 
 subdir('genxml')
+subdir('kmod')
 
 pixel_format_versions = ['6', '7', '9']
 libpanfrost_pixel_format = []
@@ -39,7 +40,7 @@ endforeach
 
 libpanfrost_per_arch = []
 
-foreach ver : ['4', '5', '6', '7', '9']
+foreach ver : ['4', '5', '6', '7', '9', '10']
   libpanfrost_per_arch += static_library(
     'pan-arch-v' + ver,
     [
@@ -92,13 +93,13 @@ libpanfrost_lib = static_library(
   gnu_symbol_visibility : 'hidden',
   dependencies: [dep_libdrm, idep_nir, idep_mesautil],
   build_by_default : false,
-  link_with: [libpanfrost_pixel_format, libpanfrost_per_arch],
+  link_with: [libpanfrost_pixel_format, libpanfrost_per_arch, libpankmod_lib],
 )
 
 libpanfrost_dep = declare_dependency(
   link_with: [libpanfrost_lib, libpanfrost_decode, libpanfrost_midgard, libpanfrost_bifrost, libpanfrost_pixel_format, libpanfrost_per_arch],
   include_directories: [inc_include, inc_src, inc_mapi, inc_mesa, inc_gallium, inc_gallium_aux, inc_panfrost_hw, inc_panfrost],
-  dependencies: [dep_libdrm, idep_nir, idep_pan_packers],
+  dependencies: [dep_libdrm, libpankmod_dep, idep_nir, idep_pan_packers],
 )
 
 if with_tests
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_afbc.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_afbc.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_afbc.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_afbc.c	2023-11-24 23:33:24.963610835 +0100
@@ -187,20 +187,6 @@ panfrost_afbc_can_ytr(enum pipe_format f
    return desc->colorspace == UTIL_FORMAT_COLORSPACE_RGB;
 }
 
-/* Only support packing for RGB formats for now. */
-
-bool
-panfrost_afbc_can_pack(enum pipe_format format)
-{
-   const struct util_format_description *desc = util_format_description(format);
-
-   if (desc->nr_channels != 1 && desc->nr_channels != 3 &&
-       desc->nr_channels != 4)
-      return false;
-
-   return desc->colorspace == UTIL_FORMAT_COLORSPACE_RGB;
-}
-
 /*
  * Check if the device supports AFBC with tiled headers (and hence also solid
  * colour blocks).
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_blend.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_blend.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_blend.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_blend.c	2023-11-24 23:33:24.963610835 +0100
@@ -878,7 +878,7 @@ GENX(pan_blend_get_shader_locked)(const
 
    /* Compile the NIR shader */
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .is_blend = true,
       .blend.nr_samples = key.nr_samples,
    };
@@ -899,7 +899,7 @@ GENX(pan_blend_get_shader_locked)(const
 #else
    NIR_PASS_V(nir, pan_lower_framebuffer, rt_formats,
               pan_raw_format_mask_midgard(rt_formats), MAX2(key.nr_samples, 1),
-              dev->gpu_id < 0x700);
+              panfrost_device_gpu_id(dev) < 0x700);
 #endif
 
    GENX(pan_shader_compile)(nir, &inputs, &variant->binary, &info);
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_blitter.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_blitter.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_blitter.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_blitter.c	2023-11-24 23:33:24.964610841 +0100
@@ -606,7 +606,7 @@ pan_blitter_get_blit_shader(struct panfr
    }
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .is_blit = true,
       .no_idvs = true,
    };
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.c	2023-11-24 23:33:24.964610841 +0100
@@ -28,7 +28,6 @@
 #include <pthread.h>
 #include <stdio.h>
 #include <xf86drm.h>
-#include "drm-uapi/panfrost_drm.h"
 
 #include "pan_bo.h"
 #include "pan_device.h"
@@ -55,34 +54,42 @@
  * around the linked list.
  */
 
+static uint32_t
+to_kmod_bo_flags(uint32_t flags)
+{
+   uint32_t kmod_bo_flags = 0;
+
+   if (flags & PAN_BO_EXECUTE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_EXECUTABLE;
+   if (flags & PAN_BO_GROWABLE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_ALLOC_ON_FAULT;
+   if (flags & PAN_BO_INVISIBLE)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_NO_MMAP;
+   if (flags & PAN_BO_GPU_UNCACHED)
+      kmod_bo_flags |= PAN_KMOD_BO_FLAG_GPU_UNCACHED;
+
+   return kmod_bo_flags;
+}
+
 static struct panfrost_bo *
 panfrost_bo_alloc(struct panfrost_device *dev, size_t size, uint32_t flags,
                   const char *label)
 {
-   struct drm_panfrost_create_bo create_bo = {.size = size};
+   struct pan_kmod_vm *exclusive_vm =
+      !(flags & PAN_BO_SHAREABLE) ? dev->kmod.vm : NULL;
+   struct pan_kmod_bo *kmod_bo;
    struct panfrost_bo *bo;
-   int ret;
 
-   if (dev->kernel_version->version_major > 1 ||
-       dev->kernel_version->version_minor >= 1) {
-      if (flags & PAN_BO_GROWABLE)
-         create_bo.flags |= PANFROST_BO_HEAP;
-      if (!(flags & PAN_BO_EXECUTE))
-         create_bo.flags |= PANFROST_BO_NOEXEC;
-   }
-
-   ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_CREATE_BO, &create_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_CREATE_BO failed: %m\n");
-      return NULL;
-   }
+   kmod_bo = pan_kmod_bo_alloc(dev->kmod.dev, exclusive_vm, size,
+                               to_kmod_bo_flags(flags));
+   assert(kmod_bo);
 
-   bo = pan_lookup_bo(dev, create_bo.handle);
+   bo = pan_lookup_bo(dev, kmod_bo->handle);
    assert(!memcmp(bo, &((struct panfrost_bo){}), sizeof(*bo)));
+   bo->kmod_bo = kmod_bo;
 
-   bo->size = create_bo.size;
-   bo->ptr.gpu = create_bo.offset;
-   bo->gem_handle = create_bo.handle;
+   bo->ptr.gpu = pan_kmod_vm_map(dev->kmod.vm, bo->kmod_bo,
+                                 PAN_KMOD_VM_MAP_AUTO_VA, 0, bo->kmod_bo->size);
    bo->flags = flags;
    bo->dev = dev;
    bo->label = label;
@@ -92,18 +99,15 @@ panfrost_bo_alloc(struct panfrost_device
 static void
 panfrost_bo_free(struct panfrost_bo *bo)
 {
-   struct drm_gem_close gem_close = {.handle = bo->gem_handle};
-   int fd = bo->dev->fd;
-   int ret;
+   struct pan_kmod_bo *kmod_bo = bo->kmod_bo;
+   struct pan_kmod_vm *vm = bo->dev->kmod.vm;
+   uint64_t va = bo->ptr.gpu;
 
    /* BO will be freed with the sparse array, but zero to indicate free */
    memset(bo, 0, sizeof(*bo));
 
-   ret = drmIoctl(fd, DRM_IOCTL_GEM_CLOSE, &gem_close);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_GEM_CLOSE failed: %m\n");
-      assert(0);
-   }
+   pan_kmod_vm_unmap(vm, va, kmod_bo->size);
+   pan_kmod_bo_free(kmod_bo);
 }
 
 /* Returns true if the BO is ready, false otherwise.
@@ -114,12 +118,6 @@ panfrost_bo_free(struct panfrost_bo *bo)
 bool
 panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns, bool wait_readers)
 {
-   struct drm_panfrost_wait_bo req = {
-      .handle = bo->gem_handle,
-      .timeout_ns = timeout_ns,
-   };
-   int ret;
-
    /* If the BO has been exported or imported we can't rely on the cached
     * state, we need to call the WAIT_BO ioctl.
     */
@@ -135,11 +133,7 @@ panfrost_bo_wait(struct panfrost_bo *bo,
          return true;
    }
 
-   /* The ioctl returns >= 0 value when the BO we are waiting for is ready
-    * -1 otherwise.
-    */
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_WAIT_BO, &req);
-   if (ret != -1) {
+   if (pan_kmod_bo_wait(bo->kmod_bo, timeout_ns, !wait_readers)) {
       /* Set gpu_access to 0 so that the next call to bo_wait()
        * doesn't have to call the WAIT_BO ioctl.
        */
@@ -147,10 +141,6 @@ panfrost_bo_wait(struct panfrost_bo *bo,
       return true;
    }
 
-   /* If errno is not ETIMEDOUT or EBUSY that means the handle we passed
-    * is invalid, which shouldn't happen here.
-    */
-   assert(errno == ETIMEDOUT || errno == EBUSY);
    return false;
 }
 
@@ -193,26 +183,19 @@ panfrost_bo_cache_fetch(struct panfrost_
 
    /* Iterate the bucket looking for something suitable */
    list_for_each_entry_safe(struct panfrost_bo, entry, bucket, bucket_link) {
-      if (entry->size < size || entry->flags != flags)
+      if (panfrost_bo_size(entry) < size || entry->flags != flags)
          continue;
 
       /* If the oldest BO in the cache is busy, likely so is
        * everything newer, so bail. */
-      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, PAN_BO_ACCESS_RW))
+      if (!panfrost_bo_wait(entry, dontwait ? 0 : INT64_MAX, true))
          break;
 
-      struct drm_panfrost_madvise madv = {
-         .handle = entry->gem_handle,
-         .madv = PANFROST_MADV_WILLNEED,
-      };
-      int ret;
-
       /* This one works, splice it out of the cache */
       list_del(&entry->bucket_link);
       list_del(&entry->lru_link);
 
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
-      if (!ret && !madv.retained) {
+      if (!pan_kmod_bo_make_unevictable(entry->kmod_bo)) {
          panfrost_bo_free(entry);
          continue;
       }
@@ -265,15 +248,10 @@ panfrost_bo_cache_put(struct panfrost_bo
    /* Must be first */
    pthread_mutex_lock(&dev->bo_cache.lock);
 
-   struct list_head *bucket = pan_bucket(dev, MAX2(bo->size, 4096));
-   struct drm_panfrost_madvise madv;
+   struct list_head *bucket = pan_bucket(dev, MAX2(panfrost_bo_size(bo), 4096));
    struct timespec time;
 
-   madv.handle = bo->gem_handle;
-   madv.madv = PANFROST_MADV_DONTNEED;
-   madv.retained = 0;
-
-   drmIoctl(dev->fd, DRM_IOCTL_PANFROST_MADVISE, &madv);
+   pan_kmod_bo_make_evictable(bo->kmod_bo);
 
    /* Add us to the bucket */
    list_addtail(&bo->bucket_link, bucket);
@@ -321,26 +299,15 @@ panfrost_bo_cache_evict_all(struct panfr
 void
 panfrost_bo_mmap(struct panfrost_bo *bo)
 {
-   struct drm_panfrost_mmap_bo mmap_bo = {.handle = bo->gem_handle};
-   int ret;
-
    if (bo->ptr.cpu)
       return;
 
-   ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PANFROST_MMAP_BO, &mmap_bo);
-   if (ret) {
-      fprintf(stderr, "DRM_IOCTL_PANFROST_MMAP_BO failed: %m\n");
-      assert(0);
-   }
-
-   bo->ptr.cpu = os_mmap(NULL, bo->size, PROT_READ | PROT_WRITE, MAP_SHARED,
-                         bo->dev->fd, mmap_bo.offset);
+   bo->ptr.cpu = pan_kmod_bo_mmap(bo->kmod_bo, 0, panfrost_bo_size(bo),
+                                  PROT_READ | PROT_WRITE, MAP_SHARED);
    if (bo->ptr.cpu == MAP_FAILED) {
       bo->ptr.cpu = NULL;
-      fprintf(stderr,
-              "mmap failed: result=%p size=0x%llx fd=%i offset=0x%llx %m\n",
-              bo->ptr.cpu, (long long)bo->size, bo->dev->fd,
-              (long long)mmap_bo.offset);
+      fprintf(stderr, "mmap failed: result=%p size=0x%llx\n", bo->ptr.cpu,
+              (long long)panfrost_bo_size(bo));
    }
 }
 
@@ -350,7 +317,7 @@ panfrost_bo_munmap(struct panfrost_bo *b
    if (!bo->ptr.cpu)
       return;
 
-   if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, bo->size)) {
+   if (os_munmap((void *)(uintptr_t)bo->ptr.cpu, panfrost_bo_size(bo))) {
       perror("munmap");
       abort();
    }
@@ -405,11 +372,11 @@ panfrost_bo_create(struct panfrost_devic
 
    if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC)) {
       if (flags & PAN_BO_INVISIBLE)
-         pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, NULL, bo->size,
-                               NULL);
+         pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, NULL,
+                               panfrost_bo_size(bo), NULL);
       else if (!(flags & PAN_BO_DELAY_MMAP))
          pandecode_inject_mmap(dev->decode_ctx, bo->ptr.gpu, bo->ptr.cpu,
-                               bo->size, NULL);
+                               panfrost_bo_size(bo), NULL);
    }
 
    return bo;
@@ -446,7 +413,8 @@ panfrost_bo_unreference(struct panfrost_
       panfrost_bo_munmap(bo);
 
       if (dev->debug & (PAN_DBG_TRACE | PAN_DBG_SYNC))
-         pandecode_inject_free(dev->decode_ctx, bo->ptr.gpu, bo->size);
+         pandecode_inject_free(dev->decode_ctx, bo->ptr.gpu,
+                               panfrost_bo_size(bo));
 
       /* Rather than freeing the BO now, we'll cache the BO for later
        * allocations if we're allowed to.
@@ -461,37 +429,22 @@ struct panfrost_bo *
 panfrost_bo_import(struct panfrost_device *dev, int fd)
 {
    struct panfrost_bo *bo;
-   struct drm_panfrost_get_bo_offset get_bo_offset = {
-      0,
-   };
    ASSERTED int ret;
    unsigned gem_handle;
 
    pthread_mutex_lock(&dev->bo_map_lock);
-
-   ret = drmPrimeFDToHandle(dev->fd, fd, &gem_handle);
+   ret = drmPrimeFDToHandle(dev->kmod.dev->fd, fd, &gem_handle);
    assert(!ret);
 
    bo = pan_lookup_bo(dev, gem_handle);
 
    if (!bo->dev) {
-      get_bo_offset.handle = gem_handle;
-      ret = drmIoctl(dev->fd, DRM_IOCTL_PANFROST_GET_BO_OFFSET, &get_bo_offset);
-      assert(!ret);
-
       bo->dev = dev;
-      bo->ptr.gpu = (mali_ptr)get_bo_offset.offset;
-      bo->size = lseek(fd, 0, SEEK_END);
-      /* Sometimes this can fail and return -1. size of -1 is not
-       * a nice thing for mmap to try mmap. Be more robust also
-       * for zero sized maps and fail nicely too
-       */
-      if ((bo->size == 0) || (bo->size == (size_t)-1)) {
-         pthread_mutex_unlock(&dev->bo_map_lock);
-         return NULL;
-      }
+      bo->kmod_bo = pan_kmod_bo_import(dev->kmod.dev, fd);
+      bo->ptr.gpu =
+         pan_kmod_vm_map(dev->kmod.vm, bo->kmod_bo, PAN_KMOD_VM_MAP_AUTO_VA, 0,
+                         panfrost_bo_size(bo));
       bo->flags = PAN_BO_SHARED;
-      bo->gem_handle = gem_handle;
       p_atomic_set(&bo->refcnt, 1);
    } else {
       /* bo->refcnt == 0 can happen if the BO
@@ -517,15 +470,9 @@ panfrost_bo_import(struct panfrost_devic
 int
 panfrost_bo_export(struct panfrost_bo *bo)
 {
-   struct drm_prime_handle args = {
-      .handle = bo->gem_handle,
-      .flags = DRM_CLOEXEC,
-   };
-
-   int ret = drmIoctl(bo->dev->fd, DRM_IOCTL_PRIME_HANDLE_TO_FD, &args);
-   if (ret == -1)
-      return -1;
+   int ret = pan_kmod_bo_export(bo->kmod_bo);
+   if (ret >= 0)
+      bo->flags |= PAN_BO_SHARED;
 
-   bo->flags |= PAN_BO_SHARED;
-   return args.fd;
+   return ret;
 }
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_bo.h	2023-11-24 23:33:24.964610841 +0100
@@ -30,6 +30,8 @@
 #include "util/list.h"
 #include "panfrost-job.h"
 
+#include "kmod/pan_kmod.h"
+
 /* Flags for allocated memory */
 
 /* This memory region is executable */
@@ -50,6 +52,13 @@
  * cached locally */
 #define PAN_BO_SHARED (1 << 4)
 
+/* BO might be exported at some point. PAN_BO_SHAREABLE does not imply
+ * PAN_BO_SHARED if the BO has not been exported yet */
+#define PAN_BO_SHAREABLE (1 << 5)
+
+/* BO should be mapped uncached in the GPU VM. */
+#define PAN_BO_GPU_UNCACHED (1 << 6)
+
 /* GPU access flags */
 
 /* BO is either shared (can be accessed by more than one GPU batch) or private
@@ -95,16 +104,14 @@ struct panfrost_bo {
    /* Atomic reference count */
    int32_t refcnt;
 
+   /* Kernel representation of a buffer object. */
+   struct pan_kmod_bo *kmod_bo;
+
    struct panfrost_device *dev;
 
    /* Mapping for the entire object (all levels) */
    struct panfrost_ptr ptr;
 
-   /* Size of all entire trees */
-   size_t size;
-
-   int gem_handle;
-
    uint32_t flags;
 
    /* Combination of PAN_BO_ACCESS_{READ,WRITE} flags encoding pending
@@ -117,6 +124,18 @@ struct panfrost_bo {
    const char *label;
 };
 
+static inline size_t
+panfrost_bo_size(struct panfrost_bo *bo)
+{
+   return bo->kmod_bo->size;
+}
+
+static inline size_t
+panfrost_bo_handle(struct panfrost_bo *bo)
+{
+   return bo->kmod_bo->handle;
+}
+
 bool panfrost_bo_wait(struct panfrost_bo *bo, int64_t timeout_ns,
                       bool wait_readers);
 void panfrost_bo_reference(struct panfrost_bo *bo);
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.c	2023-11-24 23:33:24.964610841 +0100
@@ -446,40 +446,6 @@ pan_rt_init_format(const struct pan_imag
    cfg->swizzle = panfrost_translate_swizzle_4(swizzle);
 }
 
-#if PAN_ARCH >= 9
-enum mali_afbc_compression_mode
-pan_afbc_compression_mode(enum pipe_format format)
-{
-   /* There's a special case for texturing the stencil part from a combined
-    * depth/stencil texture, handle it separately.
-    */
-   if (format == PIPE_FORMAT_X24S8_UINT)
-      return MALI_AFBC_COMPRESSION_MODE_X24S8;
-
-   /* Otherwise, map canonical formats to the hardware enum. This only
-    * needs to handle the subset of formats returned by
-    * panfrost_afbc_format.
-    */
-   /* clang-format off */
-   switch (panfrost_afbc_format(PAN_ARCH, format)) {
-   case PAN_AFBC_MODE_R8:          return MALI_AFBC_COMPRESSION_MODE_R8;
-   case PAN_AFBC_MODE_R8G8:        return MALI_AFBC_COMPRESSION_MODE_R8G8;
-   case PAN_AFBC_MODE_R5G6B5:      return MALI_AFBC_COMPRESSION_MODE_R5G6B5;
-   case PAN_AFBC_MODE_R4G4B4A4:    return MALI_AFBC_COMPRESSION_MODE_R4G4B4A4;
-   case PAN_AFBC_MODE_R5G5B5A1:    return MALI_AFBC_COMPRESSION_MODE_R5G5B5A1;
-   case PAN_AFBC_MODE_R8G8B8:      return MALI_AFBC_COMPRESSION_MODE_R8G8B8;
-   case PAN_AFBC_MODE_R8G8B8A8:    return MALI_AFBC_COMPRESSION_MODE_R8G8B8A8;
-   case PAN_AFBC_MODE_R10G10B10A2: return MALI_AFBC_COMPRESSION_MODE_R10G10B10A2;
-   case PAN_AFBC_MODE_R11G11B10:   return MALI_AFBC_COMPRESSION_MODE_R11G11B10;
-   case PAN_AFBC_MODE_S8:          return MALI_AFBC_COMPRESSION_MODE_S8;
-   case PAN_AFBC_MODE_INVALID:     unreachable("Invalid AFBC format");
-   }
-   /* clang-format on */
-
-   unreachable("all AFBC formats handled");
-}
-#endif
-
 static void
 pan_prepare_rt(const struct pan_fb_info *fb, unsigned idx, unsigned cbuf_offset,
                struct MALI_RENDER_TARGET *cfg)
@@ -540,7 +506,7 @@ pan_prepare_rt(const struct pan_fb_info
       cfg->afbc.body_offset = surf.afbc.body - surf.afbc.header;
       assert(surf.afbc.body >= surf.afbc.header);
 
-      cfg->afbc.compression_mode = pan_afbc_compression_mode(rt->format);
+      cfg->afbc.compression_mode = GENX(pan_afbc_compression_mode)(rt->format);
       cfg->afbc.row_stride = row_stride;
 #else
       const struct pan_image_slice_layout *slice = &image->layout.slices[level];
@@ -638,7 +604,8 @@ pan_emit_midgard_tiler(const struct panf
          cfg.polygon_list_size = panfrost_tiler_full_size(
             fb->width, fb->height, cfg.hierarchy_mask, hierarchy);
          cfg.heap_start = dev->tiler_heap->ptr.gpu;
-         cfg.heap_end = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
+         cfg.heap_end =
+            dev->tiler_heap->ptr.gpu + panfrost_bo_size(dev->tiler_heap);
       }
 
       cfg.polygon_list = tiler_ctx->midgard.polygon_list->ptr.gpu;
@@ -754,6 +721,7 @@ GENX(pan_emit_fbd)(const struct panfrost
 
       cfg.sample_locations =
          panfrost_sample_positions(dev, pan_sample_pattern(fb->nr_samples));
+      assert(cfg.sample_locations != 0);
       cfg.pre_frame_0 = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[0],
                                                   force_clean_write);
       cfg.pre_frame_1 = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[1],
@@ -761,7 +729,7 @@ GENX(pan_emit_fbd)(const struct panfrost
       cfg.post_frame = pan_fix_frame_shader_mode(fb->bifrost.pre_post.modes[2],
                                                  force_clean_write);
       cfg.frame_shader_dcds = fb->bifrost.pre_post.dcds.gpu;
-      cfg.tiler = tiler_ctx->bifrost;
+      cfg.tiler = tiler_ctx->bifrost.ctx;
 #endif
       cfg.width = fb->width;
       cfg.height = fb->height;
@@ -965,24 +933,26 @@ void
 GENX(pan_emit_tiler_heap)(const struct panfrost_device *dev, void *out)
 {
    pan_pack(out, TILER_HEAP, heap) {
-      heap.size = dev->tiler_heap->size;
+      heap.size = dev->tiler_heap->kmod_bo->size;
       heap.base = dev->tiler_heap->ptr.gpu;
       heap.bottom = dev->tiler_heap->ptr.gpu;
-      heap.top = dev->tiler_heap->ptr.gpu + dev->tiler_heap->size;
+      heap.top = dev->tiler_heap->ptr.gpu + panfrost_bo_size(dev->tiler_heap);
    }
 }
 
 void
 GENX(pan_emit_tiler_ctx)(const struct panfrost_device *dev, unsigned fb_width,
                          unsigned fb_height, unsigned nr_samples,
-                         bool first_provoking_vertex, mali_ptr heap, void *out)
+                         bool first_provoking_vertex, mali_ptr heap,
+                         mali_ptr tmp_geom_buf, size_t tmp_geom_buf_size,
+                         void *out)
 {
    unsigned max_levels = dev->tiler_features.max_levels;
    assert(max_levels >= 2);
 
    pan_pack(out, TILER_CONTEXT, tiler) {
       /* TODO: Select hierarchy mask more effectively */
-      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFF : 0x28;
+      tiler.hierarchy_mask = (max_levels >= 8) ? 0xFE : 0x28;
 
       /* For large framebuffers, disable the smallest bin size to
        * avoid pathological tiler memory usage. Required to avoid OOM
@@ -999,10 +969,16 @@ GENX(pan_emit_tiler_ctx)(const struct pa
 #if PAN_ARCH >= 9
       tiler.first_provoking_vertex = first_provoking_vertex;
 #endif
+#if PAN_ARCH >= 10
+      /* Note: DDK assigns this pointer in the CS. */
+      tiler.geometry_buffer = tmp_geom_buf;
+      tiler.geometry_buffer_size = tmp_geom_buf_size;
+#endif
    }
 }
 #endif
 
+#if PAN_ARCH <= 9
 void
 GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
                             void *out)
@@ -1028,3 +1004,4 @@ GENX(pan_emit_fragment_job)(const struct
 #endif
    }
 }
+#endif
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_cs.h	2023-11-24 23:33:24.964610841 +0100
@@ -76,11 +76,14 @@ struct pan_tiler_context {
    uint32_t vertex_count;
 
    union {
-      mali_ptr bifrost;
       struct {
          bool disable;
          struct panfrost_bo *polygon_list;
       } midgard;
+      struct {
+         mali_ptr ctx;
+         mali_ptr heap;
+      } bifrost;
    };
 };
 
@@ -168,11 +171,15 @@ void GENX(pan_emit_tiler_heap)(const str
 void GENX(pan_emit_tiler_ctx)(const struct panfrost_device *dev,
                               unsigned fb_width, unsigned fb_height,
                               unsigned nr_samples, bool first_provoking_vertex,
-                              mali_ptr heap, void *out);
+                              mali_ptr heap, mali_ptr tmp_geom_buf,
+                              size_t tmp_geom_buf_size, void *out);
 #endif
 
+#if PAN_ARCH <= 9
 void GENX(pan_emit_fragment_job)(const struct pan_fb_info *fb, mali_ptr fbd,
                                  void *out);
+#endif
+
 #endif /* ifdef PAN_ARCH */
 
 #endif
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_device.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_device.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_device.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_device.h	2023-11-24 23:33:24.964610841 +0100
@@ -36,11 +36,14 @@
 #include "util/list.h"
 #include "util/sparse_array.h"
 #include "util/u_dynarray.h"
+#include "util/vma.h"
 
 #include "panfrost/util/pan_ir.h"
 #include "pan_pool.h"
 #include "pan_util.h"
 
+#include "kmod/pan_kmod.h"
+
 #include <genxml/gen_macros.h>
 
 #if defined(__cplusplus)
@@ -128,15 +131,17 @@ struct panfrost_device {
    /* For ralloc */
    void *memctx;
 
-   int fd;
+   struct {
+      struct pan_kmod_dev *dev;
+      struct pan_kmod_vm *vm;
+      struct pan_kmod_dev_props props;
+   } kmod;
 
    /* For pandecode */
    struct pandecode_context *decode_ctx;
 
    /* Properties of the GPU in use */
    unsigned arch;
-   unsigned gpu_id;
-   unsigned revision;
 
    /* Number of shader cores */
    unsigned core_count;
@@ -163,8 +168,6 @@ struct panfrost_device {
    /* debug flags, see pan_util.h how to interpret */
    unsigned debug;
 
-   drmVersionPtr kernel_version;
-
    struct renderonly *ro;
 
    pthread_mutex_t bo_map_lock;
@@ -214,6 +217,36 @@ struct panfrost_device {
    struct panfrost_bo *sample_positions;
 };
 
+static inline int
+panfrost_device_fd(const struct panfrost_device *dev)
+{
+   return dev->kmod.dev->fd;
+}
+
+static inline uint32_t
+panfrost_device_gpu_id(const struct panfrost_device *dev)
+{
+   return dev->kmod.props.gpu_prod_id;
+}
+
+static inline uint32_t
+panfrost_device_gpu_rev(const struct panfrost_device *dev)
+{
+   return dev->kmod.props.gpu_revision;
+}
+
+static inline int
+panfrost_device_kmod_version_major(const struct panfrost_device *dev)
+{
+   return dev->kmod.dev->driver.version.major;
+}
+
+static inline int
+panfrost_device_kmod_version_minor(const struct panfrost_device *dev)
+{
+   return dev->kmod.dev->driver.version.minor;
+}
+
 void panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev);
 
 void panfrost_close_device(struct panfrost_device *dev);
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_indirect_dispatch.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_indirect_dispatch.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_indirect_dispatch.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_indirect_dispatch.c	2023-11-24 23:33:24.964610841 +0100
@@ -121,7 +121,7 @@ pan_indirect_dispatch_init(struct panfro
    nir_pop_if(&b, NULL);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = dev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(dev),
       .no_ubo_to_push = true,
    };
    struct pan_shader_info shader_info;
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_props.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_props.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_props.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_props.c	2023-11-24 23:33:24.964610841 +0100
@@ -27,8 +27,10 @@
 #include <xf86drm.h>
 
 #include "drm-uapi/panfrost_drm.h"
+#include "drm-uapi/panthor_drm.h"
 #include "util/hash_table.h"
 #include "util/macros.h"
+#include "util/os_mman.h"
 #include "util/u_math.h"
 #include "util/u_thread.h"
 #include "pan_bo.h"
@@ -71,6 +73,8 @@ const struct panfrost_model panfrost_mod
         MODEL(0x7402, "G52 r1", "TGOx", HAS_ANISO,         16384, {}),
         MODEL(0x9091, "G57",    "TNAx", HAS_ANISO,         16384, {}),
         MODEL(0x9093, "G57",    "TNAx", HAS_ANISO,         16384, {}),
+
+        MODEL(0xa867, "G610",   "TNAx", HAS_ANISO,         16384, {}), // TODO
 };
 /* clang-format on */
 
@@ -93,58 +97,18 @@ panfrost_get_model(uint32_t gpu_id)
    return NULL;
 }
 
-/* Abstraction over the raw drm_panfrost_get_param ioctl for fetching
- * information about devices */
-
-static __u64
-panfrost_query_raw(int fd, enum drm_panfrost_param param, bool required,
-                   unsigned default_value)
-{
-   struct drm_panfrost_get_param get_param = {
-      0,
-   };
-   ASSERTED int ret;
-
-   get_param.param = param;
-   ret = drmIoctl(fd, DRM_IOCTL_PANFROST_GET_PARAM, &get_param);
-
-   if (ret) {
-      assert(!required);
-      return default_value;
-   }
-
-   return get_param.value;
-}
-
-static unsigned
-panfrost_query_gpu_version(int fd)
-{
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_PROD_ID, true, 0);
-}
-
-static unsigned
-panfrost_query_gpu_revision(int fd)
-{
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_GPU_REVISION, true, 0);
-}
-
 unsigned
 panfrost_query_l2_slices(const struct panfrost_device *dev)
 {
-   /* Query MEM_FEATURES register */
-   uint32_t mem_features =
-      panfrost_query_raw(dev->fd, DRM_PANFROST_PARAM_MEM_FEATURES, true, 0);
-
    /* L2_SLICES is MEM_FEATURES[11:8] minus(1) */
-   return ((mem_features >> 8) & 0xF) + 1;
+   return ((dev->kmod.props.mem_features >> 8) & 0xF) + 1;
 }
 
 static struct panfrost_tiler_features
-panfrost_query_tiler_features(int fd)
+panfrost_query_tiler_features(const struct panfrost_device *dev)
 {
    /* Default value (2^9 bytes and 8 levels) to match old behaviour */
-   uint32_t raw =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_TILER_FEATURES, false, 0x809);
+   uint32_t raw = dev->kmod.props.tiler_features;
 
    /* Bin size is log2 in the first byte, max levels in the second byte */
    return (struct panfrost_tiler_features){
@@ -154,12 +118,12 @@ panfrost_query_tiler_features(int fd)
 }
 
 static unsigned
-panfrost_query_core_count(int fd, unsigned *core_id_range)
+panfrost_query_core_count(const struct panfrost_device *dev,
+                          unsigned *core_id_range)
 {
    /* On older kernels, worst-case to 16 cores */
 
-   unsigned mask =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_SHADER_PRESENT, false, 0xffff);
+   unsigned mask = dev->kmod.props.shader_present;
 
    /* Some cores might be absent. In some cases, we care
     * about the range of core IDs (that is, the greatest core ID + 1). If
@@ -172,31 +136,18 @@ panfrost_query_core_count(int fd, unsign
 }
 
 static unsigned
-panfrost_query_thread_tls_alloc(int fd, unsigned major)
+panfrost_query_thread_tls_alloc(const struct panfrost_device *dev,
+                                unsigned major)
 {
-   unsigned tls =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_THREAD_TLS_ALLOC, false, 0);
+   unsigned tls = dev->kmod.props.thread_tls_alloc;
 
    return (tls > 0) ? tls : panfrost_max_thread_count(major, 0);
 }
 
 static uint32_t
-panfrost_query_compressed_formats(int fd)
+panfrost_query_compressed_formats(const struct panfrost_device *dev)
 {
-   /* If unspecified, assume ASTC/ETC only. Factory default for Juno, and
-    * should exist on any Mali configuration. All hardware should report
-    * these texture formats but the kernel might not be new enough. */
-
-   uint32_t default_set = (1 << MALI_ETC2_RGB8) | (1 << MALI_ETC2_R11_UNORM) |
-                          (1 << MALI_ETC2_RGBA8) | (1 << MALI_ETC2_RG11_UNORM) |
-                          (1 << MALI_ETC2_R11_SNORM) |
-                          (1 << MALI_ETC2_RG11_SNORM) |
-                          (1 << MALI_ETC2_RGB8A1) | (1 << MALI_ASTC_3D_LDR) |
-                          (1 << MALI_ASTC_3D_HDR) | (1 << MALI_ASTC_2D_LDR) |
-                          (1 << MALI_ASTC_2D_HDR);
-
-   return panfrost_query_raw(fd, DRM_PANFROST_PARAM_TEXTURE_FEATURES0, false,
-                             default_set);
+   return dev->kmod.props.texture_features[0];
 }
 
 /* DRM_PANFROST_PARAM_TEXTURE_FEATURES0 will return a bitmask of supported
@@ -218,10 +169,9 @@ panfrost_supports_compressed_format(stru
  * may omit it, signaled as a nonzero value in the AFBC_FEATURES property. */
 
 static bool
-panfrost_query_afbc(int fd, unsigned arch)
+panfrost_query_afbc(struct panfrost_device *dev, unsigned arch)
 {
-   unsigned reg =
-      panfrost_query_raw(fd, DRM_PANFROST_PARAM_AFBC_FEATURES, false, 0);
+   unsigned reg = dev->kmod.props.afbc_features;
 
    return (arch >= 5) && (reg == 0);
 }
@@ -248,27 +198,33 @@ panfrost_query_optimal_tib_size(const st
 void
 panfrost_open_device(void *memctx, int fd, struct panfrost_device *dev)
 {
-   dev->fd = fd;
    dev->memctx = memctx;
-   dev->gpu_id = panfrost_query_gpu_version(fd);
-   dev->arch = pan_arch(dev->gpu_id);
-   dev->kernel_version = drmGetVersion(fd);
-   dev->revision = panfrost_query_gpu_revision(fd);
-   dev->model = panfrost_get_model(dev->gpu_id);
 
-   if (!dev->kernel_version)
-      return;
+   dev->kmod.dev = pan_kmod_dev_create(fd, NULL);
+   if (!dev->kmod.dev)
+      goto err_close_fd;
+
+   pan_kmod_dev_query_props(dev->kmod.dev, &dev->kmod.props);
+
+   /* 4G address space, with the lower 32MB reserved. */
+   dev->kmod.vm = pan_kmod_vm_create(dev->kmod.dev, PAN_KMOD_VM_FLAG_AUTO_VA,
+                                     0x2000000, (1ull << 32) - 0x2000000);
+   if (!dev->kmod.vm)
+      goto err_free_kmod_dev;
+
+   dev->arch = pan_arch(dev->kmod.props.gpu_prod_id);
+   dev->model = panfrost_get_model(dev->kmod.props.gpu_prod_id);
 
    /* If we don't recognize the model, bail early */
    if (!dev->model)
-      return;
+      goto err_free_kmod_vm;
 
-   dev->core_count = panfrost_query_core_count(fd, &dev->core_id_range);
-   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(fd, dev->arch);
+   dev->core_count = panfrost_query_core_count(dev, &dev->core_id_range);
+   dev->thread_tls_alloc = panfrost_query_thread_tls_alloc(dev, dev->arch);
    dev->optimal_tib_size = panfrost_query_optimal_tib_size(dev);
-   dev->compressed_formats = panfrost_query_compressed_formats(fd);
-   dev->tiler_features = panfrost_query_tiler_features(fd);
-   dev->has_afbc = panfrost_query_afbc(fd, dev->arch);
+   dev->compressed_formats = panfrost_query_compressed_formats(dev);
+   dev->tiler_features = panfrost_query_tiler_features(dev);
+   dev->has_afbc = panfrost_query_afbc(dev, dev->arch);
 
    if (dev->arch <= 6)
       dev->formats = panfrost_pipe_format_v6;
@@ -293,13 +249,28 @@ panfrost_open_device(void *memctx, int f
     * active for a single job chain at once, so a single heap can be
     * shared across batches/contextes */
 
-   dev->tiler_heap = panfrost_bo_create(
-      dev, 128 * 1024 * 1024, PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
+   if (dev->arch < 10) {
+      dev->tiler_heap =
+         panfrost_bo_create(dev, 128 * 1024 * 1024,
+                            PAN_BO_INVISIBLE | PAN_BO_GROWABLE, "Tiler heap");
+   }
 
    pthread_mutex_init(&dev->submit_lock, NULL);
 
    /* Done once on init */
    panfrost_upload_sample_positions(dev);
+   return;
+
+err_free_kmod_vm:
+   pan_kmod_vm_destroy(dev->kmod.vm);
+   dev->kmod.vm = NULL;
+
+err_free_kmod_dev:
+   pan_kmod_dev_destroy(dev->kmod.dev);
+   dev->kmod.dev = NULL;
+
+err_close_fd:
+   close(fd);
 }
 
 void
@@ -310,13 +281,17 @@ panfrost_close_device(struct panfrost_de
     */
    if (dev->model) {
       pthread_mutex_destroy(&dev->submit_lock);
-      panfrost_bo_unreference(dev->tiler_heap);
+      if (dev->tiler_heap)
+         panfrost_bo_unreference(dev->tiler_heap);
       panfrost_bo_unreference(dev->sample_positions);
       panfrost_bo_cache_evict_all(dev);
       pthread_mutex_destroy(&dev->bo_cache.lock);
       util_sparse_array_finish(&dev->bo_map);
    }
 
-   drmFreeVersion(dev->kernel_version);
-   close(dev->fd);
+   if (dev->kmod.vm)
+      pan_kmod_vm_destroy(dev->kmod.vm);
+
+   if (dev->kmod.dev)
+      pan_kmod_dev_destroy(dev->kmod.dev);
 }
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_scoreboard.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_scoreboard.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_scoreboard.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_scoreboard.h	2023-11-24 23:33:24.964610841 +0100
@@ -54,7 +54,7 @@ struct pan_scoreboard {
    unsigned write_value_index;
 };
 
-#ifdef PAN_ARCH
+#if defined(PAN_ARCH) && PAN_ARCH <= 9
 /*
  * There are various types of Mali jobs:
  *
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.c.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.c
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.c	2023-11-24 23:33:24.964610841 +0100
@@ -517,7 +517,7 @@ panfrost_emit_plane(const struct pan_ima
          cfg.afbc.ytr = (layout->modifier & AFBC_FORMAT_MOD_YTR);
          cfg.afbc.tiled_header = (layout->modifier & AFBC_FORMAT_MOD_TILED);
          cfg.afbc.prefetch = true;
-         cfg.afbc.compression_mode = pan_afbc_compression_mode(format);
+         cfg.afbc.compression_mode = GENX(pan_afbc_compression_mode)(format);
          cfg.afbc.header_stride = layout->slices[level].afbc.header_size;
       } else {
          cfg.plane_type = is_3_planar_yuv ? MALI_PLANE_TYPE_CHROMA_2P
@@ -786,3 +786,37 @@ GENX(panfrost_new_texture)(const struct
 #endif
    }
 }
+
+#if PAN_ARCH >= 9
+enum mali_afbc_compression_mode
+GENX(pan_afbc_compression_mode)(enum pipe_format format)
+{
+   /* There's a special case for texturing the stencil part from a combined
+    * depth/stencil texture, handle it separately.
+    */
+   if (format == PIPE_FORMAT_X24S8_UINT)
+      return MALI_AFBC_COMPRESSION_MODE_X24S8;
+
+   /* Otherwise, map canonical formats to the hardware enum. This only
+    * needs to handle the subset of formats returned by
+    * panfrost_afbc_format.
+    */
+   /* clang-format off */
+   switch (panfrost_afbc_format(PAN_ARCH, format)) {
+   case PAN_AFBC_MODE_R8:          return MALI_AFBC_COMPRESSION_MODE_R8;
+   case PAN_AFBC_MODE_R8G8:        return MALI_AFBC_COMPRESSION_MODE_R8G8;
+   case PAN_AFBC_MODE_R5G6B5:      return MALI_AFBC_COMPRESSION_MODE_R5G6B5;
+   case PAN_AFBC_MODE_R4G4B4A4:    return MALI_AFBC_COMPRESSION_MODE_R4G4B4A4;
+   case PAN_AFBC_MODE_R5G5B5A1:    return MALI_AFBC_COMPRESSION_MODE_R5G5B5A1;
+   case PAN_AFBC_MODE_R8G8B8:      return MALI_AFBC_COMPRESSION_MODE_R8G8B8;
+   case PAN_AFBC_MODE_R8G8B8A8:    return MALI_AFBC_COMPRESSION_MODE_R8G8B8A8;
+   case PAN_AFBC_MODE_R10G10B10A2: return MALI_AFBC_COMPRESSION_MODE_R10G10B10A2;
+   case PAN_AFBC_MODE_R11G11B10:   return MALI_AFBC_COMPRESSION_MODE_R11G11B10;
+   case PAN_AFBC_MODE_S8:          return MALI_AFBC_COMPRESSION_MODE_S8;
+   case PAN_AFBC_MODE_INVALID:     unreachable("Invalid AFBC format");
+   }
+   /* clang-format on */
+
+   unreachable("all AFBC formats handled");
+}
+#endif
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_texture.h	2023-11-24 23:33:24.964610841 +0100
@@ -233,8 +233,6 @@ enum pan_afbc_mode panfrost_afbc_format(
 
 bool panfrost_afbc_can_ytr(enum pipe_format format);
 
-bool panfrost_afbc_can_pack(enum pipe_format format);
-
 bool panfrost_afbc_can_tile(const struct panfrost_device *dev);
 
 /*
@@ -328,7 +326,7 @@ void pan_iview_get_surface(const struct
 
 #if PAN_ARCH >= 9
 enum mali_afbc_compression_mode
-pan_afbc_compression_mode(enum pipe_format format);
+   GENX(pan_afbc_compression_mode)(enum pipe_format format);
 #endif
 
 #ifdef __cplusplus
diff -up mesa-23.3.0-rc5/src/panfrost/lib/pan_util.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/pan_util.h
--- mesa-23.3.0-rc5/src/panfrost/lib/pan_util.h.8~	2023-11-24 23:33:24.937610687 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/pan_util.h	2023-11-24 23:33:24.964610841 +0100
@@ -52,8 +52,7 @@
 #define PAN_DBG_OVERFLOW 0x8000
 #endif
 
-#define PAN_DBG_YUV        0x20000
-#define PAN_DBG_FORCE_PACK 0x40000
+#define PAN_DBG_YUV 0x20000
 
 struct panfrost_device;
 
diff -up mesa-23.3.0-rc5/src/panfrost/lib/wrap.h.8~ mesa-23.3.0-rc5/src/panfrost/lib/wrap.h
--- mesa-23.3.0-rc5/src/panfrost/lib/wrap.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/lib/wrap.h	2023-11-24 23:33:24.964610841 +0100
@@ -67,4 +67,6 @@ void pandecode_cs(struct pandecode_conte
 void pandecode_abort_on_fault(struct pandecode_context *ctx, uint64_t jc_gpu_va,
                               unsigned gpu_id);
 
+void pandecode_dump_mappings(struct pandecode_context *ctx);
+
 #endif /* __MMAP_TRACE_H__ */
diff -up mesa-23.3.0-rc5/src/panfrost/perf/pan_perf.c.8~ mesa-23.3.0-rc5/src/panfrost/perf/pan_perf.c
--- mesa-23.3.0-rc5/src/panfrost/perf/pan_perf.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/perf/pan_perf.c	2023-11-24 23:33:24.964610841 +0100
@@ -92,8 +92,8 @@ static int
 panfrost_perf_query(struct panfrost_perf *perf, uint32_t enable)
 {
    struct drm_panfrost_perfcnt_enable perfcnt_enable = {enable, 0};
-   return drmIoctl(perf->dev->fd, DRM_IOCTL_PANFROST_PERFCNT_ENABLE,
-                   &perfcnt_enable);
+   return drmIoctl(panfrost_device_fd(perf->dev),
+                   DRM_IOCTL_PANFROST_PERFCNT_ENABLE, &perfcnt_enable);
 }
 
 int
@@ -115,6 +115,6 @@ panfrost_perf_dump(struct panfrost_perf
    // counter_values
    struct drm_panfrost_perfcnt_dump perfcnt_dump = {
       (uint64_t)(uintptr_t)perf->counter_values};
-   return drmIoctl(perf->dev->fd, DRM_IOCTL_PANFROST_PERFCNT_DUMP,
-                   &perfcnt_dump);
+   return drmIoctl(panfrost_device_fd(perf->dev),
+                   DRM_IOCTL_PANFROST_PERFCNT_DUMP, &perfcnt_dump);
 }
diff -up mesa-23.3.0-rc5/src/panfrost/util/pan_ir.h.8~ mesa-23.3.0-rc5/src/panfrost/util/pan_ir.h
--- mesa-23.3.0-rc5/src/panfrost/util/pan_ir.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/util/pan_ir.h	2023-11-24 23:33:24.964610841 +0100
@@ -452,6 +452,9 @@ panfrost_max_thread_count(unsigned arch,
       return work_reg_count > 32 ? 384 : 768;
 
    /* Valhall (for completeness) */
+   case 10:
+      return 2048;
+
    default:
       return work_reg_count > 32 ? 512 : 1024;
    }
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/meson.build.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/meson.build
--- mesa-23.3.0-rc5/src/panfrost/vulkan/meson.build.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/meson.build	2023-11-24 23:33:24.964610841 +0100
@@ -35,6 +35,7 @@ panvk_entrypoints = custom_target(
 )
 
 libpanvk_files = files(
+  'panvk_bo.c',
   'panvk_cmd_buffer.c',
   'panvk_cs.c',
   'panvk_device.c',
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_bo.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_bo.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_bo.c.8~	2023-11-24 23:33:24.964610841 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_bo.c	2023-11-24 23:33:24.964610841 +0100
@@ -0,0 +1,49 @@
+#include "panvk_private.h"
+
+struct panvk_bo*
+panvk_bo_alloc(struct panvk_device *dev, size_t size, uint32_t flags, const char *label)
+{
+   struct pan_kmod_vm *exclusive_vm = dev->physical_device->pdev.kmod.vm;
+
+   struct panvk_bo *bo = calloc(1, sizeof *bo);
+
+   bo->kmod_bo = pan_kmod_bo_alloc(dev->physical_device->pdev.kmod.dev, exclusive_vm, size, flags);
+   bo->device_ptr = pan_kmod_vm_map(dev->physical_device->pdev.kmod.vm, bo->kmod_bo, PAN_KMOD_VM_MAP_AUTO_VA, 0, bo->kmod_bo->size);
+
+   return bo;
+}
+
+void
+panvk_bo_free(struct panvk_device *dev, struct panvk_bo *bo)
+{
+   panvk_bo_munmap(dev, bo);
+   pan_kmod_vm_unmap(dev->physical_device->pdev.kmod.vm, bo->device_ptr, bo->kmod_bo->size);
+   pan_kmod_bo_free(bo->kmod_bo);
+   free(bo);
+}
+
+void
+panvk_bo_mmap(struct panvk_device *dev, struct panvk_bo *bo)
+{
+   if (bo->host_ptr)
+      return;
+
+   bo->host_ptr = pan_kmod_bo_mmap(bo->kmod_bo, 0, bo->kmod_bo->size, PROT_READ | PROT_WRITE, MAP_SHARED);
+   if (bo->host_ptr == MAP_FAILED) {
+      bo->host_ptr = NULL;
+      fprintf(stderr, "mmap failed: result=%p size=0x%llx\n", bo->host_ptr,
+              (long long)bo->kmod_bo->size);
+   }
+}
+
+void
+panvk_bo_munmap(struct panvk_device *dev, struct panvk_bo *bo)
+{
+   if (bo->host_ptr) {
+      if (os_munmap((void *)(uintptr_t)bo->host_ptr, bo->kmod_bo->size)) {
+         perror("munmap");
+         abort();
+      }
+      bo->host_ptr = NULL;
+   }
+}
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_device.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_device.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_device.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_device.c	2023-11-24 23:33:24.964610841 +0100
@@ -460,7 +460,8 @@ panvk_physical_device_init(struct panvk_
    memset(device->name, 0, sizeof(device->name));
    sprintf(device->name, "%s", device->pdev.model->name);
 
-   if (panvk_device_get_cache_uuid(device->pdev.gpu_id, device->cache_uuid)) {
+   if (panvk_device_get_cache_uuid(panfrost_device_gpu_id(&device->pdev),
+                                   device->cache_uuid)) {
       result = vk_errorf(instance, VK_ERROR_INITIALIZATION_FAILED,
                          "cannot generate UUID");
       goto fail_close_device;
@@ -471,7 +472,8 @@ panvk_physical_device_init(struct panvk_
    panvk_get_driver_uuid(&device->device_uuid);
    panvk_get_device_uuid(&device->device_uuid);
 
-   device->drm_syncobj_type = vk_drm_syncobj_get_type(device->pdev.fd);
+   device->drm_syncobj_type =
+      vk_drm_syncobj_get_type(panfrost_device_fd(&device->pdev));
    /* We don't support timelines in the uAPI yet and we don't want it getting
     * suddenly turned on by vk_drm_syncobj_get_type() without us adding panvk
     * code for it first.
@@ -800,7 +802,8 @@ panvk_queue_init(struct panvk_device *de
       .flags = DRM_SYNCOBJ_CREATE_SIGNALED,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_CREATE, &create);
+   int ret =
+      drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_CREATE, &create);
    if (ret) {
       vk_queue_finish(&queue->vk);
       return VK_ERROR_OUT_OF_HOST_MEMORY;
@@ -897,7 +900,7 @@ panvk_CreateDevice(VkPhysicalDevice phys
    device->physical_device = physical_device;
 
    const struct panfrost_device *pdev = &physical_device->pdev;
-   vk_device_set_drm_fd(&device->vk, pdev->fd);
+   vk_device_set_drm_fd(&device->vk, panfrost_device_fd(pdev));
 
    for (unsigned i = 0; i < pCreateInfo->queueCreateInfoCount; i++) {
       const VkDeviceQueueCreateInfo *queue_create =
@@ -983,7 +986,7 @@ panvk_QueueWaitIdle(VkQueue _queue)
    };
    int ret;
 
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_WAIT, &wait);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_WAIT, &wait);
    assert(!ret);
 
    return VK_SUCCESS;
@@ -1046,6 +1049,7 @@ panvk_AllocateMemory(VkDevice _device,
 {
    VK_FROM_HANDLE(panvk_device, device, _device);
    struct panvk_device_memory *mem;
+   bool can_be_exported = false;
 
    assert(pAllocateInfo->sType == VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO);
 
@@ -1055,6 +1059,18 @@ panvk_AllocateMemory(VkDevice _device,
       return VK_SUCCESS;
    }
 
+   const VkExportMemoryAllocateInfo *export_info =
+      vk_find_struct_const(pAllocateInfo->pNext, EXPORT_MEMORY_ALLOCATE_INFO);
+
+   if (export_info) {
+      if (export_info->handleTypes &
+          ~(VK_EXTERNAL_MEMORY_HANDLE_TYPE_DMA_BUF_BIT_EXT |
+            VK_EXTERNAL_MEMORY_HANDLE_TYPE_OPAQUE_FD_BIT))
+         return vk_error(device, VK_ERROR_INVALID_EXTERNAL_HANDLE);
+      else if (export_info->handleTypes)
+         can_be_exported = true;
+   }
+
    mem = vk_object_alloc(&device->vk, pAllocator, sizeof(*mem),
                          VK_OBJECT_TYPE_DEVICE_MEMORY);
    if (mem == NULL)
@@ -1080,9 +1096,9 @@ panvk_AllocateMemory(VkDevice _device,
       /* take ownership and close the fd */
       close(fd_info->fd);
    } else {
-      mem->bo = panfrost_bo_create(&device->physical_device->pdev,
-                                   pAllocateInfo->allocationSize, 0,
-                                   "User-requested memory");
+      mem->bo = panfrost_bo_create(
+         &device->physical_device->pdev, pAllocateInfo->allocationSize,
+         can_be_exported ? PAN_BO_SHAREABLE : 0, "User-requested memory");
    }
 
    assert(mem->bo);
@@ -1266,7 +1282,8 @@ panvk_CreateEvent(VkDevice _device, cons
       .flags = 0,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_CREATE, &create);
+   int ret =
+      drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_CREATE, &create);
    if (ret)
       return VK_ERROR_OUT_OF_HOST_MEMORY;
 
@@ -1288,7 +1305,7 @@ panvk_DestroyEvent(VkDevice _device, VkE
       return;
 
    struct drm_syncobj_destroy destroy = {.handle = event->syncobj};
-   drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_DESTROY, &destroy);
+   drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_DESTROY, &destroy);
 
    vk_object_free(&device->vk, pAllocator, event);
 }
@@ -1308,7 +1325,7 @@ panvk_GetEventStatus(VkDevice _device, V
       .flags = DRM_SYNCOBJ_WAIT_FLAGS_WAIT_FOR_SUBMIT,
    };
 
-   int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_WAIT, &wait);
+   int ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_WAIT, &wait);
    if (ret) {
       if (errno == ETIME)
          signaled = false;
@@ -1339,7 +1356,7 @@ panvk_SetEvent(VkDevice _device, VkEvent
     * command executes.
     * https://www.khronos.org/registry/vulkan/specs/1.2/html/chap6.html#commandbuffers-submission-progress
     */
-   if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_SIGNAL, &objs))
+   if (drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_SIGNAL, &objs))
       return VK_ERROR_DEVICE_LOST;
 
    return VK_SUCCESS;
@@ -1356,7 +1373,7 @@ panvk_ResetEvent(VkDevice _device, VkEve
       .handles = (uint64_t)(uintptr_t)&event->syncobj,
       .count_handles = 1};
 
-   if (drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs))
+   if (drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_RESET, &objs))
       return VK_ERROR_DEVICE_LOST;
 
    return VK_SUCCESS;
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_mempool.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_mempool.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_mempool.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_mempool.c	2023-11-24 23:33:24.964610841 +0100
@@ -61,7 +61,7 @@ panvk_pool_alloc_backing(struct panvk_po
                               pool->base.label);
    }
 
-   if (bo->size == pool->base.slab_size)
+   if (panfrost_bo_size(bo) == pool->base.slab_size)
       util_dynarray_append(&pool->bos, struct panfrost_bo *, bo);
    else
       util_dynarray_append(&pool->big_bos, struct panfrost_bo *, bo);
@@ -149,7 +149,7 @@ panvk_pool_get_bo_handles(struct panvk_p
 {
    unsigned idx = 0;
    util_dynarray_foreach(&pool->bos, struct panfrost_bo *, bo) {
-      assert((*bo)->gem_handle > 0);
-      handles[idx++] = (*bo)->gem_handle;
+      assert(panfrost_bo_handle(*bo) > 0);
+      handles[idx++] = panfrost_bo_handle(*bo);
    }
 }
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_pipeline.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_pipeline.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_pipeline.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_pipeline.c	2023-11-24 23:33:24.964610841 +0100
@@ -47,7 +47,7 @@ panvk_DestroyPipeline(VkDevice _device,
    VK_FROM_HANDLE(panvk_device, device, _device);
    VK_FROM_HANDLE(panvk_pipeline, pipeline, _pipeline);
 
-   panfrost_bo_unreference(pipeline->binary_bo);
-   panfrost_bo_unreference(pipeline->state_bo);
+   panvk_bo_free(device, pipeline->binary_bo);
+   panvk_bo_free(device, pipeline->state_bo);
    vk_object_free(&device->vk, pAllocator, pipeline);
 }
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_private.h.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_private.h
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_private.h.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_private.h	2023-11-24 23:33:24.964610841 +0100
@@ -114,6 +114,19 @@ typedef uint32_t xcb_window_t;
 
 #define panvk_stub() assert(!"stub")
 
+typedef uint64_t panvk_device_ptr;
+
+struct panvk_bo {
+   panvk_device_ptr device_ptr;
+   void *host_ptr;
+   struct pan_kmod_bo *kmod_bo;
+};
+
+struct panvk_bo* panvk_bo_alloc(struct panvk_device *dev, size_t size, uint32_t flags, const char *label);
+void panvk_bo_free(struct panvk_device *dev, struct panvk_bo *bo);
+void panvk_bo_mmap(struct panvk_device *dev, struct panvk_bo *bo);
+void panvk_bo_munmap(struct panvk_device *dev, struct panvk_bo *bo);
+
 #define PANVK_META_COPY_BUF2IMG_NUM_FORMATS  12
 #define PANVK_META_COPY_IMG2BUF_NUM_FORMATS  12
 #define PANVK_META_COPY_IMG2IMG_NUM_FORMATS  14
@@ -802,8 +815,8 @@ struct panvk_pipeline {
 
    uint32_t dynamic_state_mask;
 
-   struct panfrost_bo *binary_bo;
-   struct panfrost_bo *state_bo;
+   struct panvk_bo *binary_bo;
+   struct panvk_bo *state_bo;
 
    mali_ptr vpd;
    mali_ptr rsds[MESA_SHADER_STAGES];
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cmd_buffer.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cmd_buffer.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cmd_buffer.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cmd_buffer.c	2023-11-24 23:33:24.964610841 +0100
@@ -431,7 +431,7 @@ panvk_per_arch(cmd_get_tiler_context)(st
    panvk_per_arch(emit_tiler_context)(cmdbuf->device, width, height, &desc);
    memcpy(batch->tiler.descs.cpu, batch->tiler.templ,
           pan_size(TILER_CONTEXT) + pan_size(TILER_HEAP));
-   batch->tiler.ctx.bifrost = batch->tiler.descs.gpu;
+   batch->tiler.ctx.bifrost.ctx = batch->tiler.descs.gpu;
 }
 
 void
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cs.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cs.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cs.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_cs.c	2023-11-24 23:33:24.965610847 +0100
@@ -540,7 +540,7 @@ panvk_per_arch(emit_tiler_job)(const str
    panvk_emit_tiler_dcd(pipeline, draw, section);
 
    pan_section_pack(job, TILER_JOB, TILER, cfg) {
-      cfg.address = draw->tiler_ctx->bifrost;
+      cfg.address = draw->tiler_ctx->bifrost.ctx;
    }
    pan_section_pack(job, TILER_JOB, PADDING, padding)
       ;
@@ -829,10 +829,10 @@ panvk_per_arch(emit_tiler_context)(const
    const struct panfrost_device *pdev = &dev->physical_device->pdev;
 
    pan_pack(descs->cpu + pan_size(TILER_CONTEXT), TILER_HEAP, cfg) {
-      cfg.size = pdev->tiler_heap->size;
+      cfg.size = panfrost_bo_size(pdev->tiler_heap);
       cfg.base = pdev->tiler_heap->ptr.gpu;
       cfg.bottom = pdev->tiler_heap->ptr.gpu;
-      cfg.top = pdev->tiler_heap->ptr.gpu + pdev->tiler_heap->size;
+      cfg.top = pdev->tiler_heap->ptr.gpu + panfrost_bo_size(pdev->tiler_heap);
    }
 
    pan_pack(descs->cpu, TILER_CONTEXT, cfg) {
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_device.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_device.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_device.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_device.c	2023-11-24 23:33:24.965610847 +0100
@@ -67,18 +67,19 @@ panvk_queue_submit_batch(struct panvk_qu
          .jc = batch->scoreboard.first_job,
       };
 
-      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      ret =
+         drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
       assert(!ret);
 
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret =
-            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret = drmSyncobjWait(panfrost_device_fd(pdev), &submit.out_sync, 1,
+                              INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
       if (debug & PANVK_DEBUG_TRACE)
          pandecode_jc(pdev->decode_ctx, batch->scoreboard.first_job,
-                      pdev->gpu_id);
+                      panfrost_device_gpu_id(pdev));
 
       if (debug & PANVK_DEBUG_DUMP)
          pandecode_dump_mappings(pdev->decode_ctx);
@@ -101,16 +102,18 @@ panvk_queue_submit_batch(struct panvk_qu
          submit.in_sync_count = nr_in_fences;
       }
 
-      ret = drmIoctl(pdev->fd, DRM_IOCTL_PANFROST_SUBMIT, &submit);
+      ret =
+         drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_PANFROST_SUBMIT, &submit);
       assert(!ret);
       if (debug & (PANVK_DEBUG_TRACE | PANVK_DEBUG_SYNC)) {
-         ret =
-            drmSyncobjWait(pdev->fd, &submit.out_sync, 1, INT64_MAX, 0, NULL);
+         ret = drmSyncobjWait(panfrost_device_fd(pdev), &submit.out_sync, 1,
+                              INT64_MAX, 0, NULL);
          assert(!ret);
       }
 
       if (debug & PANVK_DEBUG_TRACE)
-         pandecode_jc(pdev->decode_ctx, batch->fragment_job, pdev->gpu_id);
+         pandecode_jc(pdev->decode_ctx, batch->fragment_job,
+                      panfrost_device_gpu_id(pdev));
 
       if (debug & PANVK_DEBUG_DUMP)
          pandecode_dump_mappings(pdev->decode_ctx);
@@ -134,12 +137,14 @@ panvk_queue_transfer_sync(struct panvk_q
       .fd = -1,
    };
 
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD, &handle);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_HANDLE_TO_FD,
+                  &handle);
    assert(!ret);
    assert(handle.fd >= 0);
 
    handle.handle = syncobj;
-   ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE, &handle);
+   ret = drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_FD_TO_HANDLE,
+                  &handle);
    assert(!ret);
 
    close(handle.fd);
@@ -185,7 +190,8 @@ panvk_signal_event_syncobjs(struct panvk
             .handles = (uint64_t)(uintptr_t)&event->syncobj,
             .count_handles = 1};
 
-         int ret = drmIoctl(pdev->fd, DRM_IOCTL_SYNCOBJ_RESET, &objs);
+         int ret =
+            drmIoctl(panfrost_device_fd(pdev), DRM_IOCTL_SYNCOBJ_RESET, &objs);
          assert(!ret);
          break;
       }
@@ -246,20 +252,20 @@ panvk_per_arch(queue_submit)(struct vk_q
             for (unsigned i = 0; i < batch->fb.info->attachment_count; i++) {
                const struct pan_image *image = pan_image_view_get_plane(
                   &batch->fb.info->attachments[i].iview->pview, 0);
-               bos[bo_idx++] = image->data.bo->gem_handle;
+               bos[bo_idx++] = panfrost_bo_handle(image->data.bo);
             }
          }
 
          if (batch->blit.src)
-            bos[bo_idx++] = batch->blit.src->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(batch->blit.src);
 
          if (batch->blit.dst)
-            bos[bo_idx++] = batch->blit.dst->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(batch->blit.dst);
 
          if (batch->scoreboard.first_tiler)
-            bos[bo_idx++] = pdev->tiler_heap->gem_handle;
+            bos[bo_idx++] = panfrost_bo_handle(pdev->tiler_heap);
 
-         bos[bo_idx++] = pdev->sample_positions->gem_handle;
+         bos[bo_idx++] = panfrost_bo_handle(pdev->sample_positions);
          assert(bo_idx == nr_bos);
 
          /* Merge identical BO entries. */
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_image.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_image.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_image.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_image.c	2023-11-24 23:33:24.965610847 +0100
@@ -143,7 +143,7 @@ panvk_per_arch(CreateImageView)(VkDevice
                        : MALI_ATTRIBUTE_TYPE_3D_INTERLEAVED;
          cfg.pointer = image->pimage.data.bo->ptr.gpu + offset;
          cfg.stride = util_format_get_blocksize(view->pview.format);
-         cfg.size = image->pimage.data.bo->size - offset;
+         cfg.size = panfrost_bo_size(image->pimage.data.bo) - offset;
       }
 
       attrib_buf += pan_size(ATTRIBUTE_BUFFER);
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_clear.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_clear.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_clear.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_clear.c	2023-11-24 23:33:24.965610847 +0100
@@ -51,7 +51,7 @@ panvk_meta_clear_color_attachment_shader
    nir_store_var(&b, out, clear_values, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_copy.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_copy.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_copy.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_meta_copy.c	2023-11-24 23:33:24.965610847 +0100
@@ -418,7 +418,7 @@ panvk_meta_copy_img2img_shader(struct pa
    nir_store_var(&b, out, texel, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -960,7 +960,7 @@ panvk_meta_copy_buf2img_shader(struct pa
    nir_store_var(&b, out, texel, 0xff);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1418,7 +1418,7 @@ panvk_meta_copy_img2buf_shader(struct pa
    nir_pop_if(&b, NULL);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1650,7 +1650,7 @@ panvk_meta_copy_buf2buf_shader(struct pa
                     (1 << ncomps) - 1);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
@@ -1776,7 +1776,7 @@ panvk_meta_fill_buf_shader(struct panfro
    nir_store_global(&b, ptr, sizeof(uint32_t), val, 1);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .is_blit = true,
       .no_ubo_to_push = true,
    };
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_pipeline.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_pipeline.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_pipeline.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_pipeline.c	2023-11-24 23:33:24.965610847 +0100
@@ -160,19 +160,19 @@ panvk_pipeline_builder_upload_shaders(st
    if (builder->shader_total_size == 0)
       return VK_SUCCESS;
 
-   struct panfrost_bo *bin_bo =
-      panfrost_bo_create(&builder->device->physical_device->pdev,
-                         builder->shader_total_size, PAN_BO_EXECUTE, "Shader");
+   struct panvk_bo *bin_bo =
+      panvk_bo_alloc(builder->device, builder->shader_total_size,
+                     PAN_KMOD_BO_FLAG_EXECUTABLE, "Shader");
 
    pipeline->binary_bo = bin_bo;
-   panfrost_bo_mmap(bin_bo);
+   panvk_bo_mmap(builder->device, pipeline->binary_bo);
 
    for (uint32_t i = 0; i < MESA_SHADER_STAGES; i++) {
       const struct panvk_shader *shader = builder->shaders[i];
       if (!shader)
          continue;
 
-      memcpy(pipeline->binary_bo->ptr.cpu + builder->stages[i].shader_offset,
+      memcpy(pipeline->binary_bo->host_ptr + builder->stages[i].shader_offset,
              util_dynarray_element(&shader->binary, uint8_t, 0),
              util_dynarray_num_elements(&shader->binary, uint8_t));
    }
@@ -211,9 +211,8 @@ panvk_pipeline_builder_alloc_static_stat
    }
 
    if (bo_size) {
-      pipeline->state_bo =
-         panfrost_bo_create(pdev, bo_size, 0, "Pipeline descriptors");
-      panfrost_bo_mmap(pipeline->state_bo);
+      pipeline->state_bo = panvk_bo_alloc(builder->device, bo_size, 0, "Pipeline descriptors");
+      panvk_bo_mmap(builder->device, pipeline->state_bo);
    }
 }
 
@@ -259,14 +258,14 @@ panvk_pipeline_builder_init_shaders(stru
       /* Handle empty shaders gracefully */
       if (util_dynarray_num_elements(&builder->shaders[i]->binary, uint8_t)) {
          shader_ptr =
-            pipeline->binary_bo->ptr.gpu + builder->stages[i].shader_offset;
+            pipeline->binary_bo->device_ptr + builder->stages[i].shader_offset;
       }
 
       if (i != MESA_SHADER_FRAGMENT) {
          void *rsd =
-            pipeline->state_bo->ptr.cpu + builder->stages[i].rsd_offset;
+            pipeline->state_bo->host_ptr + builder->stages[i].rsd_offset;
          mali_ptr gpu_rsd =
-            pipeline->state_bo->ptr.gpu + builder->stages[i].rsd_offset;
+            pipeline->state_bo->device_ptr + builder->stages[i].rsd_offset;
 
          panvk_per_arch(emit_non_fs_rsd)(builder->device, &shader->info,
                                          shader_ptr, rsd);
@@ -280,9 +279,9 @@ panvk_pipeline_builder_init_shaders(stru
    }
 
    if (builder->create_info.gfx && !pipeline->fs.dynamic_rsd) {
-      void *rsd = pipeline->state_bo->ptr.cpu +
+      void *rsd = pipeline->state_bo->host_ptr +
                   builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
-      mali_ptr gpu_rsd = pipeline->state_bo->ptr.gpu +
+      mali_ptr gpu_rsd = pipeline->state_bo->device_ptr +
                          builder->stages[MESA_SHADER_FRAGMENT].rsd_offset;
       void *bd = rsd + pan_size(RENDERER_STATE);
 
@@ -320,11 +319,11 @@ panvk_pipeline_builder_parse_viewport(st
    if (!builder->rasterizer_discard &&
        panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT) &&
        panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_SCISSOR)) {
-      void *vpd = pipeline->state_bo->ptr.cpu + builder->vpd_offset;
+      void *vpd = pipeline->state_bo->host_ptr + builder->vpd_offset;
       panvk_per_arch(emit_viewport)(
          builder->create_info.gfx->pViewportState->pViewports,
          builder->create_info.gfx->pViewportState->pScissors, vpd);
-      pipeline->vpd = pipeline->state_bo->ptr.gpu + builder->vpd_offset;
+      pipeline->vpd = pipeline->state_bo->device_ptr + builder->vpd_offset;
    }
    if (panvk_pipeline_static_state(pipeline, VK_DYNAMIC_STATE_VIEWPORT))
       pipeline->viewport =
@@ -666,7 +665,7 @@ panvk_pipeline_builder_init_fs_state(str
 
    pipeline->fs.dynamic_rsd =
       pipeline->dynamic_state_mask & PANVK_DYNAMIC_FS_RSD_MASK;
-   pipeline->fs.address = pipeline->binary_bo->ptr.gpu +
+   pipeline->fs.address = pipeline->binary_bo->device_ptr +
                           builder->stages[MESA_SHADER_FRAGMENT].shader_offset;
    pipeline->fs.info = builder->shaders[MESA_SHADER_FRAGMENT]->info;
    pipeline->fs.rt_mask = builder->active_color_attachments;
diff -up mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_shader.c.8~ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_shader.c
--- mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_shader.c.8~	2023-11-24 20:40:33.000000000 +0100
+++ mesa-23.3.0-rc5/src/panfrost/vulkan/panvk_vX_shader.c	2023-11-24 23:33:24.965610847 +0100
@@ -254,7 +254,7 @@ panvk_per_arch(shader_create)(struct pan
               true, true);
 
    struct panfrost_compile_inputs inputs = {
-      .gpu_id = pdev->gpu_id,
+      .gpu_id = panfrost_device_gpu_id(pdev),
       .no_ubo_to_push = true,
       .no_idvs = true, /* TODO */
    };
